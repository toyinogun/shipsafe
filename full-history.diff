diff --git a/.dockerignore b/.dockerignore
new file mode 100644
index 0000000..321081d
--- /dev/null
+++ b/.dockerignore
@@ -0,0 +1,33 @@
+# Git
+.git
+.gitignore
+
+# IDE
+.idea
+.vscode
+*.swp
+*.swo
+
+# OS
+.DS_Store
+Thumbs.db
+
+# Build artifacts
+shipsafe
+*.exe
+
+# Deployment (not needed inside the image)
+deploy/
+.forgejo/
+
+# Documentation
+docs/
+*.md
+!go.sum
+
+# Tests fixtures (not needed at runtime)
+tests/
+
+# Config examples
+configs/
+shipsafe.example.yml
diff --git a/.forgejo/workflows/ci.yml b/.forgejo/workflows/ci.yml
new file mode 100644
index 0000000..f55fdc0
--- /dev/null
+++ b/.forgejo/workflows/ci.yml
@@ -0,0 +1,35 @@
+name: CI
+
+on:
+  push:
+    branches: [main]
+  pull_request:
+
+jobs:
+  build-and-test:
+    runs-on: ubuntu-latest
+    steps:
+      - name: Checkout code
+        uses: actions/checkout@v4
+
+      - name: Setup Go
+        uses: actions/setup-go@v5
+        with:
+          go-version: "1.25"
+
+      - name: Vet
+        run: go vet ./...
+
+      - name: Test
+        run: go test ./...
+
+      - name: Build
+        run: |
+          go build -ldflags "-s -w \
+            -X github.com/toyinlola/shipsafe/cmd.Version=ci \
+            -X github.com/toyinlola/shipsafe/cmd.Commit=${{ github.sha }} \
+            -X github.com/toyinlola/shipsafe/cmd.BuildDate=$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
+            -o shipsafe .
+
+      - name: Dogfood — scan clean diff
+        run: ./shipsafe scan --diff tests/fixtures/diffs/clean.diff
diff --git a/cmd/root.go b/cmd/root.go
new file mode 100644
index 0000000..f03e6ef
--- /dev/null
+++ b/cmd/root.go
@@ -0,0 +1,57 @@
+// Package cmd implements the ShipSafe CLI commands using Cobra.
+package cmd
+
+import (
+	"log/slog"
+	"os"
+
+	"github.com/spf13/cobra"
+)
+
+var (
+	cfgFile string
+	verbose bool
+	format  string
+	output  string
+)
+
+var rootCmd = &cobra.Command{
+	Use:   "shipsafe",
+	Short: "AI code verification gateway",
+	Long: `ShipSafe is a self-hosted AI code verification gateway.
+
+It sits in CI/CD pipelines between code generation and merge, running
+multi-layered verification on AI-generated code to produce trust scores
+and verification reports.`,
+	PersistentPreRunE: func(cmd *cobra.Command, args []string) error {
+		return setupLogging()
+	},
+	SilenceUsage:  true,
+	SilenceErrors: true,
+}
+
+// Execute runs the root command and returns any error.
+func Execute() error {
+	return rootCmd.Execute()
+}
+
+func init() {
+	rootCmd.PersistentFlags().StringVar(&cfgFile, "config", "", "config file path (default: .shipsafe.yml)")
+	rootCmd.PersistentFlags().BoolVarP(&verbose, "verbose", "v", false, "enable verbose output")
+	rootCmd.PersistentFlags().StringVarP(&format, "format", "f", "terminal", "output format (terminal|json|markdown|html)")
+	rootCmd.PersistentFlags().StringVarP(&output, "output", "o", "", "write output to file instead of stdout")
+}
+
+func setupLogging() error {
+	level := slog.LevelInfo
+	if verbose {
+		level = slog.LevelDebug
+	}
+
+	handler := slog.NewTextHandler(os.Stderr, &slog.HandlerOptions{
+		Level: level,
+	})
+	slog.SetDefault(slog.New(handler))
+
+	return nil
+}
diff --git a/cmd/scan.go b/cmd/scan.go
new file mode 100644
index 0000000..00e7578
--- /dev/null
+++ b/cmd/scan.go
@@ -0,0 +1,180 @@
+package cmd
+
+import (
+	"context"
+	"fmt"
+	"io"
+	"log/slog"
+	"os"
+	"os/exec"
+
+	"github.com/spf13/cobra"
+	"github.com/toyinlola/shipsafe/pkg/analyzer"
+	"github.com/toyinlola/shipsafe/pkg/cli"
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+	"github.com/toyinlola/shipsafe/pkg/report"
+	"github.com/toyinlola/shipsafe/pkg/scorer"
+	"github.com/toyinlola/shipsafe/pkg/vcs"
+)
+
+var diffFile string
+
+var scanCmd = &cobra.Command{
+	Use:   "scan [path]",
+	Short: "Scan code for issues and generate a trust score",
+	Long: `Scan analyzes code changes and produces a trust score report.
+
+Scan a diff file directly:
+  shipsafe scan --diff ./path/to/file.diff
+
+Scan a directory (compares against git HEAD):
+  shipsafe scan ./path/to/repo`,
+	Args: cobra.MaximumNArgs(1),
+	RunE: runScan,
+}
+
+func init() {
+	scanCmd.Flags().StringVar(&diffFile, "diff", "", "path to a unified diff file to analyze")
+	rootCmd.AddCommand(scanCmd)
+}
+
+// formatter writes a structured report to a writer.
+type formatter interface {
+	Format(w io.Writer, report *interfaces.Report) error
+}
+
+func runScan(cmd *cobra.Command, args []string) error {
+	ctx := cmd.Context()
+
+	var target string
+	if len(args) > 0 {
+		target = args[0]
+	}
+
+	if diffFile == "" && target == "" {
+		return fmt.Errorf("scan: provide either --diff <file> or a target path")
+	}
+
+	// 1. Load configuration.
+	cfg, err := cli.LoadConfig(cfgFile)
+	if err != nil {
+		return fmt.Errorf("scan: %w", err)
+	}
+
+	slog.Debug("config loaded",
+		"thresholds.green", cfg.Thresholds.Green,
+		"thresholds.yellow", cfg.Thresholds.Yellow,
+	)
+
+	// 2. Parse the diff.
+	parser := vcs.NewDiffParser()
+	var diff *interfaces.Diff
+
+	if diffFile != "" {
+		slog.Info("parsing diff file", "path", diffFile)
+		diff, err = parser.ParseFile(ctx, diffFile)
+	} else {
+		slog.Info("running git diff", "target", target)
+		diff, err = diffFromGit(ctx, parser, target)
+	}
+	if err != nil {
+		return fmt.Errorf("scan: %w", err)
+	}
+
+	slog.Info("diff parsed", "files", len(diff.Files))
+
+	// 3. Build analyzer registry and register enabled analyzers.
+	registry := analyzer.NewRegistry()
+	registerAnalyzers(registry, cfg)
+
+	// 4. Run all enabled analyzers.
+	engine := analyzer.NewEngine(registry)
+	results, err := engine.Run(ctx, diff)
+	if err != nil {
+		return fmt.Errorf("scan: running analysis: %w", err)
+	}
+
+	// 5. Calculate trust score.
+	calc := scorer.NewCalculator(
+		scorer.WithThresholds(cfg.Thresholds.Green, cfg.Thresholds.Yellow),
+	)
+	trustScore := calc.Score(results)
+
+	// 6. Generate report.
+	gen := report.NewGenerator()
+	rpt := gen.Generate(results, trustScore, diff)
+
+	// 7. Select formatter and write output.
+	f := selectFormatter(format)
+
+	var w io.Writer = os.Stdout
+	if output != "" {
+		file, fileErr := os.Create(output)
+		if fileErr != nil {
+			return fmt.Errorf("scan: creating output file: %w", fileErr)
+		}
+		defer file.Close() // best-effort cleanup
+		w = file
+	}
+
+	if err := f.Format(w, rpt); err != nil {
+		return fmt.Errorf("scan: writing report: %w", err)
+	}
+
+	// 8. Exit with code 1 for RED rating.
+	if trustScore.Rating == interfaces.RatingRed {
+		os.Exit(1)
+	}
+
+	return nil
+}
+
+// diffFromGit runs `git diff HEAD` in the given directory and parses the output.
+func diffFromGit(ctx context.Context, parser interfaces.DiffParser, dir string) (*interfaces.Diff, error) {
+	gitCmd := exec.CommandContext(ctx, "git", "diff", "HEAD")
+	gitCmd.Dir = dir
+
+	out, err := gitCmd.Output()
+	if err != nil {
+		return nil, fmt.Errorf("running git diff in %s: %w", dir, err)
+	}
+
+	if len(out) == 0 {
+		return nil, fmt.Errorf("no changes found in %s (git diff HEAD returned empty)", dir)
+	}
+
+	return parser.Parse(ctx, out)
+}
+
+// registerAnalyzers adds all enabled analyzers to the registry based on config.
+func registerAnalyzers(registry *analyzer.Registry, cfg *cli.Config) {
+	if cfg.Analyzers.Secrets.IsEnabled() {
+		_ = registry.Register(analyzer.NewSecretsAnalyzer())
+	}
+	if cfg.Analyzers.Patterns.IsEnabled() {
+		_ = registry.Register(analyzer.NewPatternsAnalyzer())
+	}
+	if cfg.Analyzers.Complexity.IsEnabled() {
+		_ = registry.Register(analyzer.NewComplexityAnalyzer(
+			analyzer.WithComplexityThreshold(cfg.Analyzers.Complexity.Threshold),
+		))
+	}
+	if cfg.Analyzers.Coverage.IsEnabled() {
+		_ = registry.Register(analyzer.NewCoverageAnalyzer())
+	}
+	if cfg.Analyzers.Imports.IsEnabled() {
+		_ = registry.Register(analyzer.NewImportsAnalyzer())
+	}
+}
+
+// selectFormatter returns the appropriate report formatter for the given format name.
+func selectFormatter(name string) formatter {
+	switch name {
+	case "json":
+		return report.NewJSONFormatter()
+	case "markdown":
+		return report.NewMarkdownFormatter()
+	default:
+		return report.NewTerminalFormatter()
+	}
+}
diff --git a/cmd/version.go b/cmd/version.go
new file mode 100644
index 0000000..db1d573
--- /dev/null
+++ b/cmd/version.go
@@ -0,0 +1,32 @@
+package cmd
+
+import (
+	"fmt"
+
+	"github.com/spf13/cobra"
+)
+
+// Build-time variables, injected via ldflags:
+//
+//	go build -ldflags "-X github.com/toyinlola/shipsafe/cmd.Version=1.0.0
+//	  -X github.com/toyinlola/shipsafe/cmd.Commit=$(git rev-parse --short HEAD)
+//	  -X github.com/toyinlola/shipsafe/cmd.BuildDate=$(date -u +%Y-%m-%dT%H:%M:%SZ)"
+var (
+	Version   = "dev"
+	Commit    = "none"
+	BuildDate = "unknown"
+)
+
+var versionCmd = &cobra.Command{
+	Use:   "version",
+	Short: "Print the version information",
+	Run: func(cmd *cobra.Command, args []string) {
+		fmt.Printf("shipsafe %s\n", Version)
+		fmt.Printf("  commit:  %s\n", Commit)
+		fmt.Printf("  built:   %s\n", BuildDate)
+	},
+}
+
+func init() {
+	rootCmd.AddCommand(versionCmd)
+}
diff --git a/deploy/ci/github-action.yml b/deploy/ci/github-action.yml
new file mode 100644
index 0000000..7c109cd
--- /dev/null
+++ b/deploy/ci/github-action.yml
@@ -0,0 +1,39 @@
+name: "ShipSafe Code Verification"
+description: "Run ShipSafe AI code verification on pull requests"
+
+inputs:
+  version:
+    description: "ShipSafe version to download"
+    required: false
+    default: "latest"
+  config-path:
+    description: "Path to .shipsafe.yml config file"
+    required: false
+    default: ".shipsafe.yml"
+  fail-on:
+    description: "Trust score level that causes a failure (red or yellow)"
+    required: false
+    default: "red"
+
+runs:
+  using: "composite"
+  steps:
+    - name: Download ShipSafe
+      shell: bash
+      run: |
+        VERSION="${{ inputs.version }}"
+        if [ "$VERSION" = "latest" ]; then
+          DOWNLOAD_URL="https://github.com/toyinlola/shipsafe/releases/latest/download/shipsafe-linux-amd64"
+        else
+          DOWNLOAD_URL="https://github.com/toyinlola/shipsafe/releases/download/v${VERSION}/shipsafe-linux-amd64"
+        fi
+        curl -fsSL "$DOWNLOAD_URL" -o /usr/local/bin/shipsafe
+        chmod +x /usr/local/bin/shipsafe
+
+    - name: Run ShipSafe
+      shell: bash
+      env:
+        SHIPSAFE_CONFIG: ${{ inputs.config-path }}
+        SHIPSAFE_FAIL_ON: ${{ inputs.fail-on }}
+      run: |
+        shipsafe ci --config "$SHIPSAFE_CONFIG" --fail-on "$SHIPSAFE_FAIL_ON"
diff --git a/deploy/docker/Dockerfile b/deploy/docker/Dockerfile
new file mode 100644
index 0000000..14938b6
--- /dev/null
+++ b/deploy/docker/Dockerfile
@@ -0,0 +1,39 @@
+# Stage 1: Build the ShipSafe binary
+FROM golang:1.25-alpine AS builder
+
+RUN apk add --no-cache git ca-certificates
+
+WORKDIR /src
+
+# Cache dependency downloads
+COPY go.mod go.sum ./
+RUN go mod download
+
+# Copy source and build static binary with version injection
+COPY . .
+
+ARG VERSION=dev
+ARG COMMIT=none
+ARG BUILD_DATE=unknown
+
+RUN CGO_ENABLED=0 GOOS=linux go build \
+    -ldflags "-s -w \
+      -X github.com/toyinlola/shipsafe/cmd.Version=${VERSION} \
+      -X github.com/toyinlola/shipsafe/cmd.Commit=${COMMIT} \
+      -X github.com/toyinlola/shipsafe/cmd.BuildDate=${BUILD_DATE}" \
+    -o /shipsafe .
+
+# Stage 2: Minimal runtime image
+FROM alpine:3.20
+
+# git is required for `shipsafe scan` on repositories
+RUN apk add --no-cache git ca-certificates \
+    && addgroup -S shipsafe \
+    && adduser -S -G shipsafe shipsafe
+
+COPY --from=builder /shipsafe /usr/local/bin/shipsafe
+
+USER shipsafe
+WORKDIR /workspace
+
+ENTRYPOINT ["shipsafe"]
diff --git a/go.mod b/go.mod
index ad5ffbd..91ea303 100644
--- a/go.mod
+++ b/go.mod
@@ -1,3 +1,10 @@
 module github.com/toyinlola/shipsafe
 
 go 1.25.4
+
+require (
+	github.com/inconshreveable/mousetrap v1.1.0 // indirect
+	github.com/spf13/cobra v1.10.2 // indirect
+	github.com/spf13/pflag v1.0.9 // indirect
+	gopkg.in/yaml.v3 v3.0.1 // indirect
+)
diff --git a/go.sum b/go.sum
new file mode 100644
index 0000000..ff4d6ec
--- /dev/null
+++ b/go.sum
@@ -0,0 +1,12 @@
+github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
+github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
+github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
+github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
+github.com/spf13/cobra v1.10.2 h1:DMTTonx5m65Ic0GOoRY2c16WCbHxOOw6xxezuLaBpcU=
+github.com/spf13/cobra v1.10.2/go.mod h1:7C1pvHqHw5A4vrJfjNwvOdzYu0Gml16OCs2GRiTUUS4=
+github.com/spf13/pflag v1.0.9 h1:9exaQaMOCwffKiiiYk6/BndUBv+iRViNW+4lEMi0PvY=
+github.com/spf13/pflag v1.0.9/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
+go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
+gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
+gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
+gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
diff --git a/main.go b/main.go
new file mode 100644
index 0000000..7b3b310
--- /dev/null
+++ b/main.go
@@ -0,0 +1,15 @@
+// Package main is the entrypoint for the ShipSafe CLI.
+// It delegates all command handling to the cmd package.
+package main
+
+import (
+	"os"
+
+	"github.com/toyinlola/shipsafe/cmd"
+)
+
+func main() {
+	if err := cmd.Execute(); err != nil {
+		os.Exit(1)
+	}
+}
diff --git a/pkg/analyzer/complexity.go b/pkg/analyzer/complexity.go
new file mode 100644
index 0000000..d3694b9
--- /dev/null
+++ b/pkg/analyzer/complexity.go
@@ -0,0 +1,293 @@
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"regexp"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// Default complexity thresholds.
+const (
+	defaultComplexityThreshold = 15
+	highComplexityThreshold    = 20
+	testFileThresholdBoost     = 10
+)
+
+// Decision-point patterns matched against added lines.
+// Each pattern counts as one increment to cyclomatic complexity.
+var decisionPatterns = []*regexp.Regexp{
+	regexp.MustCompile(`\b(if|else\s+if|elif)\b`),
+	regexp.MustCompile(`\bfor\b`),
+	regexp.MustCompile(`\bwhile\b`),
+	regexp.MustCompile(`\bcase\b`),
+	regexp.MustCompile(`\bcatch\b`),
+	regexp.MustCompile(`\bexcept\b`),
+}
+
+// Logical operators that add branching complexity.
+var logicalOpPattern = regexp.MustCompile(`(&&|\|\|)`)
+
+// Ternary operator.
+var ternaryPattern = regexp.MustCompile(`\?[^?].*:`)
+
+// Function definition patterns across languages.
+var funcDefPatterns = []*regexp.Regexp{
+	// Go: func name(...) or func (receiver) name(...)
+	regexp.MustCompile(`^\s*func\s+(?:\([^)]*\)\s*)?\w+\s*\(`),
+	// JavaScript/TypeScript: function name(, const name = (...) =>, async function
+	regexp.MustCompile(`^\s*(?:export\s+)?(?:async\s+)?function\s+\w+\s*\(`),
+	regexp.MustCompile(`^\s*(?:export\s+)?(?:const|let|var)\s+\w+\s*=\s*(?:async\s+)?(?:\([^)]*\)|[a-zA-Z_]\w*)\s*=>`),
+	// Python: def name(
+	regexp.MustCompile(`^\s*(?:async\s+)?def\s+\w+\s*\(`),
+	// Java/C#: access modifier + return type + name(
+	regexp.MustCompile(`^\s*(?:(?:public|private|protected|static|final|abstract)\s+)+\w+\s+\w+\s*\(`),
+	// Ruby: def name
+	regexp.MustCompile(`^\s*def\s+\w+`),
+	// Rust: fn name(
+	regexp.MustCompile(`^\s*(?:pub\s+)?(?:async\s+)?fn\s+\w+`),
+}
+
+// Compiled regex for function name extraction (compiled once).
+var (
+	goFuncNameRe   = regexp.MustCompile(`func\s+(?:\([^)]*\)\s*)?(\w+)\s*\(`)
+	jsFuncNameRe   = regexp.MustCompile(`function\s+(\w+)\s*\(`)
+	arrowFuncRe    = regexp.MustCompile(`(?:const|let|var)\s+(\w+)\s*=`)
+	pyFuncNameRe   = regexp.MustCompile(`def\s+(\w+)\s*\(`)
+	rbFuncNameRe   = regexp.MustCompile(`def\s+(\w+)`)
+	rsFuncNameRe   = regexp.MustCompile(`fn\s+(\w+)`)
+	javaFuncNameRe = regexp.MustCompile(`\s(\w+)\s*\(`)
+)
+
+// ComplexityAnalyzer measures cyclomatic complexity of functions in diffs.
+type ComplexityAnalyzer struct {
+	threshold int
+}
+
+// ComplexityOption configures the complexity analyzer.
+type ComplexityOption func(*ComplexityAnalyzer)
+
+// WithComplexityThreshold sets the maximum allowed complexity.
+func WithComplexityThreshold(t int) ComplexityOption {
+	return func(a *ComplexityAnalyzer) {
+		if t > 0 {
+			a.threshold = t
+		}
+	}
+}
+
+// NewComplexityAnalyzer creates a complexity analyzer with optional configuration.
+func NewComplexityAnalyzer(opts ...ComplexityOption) *ComplexityAnalyzer {
+	a := &ComplexityAnalyzer{
+		threshold: defaultComplexityThreshold,
+	}
+	for _, opt := range opts {
+		opt(a)
+	}
+	return a
+}
+
+// Name returns the analyzer identifier.
+func (c *ComplexityAnalyzer) Name() string {
+	return "complexity"
+}
+
+// Analyze calculates per-function complexity from added lines in the diff.
+func (c *ComplexityAnalyzer) Analyze(ctx context.Context, diff *interfaces.Diff) (*interfaces.AnalysisResult, error) {
+	result := &interfaces.AnalysisResult{
+		AnalyzerName: c.Name(),
+	}
+
+	for i := range diff.Files {
+		if ctx.Err() != nil {
+			return result, ctx.Err()
+		}
+
+		file := &diff.Files[i]
+		if file.IsBinary || file.Status == interfaces.FileDeleted {
+			continue
+		}
+
+		findings := c.analyzeFile(file)
+		result.Findings = append(result.Findings, findings...)
+	}
+
+	return result, nil
+}
+
+// addedLine pairs a line number with its content for complexity analysis.
+type addedLine struct {
+	number  int
+	content string
+}
+
+// funcRegion tracks a function's added lines for complexity counting.
+type funcRegion struct {
+	name      string
+	startLine int
+	endLine   int
+	lines     []string
+}
+
+// analyzeFile extracts function regions from added lines and scores them.
+func (c *ComplexityAnalyzer) analyzeFile(file *interfaces.FileDiff) []interfaces.Finding {
+	var lines []addedLine
+	for j := range file.Hunks {
+		for _, line := range file.Hunks[j].AddedLines {
+			lines = append(lines, addedLine{number: line.Number, content: line.Content})
+		}
+	}
+
+	if len(lines) == 0 {
+		return nil
+	}
+
+	// Test files get a higher threshold — complex fixtures are intentional.
+	threshold := c.threshold
+	highThreshold := highComplexityThreshold
+	if isTestFile(file.Path) {
+		threshold += testFileThresholdBoost
+		highThreshold += testFileThresholdBoost
+	}
+
+	regions := extractFuncRegions(lines)
+
+	var findings []interfaces.Finding
+	for _, region := range regions {
+		complexity := countComplexity(region.lines)
+		if complexity > threshold {
+			severity := interfaces.SeverityMedium
+			if complexity > highThreshold {
+				severity = interfaces.SeverityHigh
+			}
+
+			findings = append(findings, interfaces.Finding{
+				ID:        fmt.Sprintf("CX-%s-%d", sanitizeID(region.name), region.startLine),
+				Category:  interfaces.CategoryComplexity,
+				Severity:  severity,
+				File:      file.Path,
+				StartLine: region.startLine,
+				EndLine:   region.endLine,
+				Title:     fmt.Sprintf("High cyclomatic complexity in %s (%d)", region.name, complexity),
+				Description: fmt.Sprintf(
+					"Function %s has a cyclomatic complexity of %d (threshold: %d). Complex functions are harder to test and maintain.",
+					region.name, complexity, threshold,
+				),
+				Suggestion: "Break the function into smaller, focused functions with single responsibilities.",
+				Source:     "complexity",
+				Confidence: 0.80,
+				Metadata: map[string]any{
+					"complexity": complexity,
+					"threshold":  threshold,
+				},
+			})
+		}
+	}
+
+	return findings
+}
+
+// extractFuncRegions identifies function definitions in added lines and groups
+// subsequent lines until the next function definition.
+func extractFuncRegions(lines []addedLine) []funcRegion {
+	var regions []funcRegion
+	var current *funcRegion
+
+	for _, line := range lines {
+		if name, ok := extractFuncName(line.content); ok {
+			if current != nil {
+				current.endLine = line.number - 1
+				if current.endLine < current.startLine {
+					current.endLine = current.startLine
+				}
+				regions = append(regions, *current)
+			}
+			current = &funcRegion{
+				name:      name,
+				startLine: line.number,
+				lines:     []string{line.content},
+			}
+		} else if current != nil {
+			current.lines = append(current.lines, line.content)
+			current.endLine = line.number
+		}
+	}
+
+	if current != nil {
+		if current.endLine == 0 {
+			current.endLine = current.startLine
+		}
+		regions = append(regions, *current)
+	}
+
+	return regions
+}
+
+// extractFuncName tries to extract the function name from a line.
+func extractFuncName(line string) (string, bool) {
+	for _, re := range funcDefPatterns {
+		if re.MatchString(line) {
+			return parseFuncNameFromLine(line), true
+		}
+	}
+	return "", false
+}
+
+// parseFuncNameFromLine extracts the function name identifier from the line.
+func parseFuncNameFromLine(line string) string {
+	if m := goFuncNameRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	if m := jsFuncNameRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	if m := arrowFuncRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	if m := pyFuncNameRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	if m := rbFuncNameRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	if m := rsFuncNameRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	if m := javaFuncNameRe.FindStringSubmatch(line); len(m) > 1 {
+		return m[1]
+	}
+	return "unknown"
+}
+
+// countComplexity counts decision points in the given lines.
+// Base complexity is 1 (for the function itself).
+func countComplexity(lines []string) int {
+	complexity := 1
+
+	for _, line := range lines {
+		trimmed := strings.TrimSpace(line)
+		// Skip pure comments.
+		if strings.HasPrefix(trimmed, "//") || strings.HasPrefix(trimmed, "#") ||
+			strings.HasPrefix(trimmed, "/*") || strings.HasPrefix(trimmed, "*") {
+			continue
+		}
+
+		// Count decision-point keywords.
+		for _, re := range decisionPatterns {
+			matches := re.FindAllString(line, -1)
+			complexity += len(matches)
+		}
+
+		// Count logical operators.
+		matches := logicalOpPattern.FindAllString(line, -1)
+		complexity += len(matches)
+
+		// Count ternary operators.
+		matches = ternaryPattern.FindAllString(line, -1)
+		complexity += len(matches)
+	}
+
+	return complexity
+}
diff --git a/pkg/analyzer/complexity_test.go b/pkg/analyzer/complexity_test.go
new file mode 100644
index 0000000..666c936
--- /dev/null
+++ b/pkg/analyzer/complexity_test.go
@@ -0,0 +1,664 @@
+package analyzer
+
+import (
+	"context"
+	"strings"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+func TestComplexityAnalyzer_Name(t *testing.T) {
+	a := NewComplexityAnalyzer()
+	if a.Name() != "complexity" {
+		t.Errorf("expected name %q, got %q", "complexity", a.Name())
+	}
+}
+
+func TestComplexityAnalyzer_ImplementsInterface(t *testing.T) {
+	var _ Analyzer = NewComplexityAnalyzer()
+}
+
+func TestComplexityAnalyzer_WithThresholdOption(t *testing.T) {
+	a := NewComplexityAnalyzer(WithComplexityThreshold(10))
+	if a.threshold != 10 {
+		t.Errorf("expected threshold 10, got %d", a.threshold)
+	}
+}
+
+func TestComplexityAnalyzer_HighComplexityFunction_ReturnsFinding(t *testing.T) {
+	// Build a function with many decision points (complexity > 15).
+	lines := []string{
+		`func processData(input []string) error {`,
+		`    if len(input) == 0 {`,
+		`        return nil`,
+		`    }`,
+		`    for _, item := range input {`,
+		`        if item == "" {`,
+		`            continue`,
+		`        }`,
+		`        if strings.HasPrefix(item, "a") {`,
+		`            if len(item) > 5 {`,
+		`                for _, c := range item {`,
+		`                    if c == 'x' || c == 'y' {`,
+		`                        break`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        } else if strings.HasPrefix(item, "b") {`,
+		`            switch item {`,
+		`            case "ba":`,
+		`            case "bb":`,
+		`            case "bc":`,
+		`            case "bd":`,
+		`            }`,
+		`        } else if item == "c" && len(item) > 0 {`,
+		`            while true {`,
+		`                if done {`,
+		`                    break`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`    return nil`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("processor.go", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one complexity finding for high-complexity function")
+	}
+
+	f := result.Findings[0]
+	if f.Category != interfaces.CategoryComplexity {
+		t.Errorf("expected category %q, got %q", interfaces.CategoryComplexity, f.Category)
+	}
+	if !strings.Contains(f.Title, "processData") {
+		t.Errorf("expected title to contain function name, got %q", f.Title)
+	}
+}
+
+func TestComplexityAnalyzer_VeryHighComplexity_ReturnsHighSeverity(t *testing.T) {
+	// Build a function with complexity > 20.
+	// Decision points: 8 if + 2 for + 5 case + 1 while + 2 || + 2 && + 1 elif = 21
+	// Total complexity: 1 (base) + 21 = 22
+	lines := []string{
+		`func megaFunction(x int) int {`,
+		`    if x > 0 {`,
+		`        if x > 1 {`,
+		`            if x > 2 {`,
+		`                if x > 3 {`,
+		`                    if x > 4 {`,
+		`                        if x > 5 {`,
+		`                            for i := 0; i < x; i++ {`,
+		`                                if i%2 == 0 || i%3 == 0 {`,
+		`                                    switch i {`,
+		`                                    case 1:`,
+		`                                    case 2:`,
+		`                                    case 3:`,
+		`                                    case 4:`,
+		`                                    case 5:`,
+		`                                    }`,
+		`                                }`,
+		`                                while x > 0 && i < 100 {`,
+		`                                    if done || abort {`,
+		`                                    }`,
+		`                                }`,
+		`                                for j := 0; j < i; j++ {`,
+		`                                    if j > 0 && j < 50 {`,
+		`                                    }`,
+		`                                }`,
+		`                            }`,
+		`                        }`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`    return x`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("mega.go", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected findings for very high complexity function")
+	}
+
+	f := result.Findings[0]
+	if f.Severity != interfaces.SeverityHigh {
+		t.Errorf("expected HIGH severity for complexity > 20, got %q", f.Severity)
+	}
+}
+
+func TestComplexityAnalyzer_LowComplexityFunction_NoFinding(t *testing.T) {
+	lines := []string{
+		`func add(a, b int) int {`,
+		`    return a + b`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("math.go", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for simple function, got %d", len(result.Findings))
+	}
+}
+
+func TestComplexityAnalyzer_MultipleFunctions_OnlyFlagsComplex(t *testing.T) {
+	lines := []string{
+		`func simple(x int) int {`,
+		`    return x + 1`,
+		`}`,
+		`func complex(input []string) error {`,
+		`    if len(input) == 0 {`,
+		`        return nil`,
+		`    }`,
+		`    for _, item := range input {`,
+		`        if item == "" {`,
+		`            continue`,
+		`        }`,
+		`        if strings.HasPrefix(item, "a") {`,
+		`            if len(item) > 5 {`,
+		`                for _, c := range item {`,
+		`                    if c == 'x' || c == 'y' {`,
+		`                        break`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        } else if strings.HasPrefix(item, "b") {`,
+		`            switch item {`,
+		`            case "ba":`,
+		`            case "bb":`,
+		`            case "bc":`,
+		`            case "bd":`,
+		`            }`,
+		`        } else if item == "c" && len(item) > 0 {`,
+		`            while true {`,
+		`                if done {`,
+		`                    break`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`    return nil`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("mixed.go", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+
+	// Only the complex function should be flagged.
+	for _, f := range result.Findings {
+		if strings.Contains(f.Title, "simple") {
+			t.Errorf("simple function should not be flagged, but got: %s", f.Title)
+		}
+	}
+}
+
+func TestComplexityAnalyzer_PythonFunction(t *testing.T) {
+	// Python function with many decision points. Note: Python uses 'or'/'and'
+	// but our regex matches && and || operators. We use if/for/while/elif/except.
+	lines := []string{
+		`def handle_request(request):`,
+		`    if request.method == "GET":`,
+		`        if request.user.is_authenticated:`,
+		`            if request.user.is_admin:`,
+		`                for item in request.items:`,
+		`                    if item.status == "active":`,
+		`                        if item.priority > 5:`,
+		`                            for tag in item.tags:`,
+		`                                if tag == "critical":`,
+		`                                    if item.assigned:`,
+		`                                        while item.retries < 3:`,
+		`                                            if item.can_retry:`,
+		`                                                pass`,
+		`                                            elif item.force:`,
+		`                                                pass`,
+		`                                            elif item.skip:`,
+		`                                                pass`,
+		`                                            except:`,
+		`                                                pass`,
+		`                    elif item.status == "pending":`,
+		`                        for sub in item.children:`,
+		`                            if sub.ready:`,
+		`                                pass`,
+	}
+
+	diff := diffWithAddedLines("views.py", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected complexity finding for complex Python function")
+	}
+}
+
+func TestComplexityAnalyzer_JavaScriptFunction(t *testing.T) {
+	lines := []string{
+		`function processEvents(events) {`,
+		`    if (!events) return;`,
+		`    for (const event of events) {`,
+		`        if (event.type === "click") {`,
+		`            if (event.target && event.target.id) {`,
+		`                switch (event.target.id) {`,
+		`                case "submit":`,
+		`                case "cancel":`,
+		`                case "reset":`,
+		`                case "delete":`,
+		`                }`,
+		`            }`,
+		`        } else if (event.type === "keydown") {`,
+		`            if (event.key === "Enter" || event.key === "Escape") {`,
+		`                for (let i = 0; i < handlers.length; i++) {`,
+		`                    if (handlers[i].active && handlers[i].matches(event)) {`,
+		`                        break;`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("events.js", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected complexity finding for complex JavaScript function")
+	}
+}
+
+func TestComplexityAnalyzer_CustomThreshold(t *testing.T) {
+	// With threshold=3, even a moderately complex function should be flagged.
+	lines := []string{
+		`func moderate(x int) int {`,
+		`    if x > 0 {`,
+		`        for i := 0; i < x; i++ {`,
+		`            if i%2 == 0 {`,
+		`                if i > 10 {`,
+		`                    return i`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`    return 0`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("math.go", lines...)
+	result, err := NewComplexityAnalyzer(WithComplexityThreshold(3)).Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding with low threshold")
+	}
+}
+
+func TestComplexityAnalyzer_CommentsNotCounted(t *testing.T) {
+	lines := []string{
+		`func example(x int) int {`,
+		`    // if x > 0 then do something`,
+		`    // for each item in the list`,
+		`    // while condition is true`,
+		`    /* case 1: */`,
+		`    # elif this or that`,
+		`    return x`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("example.go", lines...)
+	result, err := NewComplexityAnalyzer(WithComplexityThreshold(1)).Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("comments should not contribute to complexity, got %d findings", len(result.Findings))
+	}
+}
+
+func TestComplexityAnalyzer_SkipsDeletedFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "old.go",
+				Status: interfaces.FileDeleted,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 1, Content: `func complex(x int) { if x > 0 { for i := range x { if i > 0 { if i > 1 { if i > 2 { if i > 3 { if i > 4 { if i > 5 { if i > 6 { if i > 7 { if i > 8 { if i > 9 { } } } } } } } } } } } } }`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for deleted files, got %d", len(result.Findings))
+	}
+}
+
+func TestComplexityAnalyzer_EmptyDiff(t *testing.T) {
+	diff := &interfaces.Diff{}
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for empty diff, got %d", len(result.Findings))
+	}
+}
+
+func TestComplexityAnalyzer_ContextCancellation(t *testing.T) {
+	ctx, cancel := context.WithCancel(context.Background())
+	cancel()
+
+	diff := diffWithAddedLines("main.go", `func main() {}`)
+	_, err := NewComplexityAnalyzer().Analyze(ctx, diff)
+	if err == nil {
+		t.Fatal("expected error on cancelled context")
+	}
+}
+
+func TestCountComplexity(t *testing.T) {
+	tests := []struct {
+		name  string
+		lines []string
+		want  int
+	}{
+		{
+			name:  "empty function",
+			lines: []string{`func foo() {`, `}`},
+			want:  1, // base complexity
+		},
+		{
+			name:  "single if",
+			lines: []string{`if x > 0 {`, `}`},
+			want:  2, // 1 base + 1 if
+		},
+		{
+			name:  "if with logical operators",
+			lines: []string{`if x > 0 && y < 10 || z == 5 {`},
+			want:  4, // 1 base + 1 if + 1 && + 1 ||
+		},
+		{
+			name:  "for loop",
+			lines: []string{`for i := 0; i < n; i++ {`},
+			want:  2, // 1 base + 1 for
+		},
+		{
+			name:  "switch with cases",
+			lines: []string{`switch x {`, `case 1:`, `case 2:`, `case 3:`},
+			want:  4, // 1 base + 3 case
+		},
+		{
+			name:  "ternary",
+			lines: []string{`result := x > 0 ? "positive" : "negative"`},
+			want:  2, // 1 base + 1 ternary
+		},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			got := countComplexity(tt.lines)
+			if got != tt.want {
+				t.Errorf("countComplexity() = %d, want %d", got, tt.want)
+			}
+		})
+	}
+}
+
+func TestExtractFuncName(t *testing.T) {
+	tests := []struct {
+		name     string
+		line     string
+		wantName string
+		wantOK   bool
+	}{
+		{"go func", `func processData(input string) error {`, "processData", true},
+		{"go method", `func (s *Server) Start(ctx context.Context) error {`, "Start", true},
+		{"js function", `function handleClick(event) {`, "handleClick", true},
+		{"async function", `async function fetchData() {`, "fetchData", true},
+		{"python def", `def calculate_total(items):`, "calculate_total", true},
+		{"rust fn", `fn process_data(input: &str) -> Result<()> {`, "process_data", true},
+		{"pub rust fn", `pub fn new() -> Self {`, "new", true},
+		{"java method", `    public static void processData(String input) {`, "processData", true},
+		{"not a function", `    x := 42`, "", false},
+		{"variable", `    name := "hello"`, "", false},
+		{"if statement", `    if len(input) == 0 {`, "", false},
+		{"for loop", `    for _, item := range input {`, "", false},
+		{"while loop", `    while true {`, "", false},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			name, ok := extractFuncName(tt.line)
+			if ok != tt.wantOK {
+				t.Errorf("extractFuncName(%q) ok = %v, want %v", tt.line, ok, tt.wantOK)
+			}
+			if ok && name != tt.wantName {
+				t.Errorf("extractFuncName(%q) name = %q, want %q", tt.line, name, tt.wantName)
+			}
+		})
+	}
+}
+
+func TestComplexityAnalyzer_TestFile_HigherThreshold(t *testing.T) {
+	// Build a function with complexity ~18 (above default 15, below test threshold 25).
+	lines := []string{
+		`func TestProcessData_ComplexFixtures(t *testing.T) {`,
+		`    if len(input) == 0 {`,
+		`        return`,
+		`    }`,
+		`    for _, item := range input {`,
+		`        if item == "" {`,
+		`            continue`,
+		`        }`,
+		`        if strings.HasPrefix(item, "a") {`,
+		`            if len(item) > 5 {`,
+		`                for _, c := range item {`,
+		`                    if c == 'x' || c == 'y' {`,
+		`                        break`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        } else if strings.HasPrefix(item, "b") {`,
+		`            switch item {`,
+		`            case "ba":`,
+		`            case "bb":`,
+		`            case "bc":`,
+		`            case "bd":`,
+		`            }`,
+		`        } else if item == "c" && len(item) > 0 {`,
+		`            if done {`,
+		`                break`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`}`,
+	}
+
+	testPaths := []string{
+		"pkg/analyzer/secrets_test.go",
+		"src/utils.test.js",
+		"tests/test_handler.py",
+		"src/utils.spec.ts",
+	}
+
+	for _, path := range testPaths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path, lines...)
+			result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) != 0 {
+				t.Fatalf("test file %q should have boosted threshold, got %d findings: %v",
+					path, len(result.Findings), result.Findings[0].Title)
+			}
+		})
+	}
+}
+
+func TestComplexityAnalyzer_TestFile_StillFlagsExtremeComplexity(t *testing.T) {
+	// Build a function with complexity well above the boosted threshold (25).
+	// Need > 25 decision points + base 1 = 26+ total.
+	lines := []string{
+		`func TestMega_ExtremeComplexity(t *testing.T) {`,
+		`    if x > 0 {`,
+		`        if x > 1 {`,
+		`            if x > 2 {`,
+		`                if x > 3 {`,
+		`                    if x > 4 {`,
+		`                        if x > 5 {`,
+		`                            if x > 6 {`,
+		`                                if x > 7 {`,
+		`                                    for i := 0; i < x; i++ {`,
+		`                                        if i%2 == 0 || i%3 == 0 {`,
+		`                                            switch i {`,
+		`                                            case 1:`,
+		`                                            case 2:`,
+		`                                            case 3:`,
+		`                                            case 4:`,
+		`                                            case 5:`,
+		`                                            }`,
+		`                                        }`,
+		`                                        while x > 0 && i < 100 {`,
+		`                                            if done || abort {`,
+		`                                            }`,
+		`                                        }`,
+		`                                        for j := 0; j < i; j++ {`,
+		`                                            if j > 0 && j < 50 {`,
+		`                                            }`,
+		`                                        }`,
+		`                                        if extra || flag {`,
+		`                                        }`,
+		`                                    }`,
+		`                                }`,
+		`                            }`,
+		`                        }`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("mega_test.go", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("extremely complex test function should still be flagged")
+	}
+}
+
+func TestComplexityAnalyzer_NonTestFile_OriginalThreshold(t *testing.T) {
+	// Ensure non-test files still use the original threshold.
+	lines := []string{
+		`func processData(input []string) error {`,
+		`    if len(input) == 0 {`,
+		`        return nil`,
+		`    }`,
+		`    for _, item := range input {`,
+		`        if item == "" {`,
+		`            continue`,
+		`        }`,
+		`        if strings.HasPrefix(item, "a") {`,
+		`            if len(item) > 5 {`,
+		`                for _, c := range item {`,
+		`                    if c == 'x' || c == 'y' {`,
+		`                        break`,
+		`                    }`,
+		`                }`,
+		`            }`,
+		`        } else if strings.HasPrefix(item, "b") {`,
+		`            switch item {`,
+		`            case "ba":`,
+		`            case "bb":`,
+		`            case "bc":`,
+		`            case "bd":`,
+		`            }`,
+		`        } else if item == "c" && len(item) > 0 {`,
+		`            while true {`,
+		`                if done {`,
+		`                    break`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`    return nil`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("processor.go", lines...)
+	result, err := NewComplexityAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("non-test file should use original threshold and flag this function")
+	}
+}
+
+func TestComplexityAnalyzer_MetadataIncluded(t *testing.T) {
+	// Use a low threshold to guarantee a finding for metadata inspection.
+	lines := []string{
+		`func handler(x int) int {`,
+		`    if x > 0 {`,
+		`        if x > 1 {`,
+		`            for i := 0; i < x; i++ {`,
+		`                if i > 10 {`,
+		`                    return i`,
+		`                }`,
+		`            }`,
+		`        }`,
+		`    }`,
+		`    return 0`,
+		`}`,
+	}
+
+	diff := diffWithAddedLines("processor.go", lines...)
+	result, err := NewComplexityAnalyzer(WithComplexityThreshold(3)).Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected findings")
+	}
+
+	f := result.Findings[0]
+	if f.Metadata == nil {
+		t.Fatal("expected metadata on complexity finding")
+	}
+	if _, ok := f.Metadata["complexity"]; !ok {
+		t.Error("expected 'complexity' key in metadata")
+	}
+	if _, ok := f.Metadata["threshold"]; !ok {
+		t.Error("expected 'threshold' key in metadata")
+	}
+}
diff --git a/pkg/analyzer/coverage.go b/pkg/analyzer/coverage.go
new file mode 100644
index 0000000..40131a7
--- /dev/null
+++ b/pkg/analyzer/coverage.go
@@ -0,0 +1,245 @@
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"path/filepath"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// testFileMapping defines how source files map to test files per language.
+type testFileMapping struct {
+	// sourceExts are extensions that identify source files for this language.
+	sourceExts []string
+	// isTestFile returns true if the given path is a test file for this language.
+	isTestFile func(path string) bool
+	// testPatterns returns possible test file paths for a given source file path.
+	testPatterns func(path string) []string
+}
+
+// Language-aware test file mappings.
+var testFileMappings = []testFileMapping{
+	{
+		// Go: foo.go -> foo_test.go
+		sourceExts: []string{".go"},
+		isTestFile: func(path string) bool {
+			return strings.HasSuffix(path, "_test.go")
+		},
+		testPatterns: func(path string) []string {
+			base := strings.TrimSuffix(path, ".go")
+			return []string{base + "_test.go"}
+		},
+	},
+	{
+		// Python: foo.py -> test_foo.py, foo_test.py, tests/test_foo.py
+		sourceExts: []string{".py"},
+		isTestFile: func(path string) bool {
+			base := filepath.Base(path)
+			return strings.HasPrefix(base, "test_") || strings.HasSuffix(base, "_test.py")
+		},
+		testPatterns: func(path string) []string {
+			dir := filepath.Dir(path)
+			base := filepath.Base(path)
+			name := strings.TrimSuffix(base, ".py")
+			return []string{
+				filepath.Join(dir, "test_"+base),
+				filepath.Join(dir, name+"_test.py"),
+				filepath.Join(dir, "tests", "test_"+base),
+				filepath.Join("tests", "test_"+base),
+			}
+		},
+	},
+	{
+		// JavaScript: foo.js -> foo.test.js, foo.spec.js
+		sourceExts: []string{".js", ".jsx"},
+		isTestFile: func(path string) bool {
+			return strings.HasSuffix(path, ".test.js") ||
+				strings.HasSuffix(path, ".spec.js") ||
+				strings.HasSuffix(path, ".test.jsx") ||
+				strings.HasSuffix(path, ".spec.jsx")
+		},
+		testPatterns: func(path string) []string {
+			ext := filepath.Ext(path)
+			base := strings.TrimSuffix(path, ext)
+			return []string{
+				base + ".test" + ext,
+				base + ".spec" + ext,
+			}
+		},
+	},
+	{
+		// TypeScript: foo.ts -> foo.test.ts, foo.spec.ts
+		sourceExts: []string{".ts", ".tsx"},
+		isTestFile: func(path string) bool {
+			return strings.HasSuffix(path, ".test.ts") ||
+				strings.HasSuffix(path, ".spec.ts") ||
+				strings.HasSuffix(path, ".test.tsx") ||
+				strings.HasSuffix(path, ".spec.tsx")
+		},
+		testPatterns: func(path string) []string {
+			ext := filepath.Ext(path)
+			base := strings.TrimSuffix(path, ext)
+			return []string{
+				base + ".test" + ext,
+				base + ".spec" + ext,
+			}
+		},
+	},
+	{
+		// Ruby: foo.rb -> foo_test.rb, test_foo.rb, spec/foo_spec.rb
+		sourceExts: []string{".rb"},
+		isTestFile: func(path string) bool {
+			base := filepath.Base(path)
+			return strings.HasSuffix(base, "_test.rb") ||
+				strings.HasSuffix(base, "_spec.rb") ||
+				strings.HasPrefix(base, "test_")
+		},
+		testPatterns: func(path string) []string {
+			dir := filepath.Dir(path)
+			base := filepath.Base(path)
+			name := strings.TrimSuffix(base, ".rb")
+			return []string{
+				filepath.Join(dir, name+"_test.rb"),
+				filepath.Join(dir, name+"_spec.rb"),
+				filepath.Join("spec", name+"_spec.rb"),
+			}
+		},
+	},
+	{
+		// Rust: foo.rs -> tests within the same file (mod tests), or tests/foo.rs
+		sourceExts: []string{".rs"},
+		isTestFile: func(path string) bool {
+			return strings.Contains(path, "/tests/") || strings.HasPrefix(path, "tests/")
+		},
+		testPatterns: func(path string) []string {
+			base := filepath.Base(path)
+			return []string{
+				filepath.Join("tests", base),
+			}
+		},
+	},
+}
+
+// CoverageAnalyzer checks whether new or modified source files have
+// corresponding test files in the diff.
+type CoverageAnalyzer struct{}
+
+// NewCoverageAnalyzer creates a new test coverage heuristic analyzer.
+func NewCoverageAnalyzer() *CoverageAnalyzer {
+	return &CoverageAnalyzer{}
+}
+
+// Name returns the analyzer identifier.
+func (c *CoverageAnalyzer) Name() string {
+	return "coverage"
+}
+
+// Analyze checks that source files in the diff have corresponding test changes.
+func (c *CoverageAnalyzer) Analyze(ctx context.Context, diff *interfaces.Diff) (*interfaces.AnalysisResult, error) {
+	result := &interfaces.AnalysisResult{
+		AnalyzerName: c.Name(),
+	}
+
+	// Build a set of all file paths in the diff for fast lookup.
+	diffPaths := make(map[string]bool, len(diff.Files))
+	for _, file := range diff.Files {
+		diffPaths[file.Path] = true
+	}
+
+	for i := range diff.Files {
+		if ctx.Err() != nil {
+			return result, ctx.Err()
+		}
+
+		file := &diff.Files[i]
+		if file.IsBinary || file.Status == interfaces.FileDeleted {
+			continue
+		}
+
+		// Skip if the file itself is a test file.
+		if isTestFileForCoverage(file.Path) {
+			continue
+		}
+
+		// Identify the language mapping for this file.
+		mapping := findMapping(file.Path)
+		if mapping == nil {
+			continue
+		}
+
+		// Check if any expected test file is in the diff.
+		testPatterns := mapping.testPatterns(file.Path)
+		hasTest := false
+		for _, pattern := range testPatterns {
+			if diffPaths[pattern] {
+				hasTest = true
+				break
+			}
+		}
+
+		if !hasTest {
+			severity := interfaces.SeverityLow
+			title := "Modified file has no test changes"
+			if file.Status == interfaces.FileAdded {
+				severity = interfaces.SeverityMedium
+				title = "New file has no corresponding test file"
+			}
+
+			result.Findings = append(result.Findings, interfaces.Finding{
+				ID:        fmt.Sprintf("COV-%s-%s", strings.ToUpper(string(file.Status)), sanitizePath(file.Path)),
+				Category:  interfaces.CategoryCoverage,
+				Severity:  severity,
+				File:      file.Path,
+				StartLine: 0,
+				EndLine:   0,
+				Title:     title,
+				Description: fmt.Sprintf(
+					"File %s was %s but no corresponding test file was found in the diff. Expected one of: %s",
+					file.Path, file.Status, strings.Join(testPatterns, ", "),
+				),
+				Suggestion: "Add tests for the new or modified code to maintain test coverage.",
+				Source:     "coverage",
+				Confidence: 0.70,
+			})
+		}
+	}
+
+	return result, nil
+}
+
+// findMapping returns the test file mapping for a source file, or nil if no mapping matches.
+func findMapping(path string) *testFileMapping {
+	for i := range testFileMappings {
+		mapping := &testFileMappings[i]
+		// Skip if this file is already a test file.
+		if mapping.isTestFile(path) {
+			return nil
+		}
+		for _, ext := range mapping.sourceExts {
+			if strings.HasSuffix(path, ext) {
+				return mapping
+			}
+		}
+	}
+	return nil
+}
+
+// isTestFileForCoverage checks if a file is any kind of test file.
+func isTestFileForCoverage(path string) bool {
+	for _, mapping := range testFileMappings {
+		if mapping.isTestFile(path) {
+			return true
+		}
+	}
+	return false
+}
+
+// sanitizePath converts a file path to a short ID-safe string.
+func sanitizePath(path string) string {
+	base := filepath.Base(path)
+	base = strings.TrimSuffix(base, filepath.Ext(base))
+	r := strings.NewReplacer("/", "-", "\\", "-", " ", "-", ".", "-")
+	return strings.ToUpper(r.Replace(base))
+}
diff --git a/pkg/analyzer/coverage_test.go b/pkg/analyzer/coverage_test.go
new file mode 100644
index 0000000..bf134e4
--- /dev/null
+++ b/pkg/analyzer/coverage_test.go
@@ -0,0 +1,328 @@
+package analyzer
+
+import (
+	"context"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+func TestCoverageAnalyzer_Name(t *testing.T) {
+	a := NewCoverageAnalyzer()
+	if a.Name() != "coverage" {
+		t.Errorf("expected name %q, got %q", "coverage", a.Name())
+	}
+}
+
+func TestCoverageAnalyzer_ImplementsInterface(t *testing.T) {
+	var _ Analyzer = NewCoverageAnalyzer()
+}
+
+func TestCoverageAnalyzer_GoFile_WithTest_NoFinding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/handler/handler.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package handler"}}}}},
+			{Path: "pkg/handler/handler_test.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package handler"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings when test file exists, got %d: %v", len(result.Findings), findingIDs(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_GoFile_WithoutTest_Finding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/handler/handler.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package handler"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding when Go file has no test")
+	}
+	f := result.Findings[0]
+	if f.Category != interfaces.CategoryCoverage {
+		t.Errorf("expected category %q, got %q", interfaces.CategoryCoverage, f.Category)
+	}
+}
+
+func TestCoverageAnalyzer_NewFile_MediumSeverity(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/service/service.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package service"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new file without tests")
+	}
+	if result.Findings[0].Severity != interfaces.SeverityMedium {
+		t.Errorf("expected MEDIUM severity for new file, got %q", result.Findings[0].Severity)
+	}
+}
+
+func TestCoverageAnalyzer_ModifiedFile_LowSeverity(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/service/service.go", Status: interfaces.FileModified, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "x := 1"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for modified file without test changes")
+	}
+	if result.Findings[0].Severity != interfaces.SeverityLow {
+		t.Errorf("expected LOW severity for modified file, got %q", result.Findings[0].Severity)
+	}
+}
+
+func TestCoverageAnalyzer_PythonFile_WithTest_NoFinding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "app/services/user.py", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "class User:"}}}}},
+			{Path: "app/services/test_user.py", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "class TestUser:"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings when Python test file exists, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_PythonFile_WithoutTest_Finding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "app/services/user.py", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "class User:"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding when Python file has no test")
+	}
+}
+
+func TestCoverageAnalyzer_JSFile_WithTestJS_NoFinding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "src/utils/format.js", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "export function format() {}"}}}}},
+			{Path: "src/utils/format.test.js", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "test('format', () => {})"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings when JS test file exists, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_JSFile_WithSpecJS_NoFinding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "src/utils/format.js", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "export function format() {}"}}}}},
+			{Path: "src/utils/format.spec.js", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "describe('format', () => {})"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings when JS spec file exists, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_TSFile_WithoutTest_Finding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "src/components/Button.tsx", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "export const Button = () => {}"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding when TSX file has no test")
+	}
+}
+
+func TestCoverageAnalyzer_TSFile_WithSpecTS_NoFinding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "src/services/api.ts", Status: interfaces.FileModified, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "export async function fetch() {}"}}}}},
+			{Path: "src/services/api.spec.ts", Status: interfaces.FileModified, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "test('fetch', () => {})"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings when TS spec file exists, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_TestFileOnly_NoFinding(t *testing.T) {
+	// If only test files are modified, no coverage finding should be raised.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/handler/handler_test.go", Status: interfaces.FileModified, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "func TestNew(t *testing.T) {}"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for test-only changes, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_SkipsDeletedFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/old.go", Status: interfaces.FileDeleted},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for deleted files, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_SkipsBinaryFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "image.png", Status: interfaces.FileAdded, IsBinary: true},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for binary files, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_UnknownExtension_NoFinding(t *testing.T) {
+	// Files with unrecognized extensions should be skipped.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "README.md", Status: interfaces.FileModified, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "# Readme"}}}}},
+			{Path: "config.yaml", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "key: value"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for unknown file types, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_EmptyDiff(t *testing.T) {
+	diff := &interfaces.Diff{}
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for empty diff, got %d", len(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_ContextCancellation(t *testing.T) {
+	ctx, cancel := context.WithCancel(context.Background())
+	cancel()
+
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "pkg/service.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package pkg"}}}}},
+		},
+	}
+
+	_, err := NewCoverageAnalyzer().Analyze(ctx, diff)
+	if err == nil {
+		t.Fatal("expected error on cancelled context")
+	}
+}
+
+func TestCoverageAnalyzer_MultipleFiles_MixedResults(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			// Go file with test — no finding.
+			{Path: "pkg/a/a.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package a"}}}}},
+			{Path: "pkg/a/a_test.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package a"}}}}},
+			// Go file without test — finding.
+			{Path: "pkg/b/b.go", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "package b"}}}}},
+			// JS file without test — finding.
+			{Path: "src/c.js", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "export const c = 1"}}}}},
+			// Markdown — no finding (unknown type).
+			{Path: "docs/README.md", Status: interfaces.FileModified, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "# Docs"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 2 {
+		t.Fatalf("expected 2 findings (b.go and c.js), got %d: %v", len(result.Findings), findingIDs(result.Findings))
+	}
+}
+
+func TestCoverageAnalyzer_RubyFile_WithoutTest_Finding(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{Path: "app/models/user.rb", Status: interfaces.FileAdded, Hunks: []interfaces.Hunk{{AddedLines: []interfaces.Line{{Number: 1, Content: "class User"}}}}},
+		},
+	}
+
+	result, err := NewCoverageAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for Ruby file without test")
+	}
+}
diff --git a/pkg/analyzer/engine.go b/pkg/analyzer/engine.go
new file mode 100644
index 0000000..0c8e1d2
--- /dev/null
+++ b/pkg/analyzer/engine.go
@@ -0,0 +1,99 @@
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"log/slog"
+	"sync"
+	"time"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// Engine orchestrates running all enabled analyzers against a diff.
+type Engine struct {
+	registry *Registry
+}
+
+// NewEngine creates an analysis engine backed by the given registry.
+func NewEngine(registry *Registry) *Engine {
+	return &Engine{registry: registry}
+}
+
+// Run executes all enabled analyzers against the diff in parallel.
+// A failing analyzer does not stop other analyzers from running.
+// Returns results for every enabled analyzer, including those that errored.
+// Respects context cancellation.
+func (e *Engine) Run(ctx context.Context, diff *interfaces.Diff) ([]*interfaces.AnalysisResult, error) {
+	if diff == nil {
+		return nil, fmt.Errorf("analyzer: diff must not be nil")
+	}
+
+	analyzers := e.registry.EnabledAnalyzers()
+	if len(analyzers) == 0 {
+		slog.Info("no enabled analyzers to run")
+		return nil, nil
+	}
+
+	slog.Info("starting analysis", "analyzer_count", len(analyzers))
+
+	var (
+		mu      sync.Mutex
+		wg      sync.WaitGroup
+		results = make([]*interfaces.AnalysisResult, 0, len(analyzers))
+	)
+
+	for _, a := range analyzers {
+		wg.Add(1)
+		go func(a Analyzer) {
+			defer wg.Done()
+
+			// Check for context cancellation before starting.
+			if ctx.Err() != nil {
+				return
+			}
+
+			name := a.Name()
+			start := time.Now()
+			slog.Info("running analyzer", "name", name)
+
+			result, err := a.Analyze(ctx, diff)
+			elapsed := time.Since(start)
+
+			if err != nil {
+				slog.Error("analyzer failed", "name", name, "error", err, "duration", elapsed)
+				result = &interfaces.AnalysisResult{
+					AnalyzerName: name,
+					Duration:     elapsed,
+					Error:        fmt.Errorf("analyzer %s: %w", name, err),
+				}
+			} else {
+				result.Duration = elapsed
+				slog.Info("analyzer complete", "name", name, "findings", len(result.Findings), "duration", elapsed)
+			}
+
+			mu.Lock()
+			results = append(results, result)
+			mu.Unlock()
+		}(a)
+	}
+
+	// Wait for all analyzers or context cancellation.
+	done := make(chan struct{})
+	go func() {
+		wg.Wait()
+		close(done)
+	}()
+
+	select {
+	case <-done:
+		// All analyzers finished.
+	case <-ctx.Done():
+		slog.Warn("analysis cancelled", "error", ctx.Err())
+		// Wait for in-flight goroutines to notice cancellation and finish.
+		<-done
+		return results, ctx.Err()
+	}
+
+	return results, nil
+}
diff --git a/pkg/analyzer/imports.go b/pkg/analyzer/imports.go
new file mode 100644
index 0000000..b7ad9ce
--- /dev/null
+++ b/pkg/analyzer/imports.go
@@ -0,0 +1,369 @@
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"regexp"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// Dependency manifest filenames to watch for.
+// Derived/lock files (go.sum, package-lock.json, etc.) are excluded to avoid
+// duplicate noise when the primary manifest is also in the diff.
+var dependencyManifests = map[string]string{
+	"go.mod":           "Go",
+	"package.json":     "JavaScript/TypeScript",
+	"package-lock.json": "JavaScript/TypeScript",
+	"yarn.lock":        "JavaScript/TypeScript",
+	"pnpm-lock.yaml":   "JavaScript/TypeScript",
+	"requirements.txt": "Python",
+	"Pipfile":          "Python",
+	"Pipfile.lock":     "Python",
+	"pyproject.toml":   "Python",
+	"poetry.lock":      "Python",
+	"Cargo.toml":       "Rust",
+	"Cargo.lock":       "Rust",
+	"pom.xml":          "Java",
+	"build.gradle":     "Java",
+	"build.gradle.kts": "Kotlin",
+	"Gemfile":          "Ruby",
+	"Gemfile.lock":     "Ruby",
+	"composer.json":    "PHP",
+	"composer.lock":    "PHP",
+}
+
+// derivedManifests maps lock/derived files to their primary manifest.
+// A derived file is skipped when its primary manifest is also present in the diff.
+// go.sum is always skipped (it is purely derived from go.mod).
+var derivedManifests = map[string]string{
+	"go.sum":            "", // always skip
+	"package-lock.json": "package.json",
+	"yarn.lock":         "package.json",
+	"pnpm-lock.yaml":    "package.json",
+	"Pipfile.lock":      "Pipfile",
+	"poetry.lock":       "pyproject.toml",
+	"Cargo.lock":        "Cargo.toml",
+	"Gemfile.lock":      "Gemfile",
+	"composer.lock":     "composer.json",
+}
+
+// Patterns to detect new dependency additions in various manifest files.
+var newDepPatterns = []struct {
+	files   []string
+	pattern *regexp.Regexp
+	desc    string
+}{
+	{
+		files:   []string{"go.mod"},
+		pattern: regexp.MustCompile(`^\s*require\s+\S+|^\s+\S+\s+v[\d.]+`),
+		desc:    "Go module dependency",
+	},
+	{
+		files:   []string{"package.json"},
+		pattern: regexp.MustCompile(`^\s*"[^"]+"\s*:\s*"[\^~>=<]*\d`),
+		desc:    "npm package",
+	},
+	{
+		files:   []string{"requirements.txt"},
+		pattern: regexp.MustCompile(`^\s*[a-zA-Z][a-zA-Z0-9._-]*\s*[><=!~]+`),
+		desc:    "Python package",
+	},
+	{
+		files:   []string{"Cargo.toml"},
+		pattern: regexp.MustCompile(`^\s*[a-zA-Z][a-zA-Z0-9_-]*\s*=\s*(?:"[\d.]+"|{)`),
+		desc:    "Rust crate",
+	},
+	{
+		files:   []string{"Gemfile"},
+		pattern: regexp.MustCompile(`^\s*gem\s+['"]`),
+		desc:    "Ruby gem",
+	},
+	{
+		files:   []string{"pom.xml"},
+		pattern: regexp.MustCompile(`<dependency>`),
+		desc:    "Maven dependency",
+	},
+}
+
+// Pattern to detect semver major version in added lines.
+var majorVersionPattern = regexp.MustCompile(`v?(\d+)\.\d+\.\d+`)
+
+// ImportsAnalyzer detects changes to dependency manifest files.
+type ImportsAnalyzer struct{}
+
+// NewImportsAnalyzer creates a new dependency change analyzer.
+func NewImportsAnalyzer() *ImportsAnalyzer {
+	return &ImportsAnalyzer{}
+}
+
+// Name returns the analyzer identifier.
+func (im *ImportsAnalyzer) Name() string {
+	return "imports"
+}
+
+// Analyze scans the diff for dependency manifest changes.
+func (im *ImportsAnalyzer) Analyze(ctx context.Context, diff *interfaces.Diff) (*interfaces.AnalysisResult, error) {
+	result := &interfaces.AnalysisResult{
+		AnalyzerName: im.Name(),
+	}
+
+	// Collect all basenames present in the diff so we can deduplicate
+	// derived manifests when their primary is also present.
+	presentFiles := make(map[string]bool)
+	for i := range diff.Files {
+		presentFiles[fileBaseName(diff.Files[i].Path)] = true
+	}
+
+	for i := range diff.Files {
+		if ctx.Err() != nil {
+			return result, ctx.Err()
+		}
+
+		file := &diff.Files[i]
+		if file.IsBinary {
+			continue
+		}
+
+		filename := fileBaseName(file.Path)
+
+		// Skip derived/lock files to avoid duplicate noise.
+		if im.shouldSkipDerived(filename, presentFiles) {
+			continue
+		}
+
+		lang, isManifest := dependencyManifests[filename]
+		if !isManifest {
+			continue
+		}
+
+		findings := im.analyzeManifest(file, filename, lang)
+		result.Findings = append(result.Findings, findings...)
+	}
+
+	return result, nil
+}
+
+// shouldSkipDerived checks if a manifest file is a derived/lock file that
+// should be skipped. go.sum is always skipped; other lock files are skipped
+// when their primary manifest is also present in the diff.
+func (im *ImportsAnalyzer) shouldSkipDerived(filename string, presentFiles map[string]bool) bool {
+	primary, isDerived := derivedManifests[filename]
+	if !isDerived {
+		return false
+	}
+	// Empty primary means always skip (e.g., go.sum).
+	if primary == "" {
+		return true
+	}
+	return presentFiles[primary]
+}
+
+// analyzeManifest inspects a dependency manifest file for changes.
+func (im *ImportsAnalyzer) analyzeManifest(file *interfaces.FileDiff, filename, lang string) []interfaces.Finding {
+	var findings []interfaces.Finding
+
+	// Collect added and removed lines.
+	var addedLines []interfaces.Line
+	var removedLines []interfaces.Line
+	for j := range file.Hunks {
+		addedLines = append(addedLines, file.Hunks[j].AddedLines...)
+		removedLines = append(removedLines, file.Hunks[j].RemovedLines...)
+	}
+
+	// Check for new dependencies (added lines matching dep patterns).
+	for _, line := range addedLines {
+		if isDepLine(filename, line.Content) {
+			depName := extractDepName(line.Content)
+			// Check if this is a major version bump (same dep in removed lines with different major).
+			if majorBump, oldMajor, newMajor := isMajorVersionBump(line.Content, removedLines, depName); majorBump {
+				findings = append(findings, interfaces.Finding{
+					ID:        fmt.Sprintf("IMP-MAJOR-%d", line.Number),
+					Category:  interfaces.CategoryImport,
+					Severity:  interfaces.SeverityMedium,
+					File:      file.Path,
+					StartLine: line.Number,
+					EndLine:   line.Number,
+					Title:     fmt.Sprintf("Major version bump in %s dependency", lang),
+					Description: fmt.Sprintf(
+						"Dependency %q was upgraded from major version %s to %s. Major version bumps may include breaking changes.",
+						depName, oldMajor, newMajor,
+					),
+					Suggestion: "Review the dependency changelog for breaking changes and update consuming code accordingly.",
+					Source:     "imports",
+					Confidence: 0.80,
+					Metadata: map[string]any{
+						"language":    lang,
+						"dependency":  depName,
+						"old_major":   oldMajor,
+						"new_major":   newMajor,
+						"manifest":    filename,
+					},
+				})
+			} else {
+				findings = append(findings, interfaces.Finding{
+					ID:        fmt.Sprintf("IMP-NEW-%d", line.Number),
+					Category:  interfaces.CategoryImport,
+					Severity:  interfaces.SeverityLow,
+					File:      file.Path,
+					StartLine: line.Number,
+					EndLine:   line.Number,
+					Title:     fmt.Sprintf("New %s dependency added", lang),
+					Description: fmt.Sprintf(
+						"A new dependency was added to %s: %s. New dependencies increase the supply chain attack surface.",
+						filename, strings.TrimSpace(line.Content),
+					),
+					Suggestion: "Verify the dependency is from a trusted source, is actively maintained, and has no known vulnerabilities.",
+					Source:     "imports",
+					Confidence: 0.70,
+					Metadata: map[string]any{
+						"language":   lang,
+						"dependency": depName,
+						"manifest":   filename,
+					},
+				})
+			}
+		}
+	}
+
+	// Check for removed dependencies.
+	for _, line := range removedLines {
+		if isDepLine(filename, line.Content) {
+			depName := extractDepName(line.Content)
+			// Only flag if the dep wasn't re-added (i.e., not a version change).
+			if !depExistsInLines(depName, addedLines) {
+				findings = append(findings, interfaces.Finding{
+					ID:        fmt.Sprintf("IMP-REMOVED-%d", line.Number),
+					Category:  interfaces.CategoryImport,
+					Severity:  interfaces.SeverityInfo,
+					File:      file.Path,
+					StartLine: line.Number,
+					EndLine:   line.Number,
+					Title:     fmt.Sprintf("%s dependency removed", lang),
+					Description: fmt.Sprintf(
+						"Dependency %q was removed from %s.",
+						depName, filename,
+					),
+					Suggestion: "Ensure no code still references the removed dependency.",
+					Source:     "imports",
+					Confidence: 0.75,
+				})
+			}
+		}
+	}
+
+	return findings
+}
+
+// isDepLine checks if a line looks like a dependency declaration for the given manifest.
+func isDepLine(filename, content string) bool {
+	trimmed := strings.TrimSpace(content)
+	if trimmed == "" || strings.HasPrefix(trimmed, "//") || strings.HasPrefix(trimmed, "#") {
+		return false
+	}
+
+	for _, p := range newDepPatterns {
+		for _, f := range p.files {
+			if f == filename && p.pattern.MatchString(content) {
+				return true
+			}
+		}
+	}
+
+	// Generic fallback: if the manifest is known but no specific pattern, check for version-like strings.
+	if _, ok := dependencyManifests[filename]; ok {
+		return majorVersionPattern.MatchString(content)
+	}
+
+	return false
+}
+
+// extractDepName tries to extract the dependency name from a manifest line.
+func extractDepName(content string) string {
+	trimmed := strings.TrimSpace(content)
+
+	// Go module: github.com/foo/bar v1.2.3
+	goModRe := regexp.MustCompile(`^\s*(?:require\s+)?(\S+)\s+v`)
+	if m := goModRe.FindStringSubmatch(trimmed); len(m) > 1 {
+		return m[1]
+	}
+
+	// package.json: "name": "^1.2.3"
+	npmRe := regexp.MustCompile(`^\s*"([^"]+)"\s*:`)
+	if m := npmRe.FindStringSubmatch(trimmed); len(m) > 1 {
+		return m[1]
+	}
+
+	// requirements.txt: name==1.2.3 or name>=1.2.3
+	pipRe := regexp.MustCompile(`^\s*([a-zA-Z][a-zA-Z0-9._-]*)\s*[><=!~]`)
+	if m := pipRe.FindStringSubmatch(trimmed); len(m) > 1 {
+		return m[1]
+	}
+
+	// Gemfile: gem 'name'
+	gemRe := regexp.MustCompile(`gem\s+['"]([^'"]+)['"]`)
+	if m := gemRe.FindStringSubmatch(trimmed); len(m) > 1 {
+		return m[1]
+	}
+
+	// Cargo.toml: name = "1.2.3" or name = {version = ...}
+	cargoRe := regexp.MustCompile(`^\s*([a-zA-Z][a-zA-Z0-9_-]*)\s*=`)
+	if m := cargoRe.FindStringSubmatch(trimmed); len(m) > 1 {
+		return m[1]
+	}
+
+	// Fallback: return the first word-like token.
+	fields := strings.Fields(trimmed)
+	if len(fields) > 0 {
+		return strings.Trim(fields[0], `"'<>/`)
+	}
+
+	return "unknown"
+}
+
+// isMajorVersionBump checks if the added line represents a major version bump
+// compared to a removed line with the same dependency name.
+func isMajorVersionBump(addedContent string, removedLines []interfaces.Line, depName string) (bool, string, string) {
+	newMajor := extractMajorVersion(addedContent)
+	if newMajor == "" {
+		return false, "", ""
+	}
+
+	for _, removed := range removedLines {
+		removedDep := extractDepName(removed.Content)
+		if removedDep == depName {
+			oldMajor := extractMajorVersion(removed.Content)
+			if oldMajor != "" && oldMajor != newMajor {
+				return true, oldMajor, newMajor
+			}
+		}
+	}
+
+	return false, "", ""
+}
+
+// extractMajorVersion extracts the major version number from a line containing a semver.
+func extractMajorVersion(content string) string {
+	m := majorVersionPattern.FindStringSubmatch(content)
+	if len(m) > 1 {
+		return m[1]
+	}
+	return ""
+}
+
+// depExistsInLines checks if a dependency name appears in any of the given lines.
+func depExistsInLines(depName string, lines []interfaces.Line) bool {
+	for _, line := range lines {
+		if strings.Contains(line.Content, depName) {
+			return true
+		}
+	}
+	return false
+}
+
+// fileBaseName extracts the filename from a path (last component).
+func fileBaseName(path string) string {
+	parts := strings.Split(path, "/")
+	return parts[len(parts)-1]
+}
diff --git a/pkg/analyzer/imports_test.go b/pkg/analyzer/imports_test.go
new file mode 100644
index 0000000..b0f33f6
--- /dev/null
+++ b/pkg/analyzer/imports_test.go
@@ -0,0 +1,614 @@
+package analyzer
+
+import (
+	"context"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+func TestImportsAnalyzer_Name(t *testing.T) {
+	a := NewImportsAnalyzer()
+	if a.Name() != "imports" {
+		t.Errorf("expected name %q, got %q", "imports", a.Name())
+	}
+}
+
+func TestImportsAnalyzer_ImplementsInterface(t *testing.T) {
+	var _ Analyzer = NewImportsAnalyzer()
+}
+
+func TestImportsAnalyzer_GoMod_NewDependency(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/stretchr/testify v1.9.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new Go dependency")
+	}
+	if !hasFindingWithID(result.Findings, "IMP-NEW") {
+		t.Fatalf("expected IMP-NEW finding, got: %v", findingIDs(result.Findings))
+	}
+	assertAllFindingsHaveSeverity(t, result.Findings, "IMP-NEW", interfaces.SeverityLow)
+}
+
+func TestImportsAnalyzer_GoMod_MajorVersionBump(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						RemovedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/go-chi/chi v4.1.2`},
+						},
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/go-chi/chi v5.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if !hasFindingWithID(result.Findings, "IMP-MAJOR") {
+		t.Fatalf("expected IMP-MAJOR finding for major version bump, got: %v", findingIDs(result.Findings))
+	}
+	assertAllFindingsHaveSeverity(t, result.Findings, "IMP-MAJOR", interfaces.SeverityMedium)
+}
+
+func TestImportsAnalyzer_GoMod_DependencyRemoved(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						RemovedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/old/dep v1.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if !hasFindingWithID(result.Findings, "IMP-REMOVED") {
+		t.Fatalf("expected IMP-REMOVED finding, got: %v", findingIDs(result.Findings))
+	}
+	assertAllFindingsHaveSeverity(t, result.Findings, "IMP-REMOVED", interfaces.SeverityInfo)
+}
+
+func TestImportsAnalyzer_PackageJSON_NewDependency(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "package.json",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 10, Content: `    "express": "^4.18.0"`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new npm dependency")
+	}
+	f := result.Findings[0]
+	if f.Category != interfaces.CategoryImport {
+		t.Errorf("expected category %q, got %q", interfaces.CategoryImport, f.Category)
+	}
+}
+
+func TestImportsAnalyzer_PackageJSON_MajorBump(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "package.json",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						RemovedLines: []interfaces.Line{
+							{Number: 10, Content: `    "react": "^17.0.0"`},
+						},
+						AddedLines: []interfaces.Line{
+							{Number: 10, Content: `    "react": "^18.0.0"`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if !hasFindingWithID(result.Findings, "IMP-MAJOR") {
+		t.Fatalf("expected IMP-MAJOR finding for npm major bump, got: %v", findingIDs(result.Findings))
+	}
+}
+
+func TestImportsAnalyzer_RequirementsTxt_NewDependency(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "requirements.txt",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 3, Content: `flask>=2.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new Python dependency")
+	}
+}
+
+func TestImportsAnalyzer_CargoToml_NewDependency(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "Cargo.toml",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 8, Content: `serde = "1.0"`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new Rust dependency")
+	}
+}
+
+func TestImportsAnalyzer_Gemfile_NewDependency(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "Gemfile",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `gem 'rails', '~> 7.0'`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new Ruby dependency")
+	}
+}
+
+func TestImportsAnalyzer_PomXML_NewDependency(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "pom.xml",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 20, Content: `    <dependency>`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for new Maven dependency")
+	}
+}
+
+func TestImportsAnalyzer_NonManifestFile_NoFindings(t *testing.T) {
+	files := []string{
+		"main.go",
+		"src/index.ts",
+		"README.md",
+		"config.yaml",
+	}
+
+	for _, path := range files {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path, `some content v1.0.0`)
+			result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) != 0 {
+				t.Fatalf("expected no findings for non-manifest file %q, got %d", path, len(result.Findings))
+			}
+		})
+	}
+}
+
+func TestImportsAnalyzer_NestedManifest_Detected(t *testing.T) {
+	// go.mod in a subdirectory should still be detected.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "services/api/go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/new/dep v1.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected finding for nested manifest file")
+	}
+}
+
+func TestImportsAnalyzer_MinorVersionBump_NotMajor(t *testing.T) {
+	// Minor/patch bumps should be flagged as new, not major.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						RemovedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/foo/bar v1.2.0`},
+						},
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/foo/bar v1.3.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if hasFindingWithID(result.Findings, "IMP-MAJOR") {
+		t.Fatal("minor version bump should not produce IMP-MAJOR finding")
+	}
+}
+
+func TestImportsAnalyzer_SkipsGoSum(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.sum",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=`},
+							{Number: 6, Content: `github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for go.sum, got %d: %v", len(result.Findings), findingIDs(result.Findings))
+	}
+}
+
+func TestImportsAnalyzer_SkipsGoSum_EvenWithoutGoMod(t *testing.T) {
+	// go.sum should always be skipped, even if go.mod is not in the diff.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.sum",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `github.com/new/dep v1.0.0 h1:abc123=`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("go.sum should always be skipped, got %d findings", len(result.Findings))
+	}
+}
+
+func TestImportsAnalyzer_SkipsPackageLockJSON_WhenPackageJSONPresent(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "package.json",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 10, Content: `    "express": "^4.18.0"`},
+						},
+					},
+				},
+			},
+			{
+				Path:   "package-lock.json",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 100, Content: `    "express": { "version": "4.18.0" }`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	// Should only have findings from package.json, not package-lock.json.
+	for _, f := range result.Findings {
+		if f.File == "package-lock.json" {
+			t.Fatalf("package-lock.json should be skipped when package.json is present, got finding: %s", f.Title)
+		}
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected findings from package.json")
+	}
+}
+
+func TestImportsAnalyzer_ScansPackageLockJSON_WhenPackageJSONAbsent(t *testing.T) {
+	// If only package-lock.json changed (e.g., npm audit fix), it should be scanned.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "package-lock.json",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 100, Content: `    "lodash": "^4.17.21"`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected findings from package-lock.json when package.json is absent")
+	}
+}
+
+func TestImportsAnalyzer_SkipsBinaryFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:     "go.mod",
+				Status:   interfaces.FileModified,
+				IsBinary: true,
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for binary files, got %d", len(result.Findings))
+	}
+}
+
+func TestImportsAnalyzer_EmptyDiff(t *testing.T) {
+	diff := &interfaces.Diff{}
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for empty diff, got %d", len(result.Findings))
+	}
+}
+
+func TestImportsAnalyzer_ContextCancellation(t *testing.T) {
+	ctx, cancel := context.WithCancel(context.Background())
+	cancel()
+
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/new/dep v1.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	_, err := NewImportsAnalyzer().Analyze(ctx, diff)
+	if err == nil {
+		t.Fatal("expected error on cancelled context")
+	}
+}
+
+func TestImportsAnalyzer_CommentLines_Skipped(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `// github.com/commented/dep v1.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for commented lines, got %d", len(result.Findings))
+	}
+}
+
+func TestImportsAnalyzer_MetadataIncluded(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "go.mod",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 5, Content: `	github.com/new/dep v1.0.0`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewImportsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected findings")
+	}
+	f := result.Findings[0]
+	if f.Metadata == nil {
+		t.Fatal("expected metadata on imports finding")
+	}
+	if _, ok := f.Metadata["language"]; !ok {
+		t.Error("expected 'language' key in metadata")
+	}
+	if _, ok := f.Metadata["manifest"]; !ok {
+		t.Error("expected 'manifest' key in metadata")
+	}
+}
+
+func TestExtractDepName(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+		want string
+	}{
+		{"go module", `	github.com/stretchr/testify v1.9.0`, "github.com/stretchr/testify"},
+		{"npm package", `    "express": "^4.18.0"`, "express"},
+		{"pip package", `flask>=2.0.0`, "flask"},
+		{"ruby gem", `gem 'rails', '~> 7.0'`, "rails"},
+		{"cargo crate", `serde = "1.0"`, "serde"},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			got := extractDepName(tt.line)
+			if got != tt.want {
+				t.Errorf("extractDepName(%q) = %q, want %q", tt.line, got, tt.want)
+			}
+		})
+	}
+}
diff --git a/pkg/analyzer/patterns.go b/pkg/analyzer/patterns.go
new file mode 100644
index 0000000..33dafec
--- /dev/null
+++ b/pkg/analyzer/patterns.go
@@ -0,0 +1,237 @@
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"regexp"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// Anti-pattern detection rules compiled once at init.
+var (
+	// SQL string concatenation: "SELECT " + var, "INSERT " + var, fmt.Sprintf("SELECT ...
+	sqlConcatPatterns = []*regexp.Regexp{
+		regexp.MustCompile(`(?i)["'](?:SELECT|INSERT|UPDATE|DELETE|DROP|ALTER|CREATE)\s.*["']\s*\+`),
+		regexp.MustCompile(`(?i)\+\s*["'](?:\s*(?:SELECT|INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|WHERE|FROM|SET|INTO|VALUES))\b`),
+		regexp.MustCompile(`(?i)fmt\.Sprintf\(\s*["'](?:SELECT|INSERT|UPDATE|DELETE|DROP|ALTER|CREATE)\s`),
+		regexp.MustCompile(`(?i)f["'](?:SELECT|INSERT|UPDATE|DELETE|DROP|ALTER|CREATE)\s`),
+		regexp.MustCompile(`(?i)(?:SELECT|INSERT|UPDATE|DELETE)\s.*%s`),
+	}
+
+	// Empty catch/except blocks.
+	emptyCatchPatterns = []*regexp.Regexp{
+		regexp.MustCompile(`catch\s*\([^)]*\)\s*\{\s*\}`),
+		regexp.MustCompile(`except\s*(?:\([^)]*\))?\s*:\s*$`),
+		regexp.MustCompile(`except\s+\w+\s*:\s*$`),
+		regexp.MustCompile(`catch\s*\{[\s]*\}`),
+	}
+
+	// Debug/console output statements.
+	debugPrintPatterns = []*regexp.Regexp{
+		regexp.MustCompile(`\bconsole\.(log|debug|info|warn|error)\s*\(`),
+		regexp.MustCompile(`\bfmt\.Print(ln|f)?\s*\(`),
+		regexp.MustCompile(`\bprint\s*\(`),
+		regexp.MustCompile(`\bprintln\s*\(`),
+		regexp.MustCompile(`\bSystem\.out\.print(ln)?\s*\(`),
+		regexp.MustCompile(`\bputs\s+`),
+		regexp.MustCompile(`\bpp\s+`),
+	}
+
+	// TODO/FIXME/HACK comments.
+	todoPattern = regexp.MustCompile(`(?i)\b(TODO|FIXME|HACK|XXX)\b`)
+)
+
+// File extensions considered test files (debug prints are acceptable in tests).
+var testFileIndicators = []string{
+	"_test.go",
+	".test.js", ".test.ts", ".test.jsx", ".test.tsx",
+	".spec.js", ".spec.ts", ".spec.jsx", ".spec.tsx",
+	"test_", "__test__",
+	"_test.py", "_test.rb",
+}
+
+// PatternsAnalyzer detects common anti-patterns in code diffs.
+type PatternsAnalyzer struct{}
+
+// NewPatternsAnalyzer creates a new anti-pattern detector.
+func NewPatternsAnalyzer() *PatternsAnalyzer {
+	return &PatternsAnalyzer{}
+}
+
+// Name returns the analyzer identifier.
+func (p *PatternsAnalyzer) Name() string {
+	return "patterns"
+}
+
+// Analyze scans added lines for anti-patterns.
+func (p *PatternsAnalyzer) Analyze(ctx context.Context, diff *interfaces.Diff) (*interfaces.AnalysisResult, error) {
+	result := &interfaces.AnalysisResult{
+		AnalyzerName: p.Name(),
+	}
+
+	for i := range diff.Files {
+		if ctx.Err() != nil {
+			return result, ctx.Err()
+		}
+
+		file := &diff.Files[i]
+		if file.IsBinary || file.Status == interfaces.FileDeleted {
+			continue
+		}
+
+		if isFixturePath(file.Path) {
+			continue
+		}
+
+		isTest := isTestFile(file.Path)
+
+		for j := range file.Hunks {
+			hunk := &file.Hunks[j]
+			for _, line := range hunk.AddedLines {
+				if ctx.Err() != nil {
+					return result, ctx.Err()
+				}
+
+				findings := p.scanLine(file.Path, line, isTest)
+				result.Findings = append(result.Findings, findings...)
+			}
+		}
+	}
+
+	return result, nil
+}
+
+// scanLine checks a single added line for anti-patterns.
+func (p *PatternsAnalyzer) scanLine(path string, line interfaces.Line, isTest bool) []interfaces.Finding {
+	var findings []interfaces.Finding
+	content := line.Content
+
+	// Check SQL string concatenation (skip test files — tests intentionally contain bad patterns).
+	if !isTest {
+		for _, re := range sqlConcatPatterns {
+			if re.MatchString(content) {
+				findings = append(findings, interfaces.Finding{
+					ID:        fmt.Sprintf("PAT-SQL-CONCAT-%d", line.Number),
+					Category:  interfaces.CategoryPattern,
+					Severity:  interfaces.SeverityMedium,
+					File:      path,
+					StartLine: line.Number,
+					EndLine:   line.Number,
+					Title:     "SQL string concatenation detected",
+					Description: fmt.Sprintf(
+						"Line %d builds a SQL query via string concatenation, which is vulnerable to SQL injection.",
+						line.Number,
+					),
+					Suggestion: "Use parameterized queries or a query builder instead of string concatenation.",
+					Source:     "patterns",
+					Confidence: 0.80,
+				})
+				break // One finding per line for this category.
+			}
+		}
+	}
+
+	// Check empty catch/except blocks (skip test files).
+	if !isTest {
+		for _, re := range emptyCatchPatterns {
+			if re.MatchString(content) {
+				findings = append(findings, interfaces.Finding{
+					ID:        fmt.Sprintf("PAT-EMPTY-CATCH-%d", line.Number),
+					Category:  interfaces.CategoryPattern,
+					Severity:  interfaces.SeverityMedium,
+					File:      path,
+					StartLine: line.Number,
+					EndLine:   line.Number,
+					Title:     "Empty catch/except block",
+					Description: fmt.Sprintf(
+						"Line %d has an empty error handler. Swallowing errors silently hides bugs.",
+						line.Number,
+					),
+					Suggestion: "Log the error or handle it explicitly. If intentionally ignoring, add a comment explaining why.",
+					Source:     "patterns",
+					Confidence: 0.85,
+				})
+				break
+			}
+		}
+	}
+
+	// Check debug prints (skip test files).
+	if !isTest {
+		for _, re := range debugPrintPatterns {
+			if re.MatchString(content) {
+				// Skip if it's in a comment.
+				trimmed := strings.TrimSpace(content)
+				if strings.HasPrefix(trimmed, "//") || strings.HasPrefix(trimmed, "#") || strings.HasPrefix(trimmed, "*") {
+					break
+				}
+				findings = append(findings, interfaces.Finding{
+					ID:        fmt.Sprintf("PAT-DEBUG-PRINT-%d", line.Number),
+					Category:  interfaces.CategoryPattern,
+					Severity:  interfaces.SeverityLow,
+					File:      path,
+					StartLine: line.Number,
+					EndLine:   line.Number,
+					Title:     "Debug print statement",
+					Description: fmt.Sprintf(
+						"Line %d contains a debug print statement that should not be in production code.",
+						line.Number,
+					),
+					Suggestion: "Remove the debug statement or replace with structured logging (e.g., slog).",
+					Source:     "patterns",
+					Confidence: 0.75,
+				})
+				break
+			}
+		}
+	}
+
+	// Check TODO/FIXME/HACK comments (skip test files — test TODOs are lower priority).
+	if !isTest && todoPattern.MatchString(content) {
+		matches := todoPattern.FindStringSubmatch(content)
+		tag := strings.ToUpper(matches[1])
+		findings = append(findings, interfaces.Finding{
+			ID:        fmt.Sprintf("PAT-TODO-%d", line.Number),
+			Category:  interfaces.CategoryPattern,
+			Severity:  interfaces.SeverityInfo,
+			File:      path,
+			StartLine: line.Number,
+			EndLine:   line.Number,
+			Title:     fmt.Sprintf("%s comment in new code", tag),
+			Description: fmt.Sprintf(
+				"Line %d contains a %s comment. Consider resolving it before merging.",
+				line.Number, tag,
+			),
+			Suggestion: "Resolve the TODO/FIXME/HACK or create a tracked issue for follow-up.",
+			Source:     "patterns",
+			Confidence: 0.95,
+		})
+	}
+
+	return findings
+}
+
+// isTestFile checks if a file path indicates a test file.
+func isTestFile(path string) bool {
+	lower := strings.ToLower(path)
+	for _, indicator := range testFileIndicators {
+		if strings.Contains(lower, indicator) {
+			return true
+		}
+	}
+	return false
+}
+
+// isFixturePath checks if a file path is under tests/fixtures/ or is a .diff file.
+func isFixturePath(path string) bool {
+	lower := strings.ToLower(path)
+	if strings.Contains(lower, "tests/fixtures/") {
+		return true
+	}
+	if strings.HasSuffix(lower, ".diff") {
+		return true
+	}
+	return false
+}
diff --git a/pkg/analyzer/patterns_test.go b/pkg/analyzer/patterns_test.go
new file mode 100644
index 0000000..d0b9f84
--- /dev/null
+++ b/pkg/analyzer/patterns_test.go
@@ -0,0 +1,511 @@
+package analyzer
+
+import (
+	"context"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+func TestPatternsAnalyzer_Name(t *testing.T) {
+	a := NewPatternsAnalyzer()
+	if a.Name() != "patterns" {
+		t.Errorf("expected name %q, got %q", "patterns", a.Name())
+	}
+}
+
+func TestPatternsAnalyzer_ImplementsInterface(t *testing.T) {
+	var _ Analyzer = NewPatternsAnalyzer()
+}
+
+func TestPatternsAnalyzer_SQLConcatenation_Detected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"select plus", `query := "SELECT * FROM users WHERE id=" + userID`},
+		{"insert plus", `q := "INSERT INTO logs VALUES(" + val + ")"`},
+		{"update plus", `db.Exec("UPDATE users SET name=" + name)`},
+		{"delete plus", `stmt := "DELETE FROM sessions WHERE id=" + id`},
+		{"sprintf select", `q := fmt.Sprintf("SELECT * FROM users WHERE id=%d", id)`},
+		{"percent-s in query", `query := "SELECT * FROM users WHERE name='%s'"`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("db.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if !hasFindingWithID(result.Findings, "PAT-SQL-CONCAT") {
+				t.Fatalf("expected SQL concat finding for %q, got %d findings: %v", tt.line, len(result.Findings), findingIDs(result.Findings))
+			}
+			assertAllFindingsHaveSeverity(t, result.Findings, "PAT-SQL-CONCAT", interfaces.SeverityMedium)
+		})
+	}
+}
+
+func TestPatternsAnalyzer_SQLConcatenation_NotDetected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"parameterized", `db.Query("SELECT * FROM users WHERE id = $1", id)`},
+		{"orm", `users := db.Where("name = ?", name).Find(&users)`},
+		{"string concat no sql", `msg := "hello " + name`},
+		{"comment", `// SELECT * FROM users WHERE id=" + id`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("db.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-SQL-CONCAT") {
+				t.Fatalf("unexpected SQL concat finding for safe code %q", tt.line)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_EmptyCatch_Detected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"js empty catch", `} catch (e) {}`},
+		{"java empty catch", `catch (Exception e) {}`},
+		{"python bare except", `except:`},
+		{"python typed except", `except ValueError:`},
+		{"go-style empty catch", `catch {}`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("handler.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if !hasFindingWithID(result.Findings, "PAT-EMPTY-CATCH") {
+				t.Fatalf("expected empty catch finding for %q", tt.line)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_EmptyCatch_NotDetected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"catch with body", `} catch (e) { log(e); }`},
+		{"except with pass", `except ValueError as e:`},
+		{"normal code", `x := 42`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("handler.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-EMPTY-CATCH") {
+				t.Fatalf("unexpected empty catch finding for %q", tt.line)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_DebugPrint_Detected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"console.log", `console.log("debug value:", x)`},
+		{"console.debug", `console.debug(data)`},
+		{"fmt.Println", `fmt.Println("debugging:", err)`},
+		{"fmt.Printf", `fmt.Printf("value: %v\n", x)`},
+		{"fmt.Print", `fmt.Print(result)`},
+		{"print()", `print("debug")`},
+		{"println()", `println("debug value")`},
+		{"System.out.println", `System.out.println("debug")`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("service.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if !hasFindingWithID(result.Findings, "PAT-DEBUG-PRINT") {
+				t.Fatalf("expected debug print finding for %q, got: %v", tt.line, findingIDs(result.Findings))
+			}
+			assertAllFindingsHaveSeverity(t, result.Findings, "PAT-DEBUG-PRINT", interfaces.SeverityLow)
+		})
+	}
+}
+
+func TestPatternsAnalyzer_DebugPrint_SkippedInTestFiles(t *testing.T) {
+	testPaths := []string{
+		"pkg/handler/handler_test.go",
+		"src/utils.test.js",
+		"src/utils.spec.ts",
+		"tests/test_helpers.py",
+	}
+
+	for _, path := range testPaths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path, `fmt.Println("debugging test")`)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-DEBUG-PRINT") {
+				t.Fatalf("debug prints in test files should not be flagged: %s", path)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_DebugPrint_NotDetectedInComments(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"go comment", `// fmt.Println("debugging")`},
+		{"python comment", `# print("debugging")`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("service.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-DEBUG-PRINT") {
+				t.Fatalf("commented-out debug print should not be flagged: %q", tt.line)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_TODO_Detected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+		tag  string
+	}{
+		{"TODO", `// TODO: refactor this later`, "TODO"},
+		{"FIXME", `# FIXME: handle edge case`, "FIXME"},
+		{"HACK", `/* HACK: workaround for issue #123 */`, "HACK"},
+		{"XXX", `// XXX: this is fragile`, "XXX"},
+		{"lowercase todo", `// todo: clean up`, "TODO"},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("service.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if !hasFindingWithID(result.Findings, "PAT-TODO") {
+				t.Fatalf("expected TODO finding for %q", tt.line)
+			}
+			assertAllFindingsHaveSeverity(t, result.Findings, "PAT-TODO", interfaces.SeverityInfo)
+		})
+	}
+}
+
+func TestPatternsAnalyzer_TODO_NotDetected(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"normal code", `x := 42`},
+		{"normal comment", `// This function handles authentication`},
+		{"string literal", `msg := "hello world"`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("service.go", tt.line)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-TODO") {
+				t.Fatalf("unexpected TODO finding for %q", tt.line)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_MultiplePatterns_SingleLine(t *testing.T) {
+	// A line with both a debug print and a TODO should produce two findings.
+	diff := diffWithAddedLines("service.go",
+		`fmt.Println("TODO: fix this later")`,
+	)
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) < 2 {
+		t.Fatalf("expected at least 2 findings (debug print + TODO), got %d: %v", len(result.Findings), findingIDs(result.Findings))
+	}
+}
+
+func TestPatternsAnalyzer_SkipsFixturePaths(t *testing.T) {
+	fixturePaths := []string{
+		"tests/fixtures/bad-pr/handler.go",
+		"tests/fixtures/vulnerable-pr/db.go",
+	}
+
+	for _, path := range fixturePaths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path,
+				`query := "SELECT * FROM users WHERE id=" + userID`,
+				`} catch (e) {}`,
+				`fmt.Println("debug")`,
+				`// TODO: fix this`,
+			)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) != 0 {
+				t.Fatalf("expected no findings for fixture path %q, got %d: %v",
+					path, len(result.Findings), findingIDs(result.Findings))
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_SkipsDiffFiles(t *testing.T) {
+	diff := diffWithAddedLines("tests/fixtures/diffs/bad-code.diff",
+		`query := "SELECT * FROM users WHERE id=" + userID`,
+		`} catch (e) {}`,
+		`// TODO: fix later`,
+	)
+
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for .diff file, got %d: %v",
+			len(result.Findings), findingIDs(result.Findings))
+	}
+}
+
+func TestPatternsAnalyzer_SQLConcat_SkippedInTestFiles(t *testing.T) {
+	testPaths := []string{
+		"pkg/db/db_test.go",
+		"src/db.test.js",
+		"tests/test_db.py",
+	}
+
+	for _, path := range testPaths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path,
+				`query := "SELECT * FROM users WHERE id=" + userID`,
+			)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-SQL-CONCAT") {
+				t.Fatalf("SQL concat in test file %q should not be flagged", path)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_EmptyCatch_SkippedInTestFiles(t *testing.T) {
+	testPaths := []string{
+		"pkg/handler/handler_test.go",
+		"src/handler.test.ts",
+	}
+
+	for _, path := range testPaths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path,
+				`} catch (e) {}`,
+			)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-EMPTY-CATCH") {
+				t.Fatalf("empty catch in test file %q should not be flagged", path)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_TODO_SkippedInTestFiles(t *testing.T) {
+	testPaths := []string{
+		"pkg/service/service_test.go",
+		"src/service.spec.ts",
+	}
+
+	for _, path := range testPaths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path,
+				`// TODO: add more test cases`,
+			)
+			result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if hasFindingWithID(result.Findings, "PAT-TODO") {
+				t.Fatalf("TODO in test file %q should not be flagged", path)
+			}
+		})
+	}
+}
+
+func TestPatternsAnalyzer_SQLConcat_StillDetectedInProductionCode(t *testing.T) {
+	diff := diffWithAddedLines("db/queries.go",
+		`query := "SELECT * FROM users WHERE id=" + userID`,
+	)
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if !hasFindingWithID(result.Findings, "PAT-SQL-CONCAT") {
+		t.Fatal("SQL concat in production code should still be flagged")
+	}
+}
+
+func TestPatternsAnalyzer_SkipsDeletedFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "old.go",
+				Status: interfaces.FileDeleted,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 1, Content: `fmt.Println("debug")`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for deleted files, got %d", len(result.Findings))
+	}
+}
+
+func TestPatternsAnalyzer_SkipsBinaryFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:     "image.png",
+				Status:   interfaces.FileAdded,
+				IsBinary: true,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 1, Content: `console.log("debug")`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for binary files, got %d", len(result.Findings))
+	}
+}
+
+func TestPatternsAnalyzer_EmptyDiff(t *testing.T) {
+	diff := &interfaces.Diff{}
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for empty diff, got %d", len(result.Findings))
+	}
+}
+
+func TestPatternsAnalyzer_NoFindingsOnCleanCode(t *testing.T) {
+	diff := diffWithAddedLines("main.go",
+		`func main() {`,
+		`    slog.Info("starting server", "port", 8080)`,
+		`    db.Query("SELECT * FROM users WHERE id = $1", id)`,
+		`    if err != nil {`,
+		`        return fmt.Errorf("query failed: %w", err)`,
+		`    }`,
+		`}`,
+	)
+
+	result, err := NewPatternsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings on clean code, got %d: %v", len(result.Findings), findingIDs(result.Findings))
+	}
+}
+
+func TestPatternsAnalyzer_ContextCancellation(t *testing.T) {
+	ctx, cancel := context.WithCancel(context.Background())
+	cancel()
+
+	diff := diffWithAddedLines("service.go", `fmt.Println("debug")`)
+	_, err := NewPatternsAnalyzer().Analyze(ctx, diff)
+	if err == nil {
+		t.Fatal("expected error on cancelled context")
+	}
+}
+
+// --- test helpers ---
+
+func hasFindingWithID(findings []interfaces.Finding, prefix string) bool {
+	for _, f := range findings {
+		if len(f.ID) >= len(prefix) && f.ID[:len(prefix)] == prefix {
+			return true
+		}
+	}
+	return false
+}
+
+func findingIDs(findings []interfaces.Finding) []string {
+	ids := make([]string, len(findings))
+	for i, f := range findings {
+		ids[i] = f.ID
+	}
+	return ids
+}
+
+func assertAllFindingsHaveSeverity(t *testing.T, findings []interfaces.Finding, idPrefix string, severity interfaces.Severity) {
+	t.Helper()
+	for _, f := range findings {
+		if len(f.ID) >= len(idPrefix) && f.ID[:len(idPrefix)] == idPrefix {
+			if f.Severity != severity {
+				t.Errorf("finding %s has severity %q, expected %q", f.ID, f.Severity, severity)
+			}
+		}
+	}
+}
diff --git a/pkg/analyzer/registry.go b/pkg/analyzer/registry.go
new file mode 100644
index 0000000..192cd27
--- /dev/null
+++ b/pkg/analyzer/registry.go
@@ -0,0 +1,104 @@
+// Package analyzer provides static analysis checks for code diffs.
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"sync"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// Analyzer is the interface that individual analysis checks implement.
+// Defined here at the consumer site until the Architect adds it to pkg/interfaces/.
+type Analyzer interface {
+	// Name returns the unique identifier for this analyzer.
+	Name() string
+
+	// Analyze runs the analysis against a parsed diff and returns results.
+	Analyze(ctx context.Context, diff *interfaces.Diff) (*interfaces.AnalysisResult, error)
+}
+
+// Registry manages a collection of analyzers and tracks which are enabled.
+type Registry struct {
+	mu        sync.RWMutex
+	analyzers map[string]Analyzer
+	enabled   map[string]bool
+}
+
+// NewRegistry creates an empty analyzer registry.
+func NewRegistry() *Registry {
+	return &Registry{
+		analyzers: make(map[string]Analyzer),
+		enabled:   make(map[string]bool),
+	}
+}
+
+// Register adds an analyzer to the registry. It is enabled by default.
+// Returns an error if an analyzer with the same name is already registered.
+func (r *Registry) Register(a Analyzer) error {
+	r.mu.Lock()
+	defer r.mu.Unlock()
+
+	name := a.Name()
+	if _, exists := r.analyzers[name]; exists {
+		return fmt.Errorf("analyzer: %q is already registered", name)
+	}
+
+	r.analyzers[name] = a
+	r.enabled[name] = true
+	return nil
+}
+
+// Get returns an analyzer by name. Returns nil if not found.
+func (r *Registry) Get(name string) Analyzer {
+	r.mu.RLock()
+	defer r.mu.RUnlock()
+	return r.analyzers[name]
+}
+
+// List returns the names of all registered analyzers.
+func (r *Registry) List() []string {
+	r.mu.RLock()
+	defer r.mu.RUnlock()
+
+	names := make([]string, 0, len(r.analyzers))
+	for name := range r.analyzers {
+		names = append(names, name)
+	}
+	return names
+}
+
+// SetEnabled enables or disables an analyzer by name.
+// Returns an error if the analyzer is not registered.
+func (r *Registry) SetEnabled(name string, enabled bool) error {
+	r.mu.Lock()
+	defer r.mu.Unlock()
+
+	if _, exists := r.analyzers[name]; !exists {
+		return fmt.Errorf("analyzer: %q is not registered", name)
+	}
+	r.enabled[name] = enabled
+	return nil
+}
+
+// IsEnabled reports whether the named analyzer is enabled.
+func (r *Registry) IsEnabled(name string) bool {
+	r.mu.RLock()
+	defer r.mu.RUnlock()
+	return r.enabled[name]
+}
+
+// EnabledAnalyzers returns all analyzers that are currently enabled.
+func (r *Registry) EnabledAnalyzers() []Analyzer {
+	r.mu.RLock()
+	defer r.mu.RUnlock()
+
+	var result []Analyzer
+	for name, a := range r.analyzers {
+		if r.enabled[name] {
+			result = append(result, a)
+		}
+	}
+	return result
+}
diff --git a/pkg/analyzer/secrets.go b/pkg/analyzer/secrets.go
new file mode 100644
index 0000000..611e167
--- /dev/null
+++ b/pkg/analyzer/secrets.go
@@ -0,0 +1,338 @@
+package analyzer
+
+import (
+	"context"
+	"fmt"
+	"math"
+	"regexp"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// secretPattern defines a single regex-based secret detection rule.
+type secretPattern struct {
+	name     string
+	regex    *regexp.Regexp
+	severity interfaces.Severity
+}
+
+// Compiled secret detection patterns.
+var secretPatterns = []secretPattern{
+	{
+		name:     "AWS Access Key ID",
+		regex:    regexp.MustCompile(`(?:^|[^A-Za-z0-9/+=])AKIA[0-9A-Z]{16}(?:[^A-Za-z0-9/+=]|$)`),
+		severity: interfaces.SeverityHigh,
+	},
+	{
+		name:     "AWS Secret Access Key",
+		regex:    regexp.MustCompile(`(?i)(?:aws_secret_access_key|aws_secret_key|secret_access_key)\s*[:=]\s*[A-Za-z0-9/+=]{40}`),
+		severity: interfaces.SeverityCritical,
+	},
+	{
+		name:     "RSA/SSH Private Key",
+		regex:    regexp.MustCompile(`-----BEGIN (?:RSA |EC |DSA |OPENSSH )?PRIVATE KEY-----`),
+		severity: interfaces.SeverityCritical,
+	},
+	{
+		name:     "Generic API Key Assignment",
+		regex:    regexp.MustCompile(`(?i)(?:api_key|apikey|api-key|api_secret|apisecret)\s*[:=]\s*["']?[A-Za-z0-9_\-]{16,}["']?`),
+		severity: interfaces.SeverityHigh,
+	},
+	{
+		name:     "Bearer Token",
+		regex:    regexp.MustCompile(`(?i)(?:bearer\s+)[A-Za-z0-9_\-.]{20,}`),
+		severity: interfaces.SeverityHigh,
+	},
+	{
+		name:     "Database Connection String",
+		regex:    regexp.MustCompile(`(?i)(?:postgres|postgresql|mysql|mongodb|redis|amqp)://[^\s"'` + "`" + `]{10,}`),
+		severity: interfaces.SeverityHigh,
+	},
+	{
+		name:     "Password Assignment",
+		regex:    regexp.MustCompile(`(?i)(?:password|passwd|pwd)\s*[:=]\s*["'][^"']{8,}["']`),
+		severity: interfaces.SeverityHigh,
+	},
+	{
+		name:     "GitHub Personal Access Token",
+		regex:    regexp.MustCompile(`ghp_[A-Za-z0-9]{36}`),
+		severity: interfaces.SeverityHigh,
+	},
+	{
+		name:     "Generic Secret Assignment",
+		regex:    regexp.MustCompile(`(?i)(?:secret|token|auth)_?(?:key)?\s*[:=]\s*["'][A-Za-z0-9_\-/+=]{20,}["']`),
+		severity: interfaces.SeverityHigh,
+	},
+}
+
+// Substrings that indicate a line is a false positive (test data, examples, placeholders).
+var falsePositiveIndicators = []string{
+	"example",
+	"placeholder",
+	"your-",
+	"your_",
+	"xxx",
+	"changeme",
+	"replace_me",
+	"insert_here",
+	"todo",
+	"fixme",
+	"dummy",
+	"fake",
+	"test_",
+	"mock_",
+	"sample",
+	"<your",
+	"${",
+	"{{",
+}
+
+// File paths that typically contain example/test data and should be skipped.
+var falsePositivePathSuffixes = []string{
+	"_test.go",
+	".test.ts",
+	".test.js",
+	".spec.ts",
+	".spec.js",
+	".example",
+	".example.yml",
+	".example.yaml",
+	".example.json",
+	".example.env",
+	".sample",
+	"testdata/",
+	"fixtures/",
+	"__mocks__/",
+	".diff",
+	"go.sum",
+	".lock",
+	"package-lock.json",
+}
+
+// Prefixes indicating a line is a checksum, not a secret.
+var checksumPrefixes = []string{
+	"h1:",
+	"sha256:",
+	"sha512:",
+	"sha1:",
+	"sha384:",
+}
+
+// isChecksumLine reports whether the line looks like a checksum entry.
+func isChecksumLine(line string) bool {
+	trimmed := strings.TrimSpace(line)
+	for _, prefix := range checksumPrefixes {
+		if strings.HasPrefix(trimmed, prefix) {
+			return true
+		}
+	}
+	return false
+}
+
+// SecretsAnalyzer detects hardcoded secrets, API keys, and credentials in diffs.
+type SecretsAnalyzer struct {
+	entropyThreshold float64
+	entropyMinLength int
+}
+
+// NewSecretsAnalyzer creates a secrets analyzer with default settings.
+func NewSecretsAnalyzer() *SecretsAnalyzer {
+	return &SecretsAnalyzer{
+		entropyThreshold: 4.5,
+		entropyMinLength: 20,
+	}
+}
+
+// Name returns the analyzer identifier.
+func (s *SecretsAnalyzer) Name() string {
+	return "secrets"
+}
+
+// Analyze scans added lines in the diff for hardcoded secrets.
+func (s *SecretsAnalyzer) Analyze(ctx context.Context, diff *interfaces.Diff) (*interfaces.AnalysisResult, error) {
+	result := &interfaces.AnalysisResult{
+		AnalyzerName: s.Name(),
+	}
+
+	for i := range diff.Files {
+		if ctx.Err() != nil {
+			return result, ctx.Err()
+		}
+
+		file := &diff.Files[i]
+
+		if file.IsBinary || file.Status == interfaces.FileDeleted {
+			continue
+		}
+
+		if isTestFixturePath(file.Path) {
+			continue
+		}
+
+		for j := range file.Hunks {
+			hunk := &file.Hunks[j]
+			// Only scan added lines.
+			for _, line := range hunk.AddedLines {
+				if ctx.Err() != nil {
+					return result, ctx.Err()
+				}
+
+				findings := s.scanLine(file.Path, line)
+				result.Findings = append(result.Findings, findings...)
+			}
+		}
+	}
+
+	return result, nil
+}
+
+// scanLine checks a single added line against all secret patterns and entropy.
+func (s *SecretsAnalyzer) scanLine(path string, line interfaces.Line) []interfaces.Finding {
+	content := line.Content
+	if isFalsePositive(content) {
+		return nil
+	}
+
+	var findings []interfaces.Finding
+
+	// Regex pattern matching.
+	for _, p := range secretPatterns {
+		if p.regex.MatchString(content) {
+			findings = append(findings, interfaces.Finding{
+				ID:       fmt.Sprintf("SEC-%s-%d", sanitizeID(p.name), line.Number),
+				Category: interfaces.CategorySecrets,
+				Severity: p.severity,
+				File:     path,
+				StartLine: line.Number,
+				EndLine:   line.Number,
+				Title:     fmt.Sprintf("Possible %s detected", p.name),
+				Description: fmt.Sprintf(
+					"Line %d may contain a hardcoded %s. Secrets should be stored in environment variables or a secrets manager.",
+					line.Number, p.name,
+				),
+				Suggestion: "Remove the hardcoded secret and use an environment variable or secrets manager instead.",
+				Source:     "secrets",
+				Confidence: 0.85,
+			})
+		}
+	}
+
+	// Shannon entropy check for high-entropy strings.
+	if f, ok := s.checkEntropy(path, line); ok {
+		// Don't add an entropy finding if we already matched a specific pattern.
+		if len(findings) == 0 {
+			findings = append(findings, f)
+		}
+	}
+
+	return findings
+}
+
+// checkEntropy looks for high-entropy tokens on a line that may be secrets.
+func (s *SecretsAnalyzer) checkEntropy(path string, line interfaces.Line) (interfaces.Finding, bool) {
+	if isChecksumLine(line.Content) {
+		return interfaces.Finding{}, false
+	}
+
+	tokens := extractTokens(line.Content)
+	for _, token := range tokens {
+		if len(token) < s.entropyMinLength {
+			continue
+		}
+		entropy := shannonEntropy(token)
+		if entropy > s.entropyThreshold {
+			return interfaces.Finding{
+				ID:        fmt.Sprintf("SEC-ENTROPY-%d", line.Number),
+				Category:  interfaces.CategorySecrets,
+				Severity:  interfaces.SeverityMedium,
+				File:      path,
+				StartLine: line.Number,
+				EndLine:   line.Number,
+				Title:     "High-entropy string detected",
+				Description: fmt.Sprintf(
+					"Line %d contains a high-entropy string (%.2f bits/char) that may be a secret.",
+					line.Number, entropy,
+				),
+				Suggestion: "Verify this is not a hardcoded secret. If it is, move it to environment variables.",
+				Source:     "secrets",
+				Confidence: 0.60,
+				Metadata: map[string]any{
+					"entropy": entropy,
+					"length":  len(token),
+				},
+			}, true
+		}
+	}
+	return interfaces.Finding{}, false
+}
+
+// shannonEntropy calculates the Shannon entropy of a string in bits per character.
+func shannonEntropy(s string) float64 {
+	if len(s) == 0 {
+		return 0
+	}
+
+	freq := make(map[rune]int)
+	for _, c := range s {
+		freq[c]++
+	}
+
+	length := float64(len([]rune(s)))
+	entropy := 0.0
+	for _, count := range freq {
+		p := float64(count) / length
+		if p > 0 {
+			entropy -= p * math.Log2(p)
+		}
+	}
+	return entropy
+}
+
+// extractTokens splits a line into candidate secret tokens.
+// It extracts quoted strings and assignment values.
+func extractTokens(line string) []string {
+	var tokens []string
+
+	// Extract quoted strings.
+	quoteRe := regexp.MustCompile(`["']([^"']+)["']`)
+	for _, m := range quoteRe.FindAllStringSubmatch(line, -1) {
+		tokens = append(tokens, m[1])
+	}
+
+	// Extract assignment values (key=value without quotes).
+	assignRe := regexp.MustCompile(`(?:=|:)\s*([A-Za-z0-9_\-/+=.]{20,})`)
+	for _, m := range assignRe.FindAllStringSubmatch(line, -1) {
+		tokens = append(tokens, m[1])
+	}
+
+	return tokens
+}
+
+// isFalsePositive checks if a line contains indicators of test/example data.
+func isFalsePositive(line string) bool {
+	lower := strings.ToLower(line)
+	for _, indicator := range falsePositiveIndicators {
+		if strings.Contains(lower, indicator) {
+			return true
+		}
+	}
+	return false
+}
+
+// isTestFixturePath checks if a file path is a known test/fixture/example file.
+func isTestFixturePath(path string) bool {
+	lower := strings.ToLower(path)
+	for _, suffix := range falsePositivePathSuffixes {
+		if strings.HasSuffix(lower, suffix) || strings.Contains(lower, suffix) {
+			return true
+		}
+	}
+	return false
+}
+
+// sanitizeID converts a pattern name to a short uppercase ID fragment.
+func sanitizeID(name string) string {
+	r := strings.NewReplacer(" ", "-", "/", "-")
+	return strings.ToUpper(r.Replace(name))
+}
diff --git a/pkg/analyzer/secrets_test.go b/pkg/analyzer/secrets_test.go
new file mode 100644
index 0000000..e4649c3
--- /dev/null
+++ b/pkg/analyzer/secrets_test.go
@@ -0,0 +1,566 @@
+package analyzer
+
+import (
+	"context"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// helper to build a Diff with a single file containing added lines.
+func diffWithAddedLines(path string, lines ...string) *interfaces.Diff {
+	added := make([]interfaces.Line, len(lines))
+	for i, l := range lines {
+		added[i] = interfaces.Line{Number: i + 1, Content: l}
+	}
+	return &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   path,
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						NewStart:   1,
+						NewLines:   len(lines),
+						AddedLines: added,
+					},
+				},
+			},
+		},
+	}
+}
+
+func TestSecretsAnalyzer_Name(t *testing.T) {
+	a := NewSecretsAnalyzer()
+	if a.Name() != "secrets" {
+		t.Errorf("expected name %q, got %q", "secrets", a.Name())
+	}
+}
+
+func TestSecretsAnalyzer_AWSAccessKey(t *testing.T) {
+	diff := diffWithAddedLines("config.go",
+		`awsKey := "AKIAIOSFODNN7WKRB3PQ"`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for AWS access key")
+	}
+	assertHasFindingWithTitle(t, result.Findings, "AWS Access Key ID")
+}
+
+func TestSecretsAnalyzer_AWSSecretKey(t *testing.T) {
+	diff := diffWithAddedLines("config.go",
+		`aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYKZ6NR4TWAB`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for AWS secret key")
+	}
+	assertHasSeverity(t, result.Findings, interfaces.SeverityCritical)
+}
+
+func TestSecretsAnalyzer_PrivateKey(t *testing.T) {
+	diff := diffWithAddedLines("deploy.sh",
+		`-----BEGIN RSA PRIVATE KEY-----`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for private key")
+	}
+	assertHasSeverity(t, result.Findings, interfaces.SeverityCritical)
+}
+
+func TestSecretsAnalyzer_SSHPrivateKey(t *testing.T) {
+	diff := diffWithAddedLines("id_rsa",
+		`-----BEGIN OPENSSH PRIVATE KEY-----`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for SSH private key")
+	}
+	assertHasFindingWithTitle(t, result.Findings, "RSA/SSH Private Key")
+}
+
+func TestSecretsAnalyzer_GenericAPIKey(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"api_key equals", `api_key = "sk_live_1234567890abcdef"`},
+		{"apikey equals", `apikey="abcdef1234567890ghij"`},
+		{"api-key colon", `api-key: abcdefghijklmnopqrst`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("config.yaml", tt.line)
+			result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) == 0 {
+				t.Fatalf("expected finding for %q", tt.line)
+			}
+		})
+	}
+}
+
+func TestSecretsAnalyzer_BearerToken(t *testing.T) {
+	diff := diffWithAddedLines("main.go",
+		`Authorization: Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxMjM0NTY3ODkw`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for bearer token")
+	}
+	assertHasFindingWithTitle(t, result.Findings, "Bearer Token")
+}
+
+func TestSecretsAnalyzer_ConnectionStrings(t *testing.T) {
+	tests := []struct {
+		name string
+		line string
+	}{
+		{"postgres", `dsn := "postgres://user:pass@host:5432/dbname"`},
+		{"mysql", `dsn := "mysql://root:secret@localhost/mydb"`},
+		{"mongodb", `uri := "mongodb://admin:s3cretval@cluster.internal.io/db"`},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff := diffWithAddedLines("db.go", tt.line)
+			result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) == 0 {
+				t.Fatalf("expected finding for connection string: %s", tt.line)
+			}
+		})
+	}
+}
+
+func TestSecretsAnalyzer_PasswordAssignment(t *testing.T) {
+	diff := diffWithAddedLines("config.go",
+		`password = "SuperS3cretP@ssword!"`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for password")
+	}
+}
+
+func TestSecretsAnalyzer_GitHubToken(t *testing.T) {
+	diff := diffWithAddedLines("ci.go",
+		`token := "ghp_ABCDEFGHIJKLMNOPQRSTUVWXYZabcdef1234"`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected at least one finding for GitHub token")
+	}
+}
+
+func TestSecretsAnalyzer_NoFindingsOnCleanCode(t *testing.T) {
+	diff := diffWithAddedLines("main.go",
+		`func main() {`,
+		`    fmt.Println("hello world")`,
+		`    x := 42`,
+		`}`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings, got %d: %+v", len(result.Findings), result.Findings)
+	}
+}
+
+func TestSecretsAnalyzer_SkipsRemovedLines(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "config.go",
+				Status: interfaces.FileModified,
+				Hunks: []interfaces.Hunk{
+					{
+						// Secret in removed lines only — should NOT flag.
+						RemovedLines: []interfaces.Line{
+							{Number: 1, Content: `password = "SuperS3cretP@ssword!"`},
+						},
+						AddedLines: []interfaces.Line{
+							{Number: 1, Content: `password = os.Getenv("DB_PASSWORD")`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for removed lines, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_SkipsDeletedFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:   "old-config.go",
+				Status: interfaces.FileDeleted,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 1, Content: `password = "secret123456"`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for deleted files, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_SkipsBinaryFiles(t *testing.T) {
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{
+			{
+				Path:     "image.png",
+				Status:   interfaces.FileAdded,
+				IsBinary: true,
+				Hunks: []interfaces.Hunk{
+					{
+						AddedLines: []interfaces.Line{
+							{Number: 1, Content: `AKIAIOSFODNN7EXAMPLE`},
+						},
+					},
+				},
+			},
+		},
+	}
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for binary files, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_SkipsTestFixtures(t *testing.T) {
+	paths := []string{
+		"pkg/analyzer/secrets_test.go",
+		"tests/fixtures/secrets.go",
+		"testdata/config.yaml",
+		"config.example.yml",
+	}
+
+	for _, path := range paths {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path,
+				`password = "SuperS3cretP@ssword!"`,
+			)
+			result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) != 0 {
+				t.Fatalf("expected no findings for test fixture path %q, got %d", path, len(result.Findings))
+			}
+		})
+	}
+}
+
+func TestSecretsAnalyzer_FalsePositivePlaceholders(t *testing.T) {
+	lines := []string{
+		`api_key = "your-api-key-here-placeholder"`,
+		`password = "changeme"`,
+		`secret = "EXAMPLE_SECRET_KEY_VALUE_12345"`,
+		`token = "replace_me_with_real_token_value"`,
+		`dsn := "postgres://user:${DB_PASSWORD}@host/db"`,
+		`key := "{{.APIKey}}"`,
+		`api_key = "dummy_key_for_testing_only"`,
+	}
+
+	for _, line := range lines {
+		t.Run(line, func(t *testing.T) {
+			diff := diffWithAddedLines("config.go", line)
+			result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) != 0 {
+				t.Fatalf("expected false positive to be skipped for %q, got %d findings", line, len(result.Findings))
+			}
+		})
+	}
+}
+
+func TestSecretsAnalyzer_HighEntropyString(t *testing.T) {
+	// A high-entropy random string that doesn't match known patterns.
+	diff := diffWithAddedLines("auth.go",
+		`verificationCode := "aB3$kL9mN2pQ7rS4tU6vW8xY0z1cD5eF"`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) == 0 {
+		t.Fatal("expected entropy finding for high-entropy string")
+	}
+	found := false
+	for _, f := range result.Findings {
+		if f.Title == "High-entropy string detected" {
+			found = true
+			break
+		}
+	}
+	if !found {
+		t.Fatal("expected a high-entropy finding")
+	}
+}
+
+func TestSecretsAnalyzer_LowEntropyNotFlagged(t *testing.T) {
+	// A long but low-entropy string should NOT trigger.
+	diff := diffWithAddedLines("main.go",
+		`msg := "aaaaaaaaaaaaaaaaaaaaaaaaa"`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	for _, f := range result.Findings {
+		if f.Title == "High-entropy string detected" {
+			t.Fatal("low-entropy string should not trigger entropy check")
+		}
+	}
+}
+
+func TestSecretsAnalyzer_ContextCancellation(t *testing.T) {
+	ctx, cancel := context.WithCancel(context.Background())
+	cancel() // Cancel immediately.
+
+	diff := diffWithAddedLines("config.go",
+		`password = "SuperS3cretP@ssword!"`,
+	)
+
+	_, err := NewSecretsAnalyzer().Analyze(ctx, diff)
+	if err == nil {
+		t.Fatal("expected error on cancelled context")
+	}
+}
+
+func TestSecretsAnalyzer_MultipleFindings(t *testing.T) {
+	diff := diffWithAddedLines("config.go",
+		`aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYKZ6NR4TWAB`,
+		`password = "SuperS3cretP@ssword!"`,
+		`dsn := "postgres://admin:secret@db.internal.io:5432/production"`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) < 3 {
+		t.Fatalf("expected at least 3 findings, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_EmptyDiff(t *testing.T) {
+	diff := &interfaces.Diff{}
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for empty diff, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_SkipsGoSumFiles(t *testing.T) {
+	diff := diffWithAddedLines("go.sum",
+		`github.com/stretchr/testify v1.9.0 h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=`,
+		`github.com/stretchr/testify v1.9.0/go.mod h1:r2ic/lqez/lEtzL7wO/rwa5dbSLXVDPFyf8C91i36aY=`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for go.sum, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_SkipsLockFiles(t *testing.T) {
+	lockFiles := []string{
+		"yarn.lock",
+		"Cargo.lock",
+		"Gemfile.lock",
+		"package-lock.json",
+	}
+
+	for _, path := range lockFiles {
+		t.Run(path, func(t *testing.T) {
+			diff := diffWithAddedLines(path,
+				`password = "SuperS3cretP@ssword!"`,
+			)
+			result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			if len(result.Findings) != 0 {
+				t.Fatalf("expected no findings for lock file %q, got %d", path, len(result.Findings))
+			}
+		})
+	}
+}
+
+func TestSecretsAnalyzer_SkipsDiffFiles(t *testing.T) {
+	diff := diffWithAddedLines("tests/fixtures/diffs/secrets-leak.diff",
+		`password = "SuperS3cretP@ssword!"`,
+		`aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYKZ6NR4TWAB`,
+	)
+
+	result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+	if len(result.Findings) != 0 {
+		t.Fatalf("expected no findings for .diff file, got %d", len(result.Findings))
+	}
+}
+
+func TestSecretsAnalyzer_EntropySkipsChecksumLines(t *testing.T) {
+	checksumLines := []string{
+		`h1:HtqpIVDClZ4nwg75+f6Lvsy/wHu+3BoSGCbBAcpTsTg=`,
+		`sha256:2cf24dba5fb0a30e26e83b2ac5b9e29e1b161e5c1fa7425e73043362938b9824`,
+		`sha512:cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce`,
+		`sha1:a94a8fe5ccb19ba61c4c0873d391e987982fbbd3aabbccdd`,
+	}
+
+	for _, line := range checksumLines {
+		t.Run(line[:6], func(t *testing.T) {
+			diff := diffWithAddedLines("config.go", line)
+			result, err := NewSecretsAnalyzer().Analyze(context.Background(), diff)
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			for _, f := range result.Findings {
+				if f.Title == "High-entropy string detected" {
+					t.Fatalf("checksum line should not trigger entropy check: %q", line)
+				}
+			}
+		})
+	}
+}
+
+func TestSecretsAnalyzer_ImplementsInterface(t *testing.T) {
+	var _ Analyzer = NewSecretsAnalyzer()
+}
+
+func TestShannonEntropy(t *testing.T) {
+	tests := []struct {
+		name    string
+		input   string
+		wantLow float64
+		wantHi  float64
+	}{
+		{"empty string", "", 0, 0},
+		{"single char", "aaaa", 0, 0.01},
+		{"low entropy", "aabb", 0.9, 1.1},
+		{"high entropy", "aB3$kL9mN2pQ7rS4", 3.5, 5.0},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			got := shannonEntropy(tt.input)
+			if got < tt.wantLow || got > tt.wantHi {
+				t.Errorf("shannonEntropy(%q) = %f, want between %f and %f", tt.input, got, tt.wantLow, tt.wantHi)
+			}
+		})
+	}
+}
+
+// --- test helpers ---
+
+func assertHasFindingWithTitle(t *testing.T, findings []interfaces.Finding, substr string) {
+	t.Helper()
+	for _, f := range findings {
+		if contains(f.Title, substr) {
+			return
+		}
+	}
+	titles := make([]string, len(findings))
+	for i, f := range findings {
+		titles[i] = f.Title
+	}
+	t.Errorf("no finding with title containing %q, got: %v", substr, titles)
+}
+
+func assertHasSeverity(t *testing.T, findings []interfaces.Finding, severity interfaces.Severity) {
+	t.Helper()
+	for _, f := range findings {
+		if f.Severity == severity {
+			return
+		}
+	}
+	t.Errorf("no finding with severity %q", severity)
+}
+
+func contains(s, substr string) bool {
+	return len(s) >= len(substr) && searchString(s, substr)
+}
+
+func searchString(s, sub string) bool {
+	for i := 0; i <= len(s)-len(sub); i++ {
+		if s[i:i+len(sub)] == sub {
+			return true
+		}
+	}
+	return false
+}
diff --git a/pkg/cli/config.go b/pkg/cli/config.go
new file mode 100644
index 0000000..9072e14
--- /dev/null
+++ b/pkg/cli/config.go
@@ -0,0 +1,118 @@
+// Package cli provides CLI-specific logic including configuration loading.
+package cli
+
+import (
+	"fmt"
+	"os"
+
+	"gopkg.in/yaml.v3"
+)
+
+// Config represents the .shipsafe.yml configuration file.
+type Config struct {
+	Version    string          `yaml:"version"`
+	Thresholds ThresholdConfig `yaml:"thresholds"`
+	Analyzers  AnalyzersConfig `yaml:"analyzers"`
+	Output     OutputConfig    `yaml:"output"`
+	CI         CIConfig        `yaml:"ci"`
+}
+
+// ThresholdConfig holds the trust score thresholds.
+type ThresholdConfig struct {
+	Green  int `yaml:"green"`
+	Yellow int `yaml:"yellow"`
+}
+
+// AnalyzersConfig holds per-analyzer configuration.
+type AnalyzersConfig struct {
+	Complexity AnalyzerModuleConfig `yaml:"complexity"`
+	Coverage   AnalyzerModuleConfig `yaml:"coverage"`
+	Secrets    AnalyzerModuleConfig `yaml:"secrets"`
+	Imports    AnalyzerModuleConfig `yaml:"imports"`
+	Patterns   AnalyzerModuleConfig `yaml:"patterns"`
+}
+
+// AnalyzerModuleConfig configures a single analyzer module.
+type AnalyzerModuleConfig struct {
+	Enabled   *bool `yaml:"enabled"`
+	Threshold int   `yaml:"threshold,omitempty"`
+}
+
+// IsEnabled reports whether this analyzer module is enabled.
+// Returns true by default if not explicitly set.
+func (a AnalyzerModuleConfig) IsEnabled() bool {
+	if a.Enabled == nil {
+		return true
+	}
+	return *a.Enabled
+}
+
+// OutputConfig controls report output settings.
+type OutputConfig struct {
+	Format  string `yaml:"format"`
+	Verbose bool   `yaml:"verbose"`
+}
+
+// CIConfig controls CI integration behavior.
+type CIConfig struct {
+	FailOn        string `yaml:"fail_on"`
+	Comment       bool   `yaml:"comment"`
+	CommentFormat string `yaml:"comment_format"`
+}
+
+// LoadConfig reads and parses a .shipsafe.yml configuration file.
+// If path is empty, it looks for .shipsafe.yml in the current directory.
+// If the default config file is not found, sensible defaults are returned.
+// If an explicitly specified config file is not found, an error is returned.
+func LoadConfig(path string) (*Config, error) {
+	useDefault := path == ""
+	if useDefault {
+		path = ".shipsafe.yml"
+	}
+
+	data, err := os.ReadFile(path)
+	if err != nil {
+		if os.IsNotExist(err) && useDefault {
+			return DefaultConfig(), nil
+		}
+		return nil, fmt.Errorf("cli: reading config %s: %w", path, err)
+	}
+
+	cfg := &Config{}
+	if err := yaml.Unmarshal(data, cfg); err != nil {
+		return nil, fmt.Errorf("cli: parsing config %s: %w", path, err)
+	}
+
+	applyDefaults(cfg)
+	return cfg, nil
+}
+
+// DefaultConfig returns a Config with sensible defaults matching the documented
+// .shipsafe.yml schema.
+func DefaultConfig() *Config {
+	cfg := &Config{Version: "1"}
+	applyDefaults(cfg)
+	return cfg
+}
+
+// applyDefaults fills in zero-value fields with sensible defaults.
+func applyDefaults(cfg *Config) {
+	if cfg.Thresholds.Green == 0 {
+		cfg.Thresholds.Green = 80
+	}
+	if cfg.Thresholds.Yellow == 0 {
+		cfg.Thresholds.Yellow = 50
+	}
+	if cfg.Analyzers.Complexity.Threshold == 0 {
+		cfg.Analyzers.Complexity.Threshold = 15
+	}
+	if cfg.Output.Format == "" {
+		cfg.Output.Format = "terminal"
+	}
+	if cfg.CI.FailOn == "" {
+		cfg.CI.FailOn = "red"
+	}
+	if cfg.CI.CommentFormat == "" {
+		cfg.CI.CommentFormat = "markdown"
+	}
+}
diff --git a/types.go b/pkg/interfaces/types.go
similarity index 100%
rename from types.go
rename to pkg/interfaces/types.go
diff --git a/vcs.go b/pkg/interfaces/vcs.go
similarity index 100%
rename from vcs.go
rename to pkg/interfaces/vcs.go
diff --git a/pkg/report/generator.go b/pkg/report/generator.go
new file mode 100644
index 0000000..3a0a1c0
--- /dev/null
+++ b/pkg/report/generator.go
@@ -0,0 +1,129 @@
+// Package report generates verification reports from analysis results and trust scores.
+package report
+
+import (
+	"crypto/rand"
+	"fmt"
+	"sort"
+	"time"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// severityOrder defines the sort priority for findings (critical first).
+var severityOrder = map[interfaces.Severity]int{
+	interfaces.SeverityCritical: 0,
+	interfaces.SeverityHigh:     1,
+	interfaces.SeverityMedium:   2,
+	interfaces.SeverityLow:      3,
+	interfaces.SeverityInfo:     4,
+}
+
+// Generator builds reports from analysis results and trust scores.
+type Generator struct{}
+
+// NewGenerator creates a report generator.
+func NewGenerator() *Generator {
+	return &Generator{}
+}
+
+// Generate produces a Report from analysis results, a trust score, and the original diff.
+func (g *Generator) Generate(results []*interfaces.AnalysisResult, score *interfaces.TrustScore, diff *interfaces.Diff) *interfaces.Report {
+	start := time.Now()
+
+	findings := collectFindings(results)
+	sortFindingsBySeverity(findings)
+
+	meta := buildDiffMetadata(diff)
+	summary := buildSummary(score, findings)
+
+	return &interfaces.Report{
+		ID:         generateID(),
+		Timestamp:  time.Now(),
+		TrustScore: *score,
+		Findings:   findings,
+		Summary:    summary,
+		DiffMeta:   meta,
+		Duration:   time.Since(start),
+	}
+}
+
+// collectFindings merges findings from all analysis results.
+func collectFindings(results []*interfaces.AnalysisResult) []interfaces.Finding {
+	var all []interfaces.Finding
+	for _, r := range results {
+		if r == nil || r.Error != nil {
+			continue
+		}
+		all = append(all, r.Findings...)
+	}
+	return all
+}
+
+// sortFindingsBySeverity sorts findings with critical first, info last.
+func sortFindingsBySeverity(findings []interfaces.Finding) {
+	sort.Slice(findings, func(i, j int) bool {
+		oi := severityOrder[findings[i].Severity]
+		oj := severityOrder[findings[j].Severity]
+		if oi != oj {
+			return oi < oj
+		}
+		return findings[i].File < findings[j].File
+	})
+}
+
+// buildDiffMetadata calculates summary stats from the diff.
+func buildDiffMetadata(diff *interfaces.Diff) interfaces.DiffMetadata {
+	if diff == nil {
+		return interfaces.DiffMetadata{}
+	}
+
+	meta := interfaces.DiffMetadata{
+		FilesChanged: len(diff.Files),
+		BaseSHA:      diff.BaseSHA,
+		HeadSHA:      diff.HeadSHA,
+	}
+
+	for _, f := range diff.Files {
+		for _, h := range f.Hunks {
+			meta.Additions += len(h.AddedLines)
+			meta.Deletions += len(h.RemovedLines)
+		}
+	}
+
+	return meta
+}
+
+// buildSummary creates a one-line summary of the trust score and findings.
+func buildSummary(score *interfaces.TrustScore, findings []interfaces.Finding) string {
+	total := len(findings)
+	if total == 0 {
+		return fmt.Sprintf("Trust Score: %d/100 [%s] — no findings", score.Score, score.Rating)
+	}
+
+	parts := ""
+	for _, sev := range []interfaces.Severity{
+		interfaces.SeverityCritical,
+		interfaces.SeverityHigh,
+		interfaces.SeverityMedium,
+		interfaces.SeverityLow,
+		interfaces.SeverityInfo,
+	} {
+		count := score.FindingCount[sev]
+		if count > 0 {
+			if parts != "" {
+				parts += ", "
+			}
+			parts += fmt.Sprintf("%d %s", count, sev)
+		}
+	}
+
+	return fmt.Sprintf("Trust Score: %d/100 [%s] — %d findings (%s)", score.Score, score.Rating, total, parts)
+}
+
+// generateID creates a unique report identifier.
+func generateID() string {
+	b := make([]byte, 8)
+	_, _ = rand.Read(b) // best-effort; crypto/rand is reliable
+	return fmt.Sprintf("rpt-%x", b)
+}
diff --git a/pkg/report/json.go b/pkg/report/json.go
new file mode 100644
index 0000000..95433af
--- /dev/null
+++ b/pkg/report/json.go
@@ -0,0 +1,23 @@
+package report
+
+import (
+	"encoding/json"
+	"io"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// JSONFormatter writes a report as JSON.
+type JSONFormatter struct{}
+
+// NewJSONFormatter creates a JSON report formatter.
+func NewJSONFormatter() *JSONFormatter {
+	return &JSONFormatter{}
+}
+
+// Format writes the report as indented JSON to the given writer.
+func (f *JSONFormatter) Format(w io.Writer, report *interfaces.Report) error {
+	enc := json.NewEncoder(w)
+	enc.SetIndent("", "  ")
+	return enc.Encode(report)
+}
diff --git a/pkg/report/markdown.go b/pkg/report/markdown.go
new file mode 100644
index 0000000..0b7acc9
--- /dev/null
+++ b/pkg/report/markdown.go
@@ -0,0 +1,154 @@
+package report
+
+import (
+	"fmt"
+	"io"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// MarkdownFormatter writes a report as Markdown suitable for PR comments.
+type MarkdownFormatter struct{}
+
+// NewMarkdownFormatter creates a Markdown report formatter.
+func NewMarkdownFormatter() *MarkdownFormatter {
+	return &MarkdownFormatter{}
+}
+
+// Format writes the report as Markdown to the given writer.
+func (f *MarkdownFormatter) Format(w io.Writer, report *interfaces.Report) error {
+	f.writeHeader(w, report)
+	f.writeSummaryTable(w, report)
+	f.writeFindings(w, report)
+	f.writeFooter(w, report)
+	return nil
+}
+
+func (f *MarkdownFormatter) writeHeader(w io.Writer, report *interfaces.Report) {
+	badge := scoreBadge(report.TrustScore)
+	fmt.Fprintf(w, "# ShipSafe Verification Report %s\n\n", badge)
+}
+
+func (f *MarkdownFormatter) writeSummaryTable(w io.Writer, report *interfaces.Report) {
+	score := report.TrustScore
+	meta := report.DiffMeta
+
+	fmt.Fprintln(w, "| Metric | Value |")
+	fmt.Fprintln(w, "|--------|-------|")
+	fmt.Fprintf(w, "| **Trust Score** | %d/100 %s |\n", score.Score, scoreBadge(score))
+	fmt.Fprintf(w, "| **Rating** | %s |\n", score.Rating)
+	fmt.Fprintf(w, "| **Total Findings** | %d |\n", len(report.Findings))
+	fmt.Fprintf(w, "| **Files Changed** | %d |\n", meta.FilesChanged)
+	fmt.Fprintf(w, "| **Lines** | +%d / -%d |\n", meta.Additions, meta.Deletions)
+
+	if len(score.FindingCount) > 0 {
+		parts := formatFindingCounts(score.FindingCount)
+		fmt.Fprintf(w, "| **Breakdown** | %s |\n", parts)
+	}
+
+	fmt.Fprintln(w)
+}
+
+func (f *MarkdownFormatter) writeFindings(w io.Writer, report *interfaces.Report) {
+	if len(report.Findings) == 0 {
+		fmt.Fprintln(w, "> No findings — clean diff!")
+		fmt.Fprintln(w)
+		return
+	}
+
+	// Group by category.
+	grouped := groupByCategory(report.Findings)
+
+	for _, cat := range []interfaces.Category{
+		interfaces.CategorySecrets,
+		interfaces.CategorySecurity,
+		interfaces.CategoryLogic,
+		interfaces.CategoryComplexity,
+		interfaces.CategoryCoverage,
+		interfaces.CategoryPattern,
+		interfaces.CategoryImport,
+		interfaces.CategoryConvention,
+	} {
+		findings, ok := grouped[cat]
+		if !ok {
+			continue
+		}
+
+		fmt.Fprintf(w, "## %s (%d)\n\n", categoryTitle(cat), len(findings))
+
+		for _, finding := range findings {
+			location := finding.File
+			if finding.StartLine > 0 {
+				location = fmt.Sprintf("%s:%d", finding.File, finding.StartLine)
+			}
+
+			fmt.Fprintf(w, "<details>\n")
+			fmt.Fprintf(w, "<summary><strong>%s</strong> [%s] — <code>%s</code></summary>\n\n",
+				finding.Title, strings.ToUpper(string(finding.Severity)), location)
+
+			if finding.Description != "" {
+				fmt.Fprintf(w, "%s\n\n", finding.Description)
+			}
+			if finding.Suggestion != "" {
+				fmt.Fprintf(w, "**Suggestion:** %s\n\n", finding.Suggestion)
+			}
+			fmt.Fprintf(w, "*Source: %s | Confidence: %.0f%%*\n\n", finding.Source, finding.Confidence*100)
+			fmt.Fprintln(w, "</details>")
+			fmt.Fprintln(w)
+		}
+	}
+}
+
+func (f *MarkdownFormatter) writeFooter(w io.Writer, report *interfaces.Report) {
+	fmt.Fprintln(w, "---")
+	fmt.Fprintf(w, "*Report ID: %s | Generated: %s*\n",
+		report.ID, report.Timestamp.Format("2006-01-02 15:04:05"))
+}
+
+// scoreBadge returns a text badge based on the rating.
+func scoreBadge(score interfaces.TrustScore) string {
+	switch score.Rating {
+	case interfaces.RatingGreen:
+		return "🟢"
+	case interfaces.RatingYellow:
+		return "🟡"
+	case interfaces.RatingRed:
+		return "🔴"
+	default:
+		return "⚪"
+	}
+}
+
+// groupByCategory groups findings by their category.
+func groupByCategory(findings []interfaces.Finding) map[interfaces.Category][]interfaces.Finding {
+	grouped := make(map[interfaces.Category][]interfaces.Finding)
+	for _, f := range findings {
+		grouped[f.Category] = append(grouped[f.Category], f)
+	}
+	return grouped
+}
+
+// categoryTitle returns a human-readable title for a category.
+func categoryTitle(c interfaces.Category) string {
+	switch c {
+	case interfaces.CategorySecurity:
+		return "Security"
+	case interfaces.CategorySecrets:
+		return "Secrets"
+	case interfaces.CategoryLogic:
+		return "Logic"
+	case interfaces.CategoryComplexity:
+		return "Complexity"
+	case interfaces.CategoryCoverage:
+		return "Coverage"
+	case interfaces.CategoryPattern:
+		return "Patterns"
+	case interfaces.CategoryImport:
+		return "Imports"
+	case interfaces.CategoryConvention:
+		return "Conventions"
+	default:
+		return string(c)
+	}
+}
diff --git a/pkg/report/terminal.go b/pkg/report/terminal.go
new file mode 100644
index 0000000..f3c8d54
--- /dev/null
+++ b/pkg/report/terminal.go
@@ -0,0 +1,165 @@
+package report
+
+import (
+	"fmt"
+	"io"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// ANSI color codes for terminal output.
+const (
+	colorReset  = "\033[0m"
+	colorRed    = "\033[31m"
+	colorGreen  = "\033[32m"
+	colorYellow = "\033[33m"
+	colorCyan   = "\033[36m"
+	colorBold   = "\033[1m"
+	colorDim    = "\033[2m"
+)
+
+// TerminalFormatter writes a color-coded report to a terminal.
+type TerminalFormatter struct{}
+
+// NewTerminalFormatter creates a terminal report formatter.
+func NewTerminalFormatter() *TerminalFormatter {
+	return &TerminalFormatter{}
+}
+
+// Format writes the report to the given writer using ANSI colors.
+func (f *TerminalFormatter) Format(w io.Writer, report *interfaces.Report) error {
+	f.writeHeader(w, report)
+	f.writeSummary(w, report)
+	f.writeFindings(w, report)
+	f.writeFooter(w, report)
+	return nil
+}
+
+func (f *TerminalFormatter) writeHeader(w io.Writer, report *interfaces.Report) {
+	fmt.Fprintf(w, "\n%s%s══════════════════════════════════════════%s\n", colorBold, colorCyan, colorReset)
+	fmt.Fprintf(w, "%s%s  ShipSafe Verification Report%s\n", colorBold, colorCyan, colorReset)
+	fmt.Fprintf(w, "%s%s══════════════════════════════════════════%s\n\n", colorBold, colorCyan, colorReset)
+}
+
+func (f *TerminalFormatter) writeSummary(w io.Writer, report *interfaces.Report) {
+	score := report.TrustScore
+	color := ratingColor(score.Rating)
+
+	fmt.Fprintf(w, "  %s%sTrust Score: %d/100 [%s]%s\n\n",
+		colorBold, color, score.Score, score.Rating, colorReset)
+
+	total := len(report.Findings)
+	if total == 0 {
+		fmt.Fprintf(w, "  %sNo findings — clean diff!%s\n\n", colorGreen, colorReset)
+		return
+	}
+
+	parts := formatFindingCounts(score.FindingCount)
+	fmt.Fprintf(w, "  %d findings (%s)\n\n", total, parts)
+}
+
+func (f *TerminalFormatter) writeFindings(w io.Writer, report *interfaces.Report) {
+	if len(report.Findings) == 0 {
+		return
+	}
+
+	// Group findings by severity.
+	grouped := groupBySeverity(report.Findings)
+
+	for _, sev := range []interfaces.Severity{
+		interfaces.SeverityCritical,
+		interfaces.SeverityHigh,
+		interfaces.SeverityMedium,
+		interfaces.SeverityLow,
+		interfaces.SeverityInfo,
+	} {
+		findings, ok := grouped[sev]
+		if !ok {
+			continue
+		}
+
+		color := severityColor(sev)
+		label := strings.ToUpper(string(sev))
+		fmt.Fprintf(w, "  %s%s── %s (%d) ──%s\n", colorBold, color, label, len(findings), colorReset)
+
+		for _, finding := range findings {
+			location := finding.File
+			if finding.StartLine > 0 {
+				location = fmt.Sprintf("%s:%d", finding.File, finding.StartLine)
+			}
+			fmt.Fprintf(w, "    %s[%s]%s %s\n", color, finding.ID, colorReset, finding.Title)
+			fmt.Fprintf(w, "      %s%s%s\n", colorDim, location, colorReset)
+			if finding.Description != "" {
+				fmt.Fprintf(w, "      %s\n", finding.Description)
+			}
+			if finding.Suggestion != "" {
+				fmt.Fprintf(w, "      %s→ %s%s\n", colorCyan, finding.Suggestion, colorReset)
+			}
+			fmt.Fprintln(w)
+		}
+	}
+}
+
+func (f *TerminalFormatter) writeFooter(w io.Writer, report *interfaces.Report) {
+	meta := report.DiffMeta
+	fmt.Fprintf(w, "  %s%s──────────────────────────────────────────%s\n", colorDim, colorCyan, colorReset)
+	fmt.Fprintf(w, "  %sFiles: %d | +%d/-%d | Report: %s%s\n",
+		colorDim, meta.FilesChanged, meta.Additions, meta.Deletions, report.ID, colorReset)
+	fmt.Fprintf(w, "  %sGenerated: %s%s\n\n",
+		colorDim, report.Timestamp.Format("2006-01-02 15:04:05"), colorReset)
+}
+
+// ratingColor returns the ANSI color for a rating.
+func ratingColor(r interfaces.Rating) string {
+	switch r {
+	case interfaces.RatingGreen:
+		return colorGreen
+	case interfaces.RatingYellow:
+		return colorYellow
+	case interfaces.RatingRed:
+		return colorRed
+	default:
+		return colorReset
+	}
+}
+
+// severityColor returns the ANSI color for a severity level.
+func severityColor(s interfaces.Severity) string {
+	switch s {
+	case interfaces.SeverityCritical, interfaces.SeverityHigh:
+		return colorRed
+	case interfaces.SeverityMedium:
+		return colorYellow
+	case interfaces.SeverityLow, interfaces.SeverityInfo:
+		return colorDim
+	default:
+		return colorReset
+	}
+}
+
+// groupBySeverity groups findings by their severity.
+func groupBySeverity(findings []interfaces.Finding) map[interfaces.Severity][]interfaces.Finding {
+	grouped := make(map[interfaces.Severity][]interfaces.Finding)
+	for _, f := range findings {
+		grouped[f.Severity] = append(grouped[f.Severity], f)
+	}
+	return grouped
+}
+
+// formatFindingCounts produces a summary like "1 high, 4 medium, 7 low".
+func formatFindingCounts(counts map[interfaces.Severity]int) string {
+	var parts []string
+	for _, sev := range []interfaces.Severity{
+		interfaces.SeverityCritical,
+		interfaces.SeverityHigh,
+		interfaces.SeverityMedium,
+		interfaces.SeverityLow,
+		interfaces.SeverityInfo,
+	} {
+		if c := counts[sev]; c > 0 {
+			parts = append(parts, fmt.Sprintf("%d %s", c, sev))
+		}
+	}
+	return strings.Join(parts, ", ")
+}
diff --git a/pkg/scorer/calculator.go b/pkg/scorer/calculator.go
new file mode 100644
index 0000000..1717a77
--- /dev/null
+++ b/pkg/scorer/calculator.go
@@ -0,0 +1,102 @@
+package scorer
+
+import (
+	"math"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+// Calculator computes trust scores from analysis findings.
+type Calculator struct {
+	severityWeights     SeverityWeights
+	categoryMultipliers CategoryMultipliers
+	greenThreshold      int
+	yellowThreshold     int
+}
+
+// Option configures the Calculator.
+type Option func(*Calculator)
+
+// WithSeverityWeights overrides the default severity weights.
+func WithSeverityWeights(w SeverityWeights) Option {
+	return func(c *Calculator) {
+		c.severityWeights = w
+	}
+}
+
+// WithCategoryMultipliers overrides the default category multipliers.
+func WithCategoryMultipliers(m CategoryMultipliers) Option {
+	return func(c *Calculator) {
+		c.categoryMultipliers = m
+	}
+}
+
+// WithThresholds overrides the default GREEN/YELLOW thresholds.
+func WithThresholds(green, yellow int) Option {
+	return func(c *Calculator) {
+		c.greenThreshold = green
+		c.yellowThreshold = yellow
+	}
+}
+
+// NewCalculator creates a scorer with optional configuration.
+func NewCalculator(opts ...Option) *Calculator {
+	c := &Calculator{
+		severityWeights:     DefaultSeverityWeights(),
+		categoryMultipliers: DefaultCategoryMultipliers(),
+		greenThreshold:      DefaultGreenThreshold,
+		yellowThreshold:     DefaultYellowThreshold,
+	}
+	for _, opt := range opts {
+		opt(c)
+	}
+	return c
+}
+
+// Score computes a TrustScore from analysis results.
+// Formula: start at 100, subtract penalty per finding.
+// penalty = severity_weight * category_multiplier * confidence
+// Score is clamped to [0, 100].
+func (c *Calculator) Score(results []*interfaces.AnalysisResult) *interfaces.TrustScore {
+	breakdown := make(map[interfaces.Category]int)
+	findingCount := make(map[interfaces.Severity]int)
+
+	var totalPenalty float64
+
+	for _, result := range results {
+		if result == nil || result.Error != nil {
+			continue
+		}
+		for _, f := range result.Findings {
+			weight := c.severityWeights.SeverityWeight(f.Severity)
+			multiplier := c.categoryMultipliers.Multiplier(f.Category)
+			confidence := f.Confidence
+			if confidence <= 0 {
+				confidence = 1.0
+			}
+
+			penalty := float64(weight) * multiplier * confidence
+			totalPenalty += penalty
+
+			breakdown[f.Category] += int(math.Round(penalty))
+			findingCount[f.Severity]++
+		}
+	}
+
+	score := 100 - int(math.Round(totalPenalty))
+	if score < 0 {
+		score = 0
+	}
+	if score > 100 {
+		score = 100
+	}
+
+	rating := RatingFromScore(score, c.greenThreshold, c.yellowThreshold)
+
+	return &interfaces.TrustScore{
+		Score:        score,
+		Rating:       rating,
+		Breakdown:    breakdown,
+		FindingCount: findingCount,
+	}
+}
diff --git a/pkg/scorer/calculator_test.go b/pkg/scorer/calculator_test.go
new file mode 100644
index 0000000..be656e5
--- /dev/null
+++ b/pkg/scorer/calculator_test.go
@@ -0,0 +1,368 @@
+package scorer
+
+import (
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+func TestCalculator_NoFindings_Score100Green(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{AnalyzerName: "complexity"},
+		{AnalyzerName: "secrets"},
+	}
+
+	ts := calc.Score(results)
+	if ts.Score != 100 {
+		t.Errorf("expected score 100, got %d", ts.Score)
+	}
+	if ts.Rating != interfaces.RatingGreen {
+		t.Errorf("expected GREEN rating, got %s", ts.Rating)
+	}
+}
+
+func TestCalculator_OneCriticalSecurityFinding_LargeDropLikelyRed(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "security",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategorySecurity,
+					Severity:   interfaces.SeverityCritical,
+					Confidence: 1.0,
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+
+	// penalty = 25 * 1.5 * 1.0 = 37.5 → score = 100 - 38 = 62
+	// But a critical secret would be even worse: 25 * 2.0 = 50.
+	// A single critical security finding should cause a significant drop.
+	if ts.Score >= 80 {
+		t.Errorf("expected score below 80 for critical security finding, got %d", ts.Score)
+	}
+	if ts.FindingCount[interfaces.SeverityCritical] != 1 {
+		t.Errorf("expected 1 critical finding, got %d", ts.FindingCount[interfaces.SeverityCritical])
+	}
+}
+
+func TestCalculator_OneCriticalSecretsFinding_Red(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "secrets",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategorySecrets,
+					Severity:   interfaces.SeverityCritical,
+					Confidence: 1.0,
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+
+	// penalty = 25 * 2.0 * 1.0 = 50 → score = 50
+	// Score 50 is YELLOW boundary. Let's verify it's at most YELLOW.
+	if ts.Score > 50 {
+		t.Errorf("expected score <= 50 for critical secrets finding, got %d", ts.Score)
+	}
+}
+
+func TestCalculator_SeveralLowFindings_StillGreen(t *testing.T) {
+	calc := NewCalculator()
+	findings := make([]interfaces.Finding, 5)
+	for i := range findings {
+		findings[i] = interfaces.Finding{
+			Category:   interfaces.CategoryConvention,
+			Severity:   interfaces.SeverityLow,
+			Confidence: 0.8,
+		}
+	}
+
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "patterns",
+			Findings:     findings,
+		},
+	}
+
+	ts := calc.Score(results)
+
+	// penalty per finding = 3 * 0.3 * 0.8 = 0.72 → 5 findings = 3.6 → score ~96
+	if ts.Rating != interfaces.RatingGreen {
+		t.Errorf("expected GREEN rating for low-severity findings, got %s (score: %d)", ts.Rating, ts.Score)
+	}
+	if ts.Score < 90 {
+		t.Errorf("expected score above 90 for low-severity convention findings, got %d", ts.Score)
+	}
+}
+
+func TestCalculator_MixOfFindings_Yellow(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "security",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategorySecurity,
+					Severity:   interfaces.SeverityHigh,
+					Confidence: 0.9,
+				},
+			},
+		},
+		{
+			AnalyzerName: "complexity",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategoryComplexity,
+					Severity:   interfaces.SeverityMedium,
+					Confidence: 0.8,
+				},
+				{
+					Category:   interfaces.CategoryComplexity,
+					Severity:   interfaces.SeverityMedium,
+					Confidence: 0.7,
+				},
+			},
+		},
+		{
+			AnalyzerName: "patterns",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategoryPattern,
+					Severity:   interfaces.SeverityLow,
+					Confidence: 0.6,
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+
+	// high security: 15 * 1.5 * 0.9 = 20.25
+	// medium complexity: 8 * 0.8 * 0.8 = 5.12
+	// medium complexity: 8 * 0.8 * 0.7 = 4.48
+	// low pattern: 3 * 0.5 * 0.6 = 0.9
+	// total penalty = ~30.75 → score ~69 → YELLOW
+	if ts.Rating != interfaces.RatingYellow {
+		t.Errorf("expected YELLOW rating for mixed findings, got %s (score: %d)", ts.Rating, ts.Score)
+	}
+}
+
+func TestCalculator_ScoreNeverBelowZero(t *testing.T) {
+	calc := NewCalculator()
+
+	// Create many critical security findings to overwhelm the score.
+	findings := make([]interfaces.Finding, 20)
+	for i := range findings {
+		findings[i] = interfaces.Finding{
+			Category:   interfaces.CategorySecrets,
+			Severity:   interfaces.SeverityCritical,
+			Confidence: 1.0,
+		}
+	}
+
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "secrets",
+			Findings:     findings,
+		},
+	}
+
+	ts := calc.Score(results)
+
+	// penalty = 20 * 25 * 2.0 * 1.0 = 1000 → score clamped to 0
+	if ts.Score < 0 {
+		t.Errorf("score must never be negative, got %d", ts.Score)
+	}
+	if ts.Score != 0 {
+		t.Errorf("expected score 0 for massive penalty, got %d", ts.Score)
+	}
+	if ts.Rating != interfaces.RatingRed {
+		t.Errorf("expected RED rating, got %s", ts.Rating)
+	}
+}
+
+func TestCalculator_SkipsErroredResults(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "secrets",
+			Error:        errTestAnalyzer,
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategorySecrets,
+					Severity:   interfaces.SeverityCritical,
+					Confidence: 1.0,
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	if ts.Score != 100 {
+		t.Errorf("expected score 100 when all results errored, got %d", ts.Score)
+	}
+}
+
+func TestCalculator_SkipsNilResults(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{nil, nil}
+	ts := calc.Score(results)
+	if ts.Score != 100 {
+		t.Errorf("expected score 100 for nil results, got %d", ts.Score)
+	}
+}
+
+func TestCalculator_BreakdownPerCategory(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "mixed",
+			Findings: []interfaces.Finding{
+				{Category: interfaces.CategorySecurity, Severity: interfaces.SeverityHigh, Confidence: 1.0},
+				{Category: interfaces.CategoryComplexity, Severity: interfaces.SeverityMedium, Confidence: 1.0},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	if _, ok := ts.Breakdown[interfaces.CategorySecurity]; !ok {
+		t.Error("expected security category in breakdown")
+	}
+	if _, ok := ts.Breakdown[interfaces.CategoryComplexity]; !ok {
+		t.Error("expected complexity category in breakdown")
+	}
+}
+
+func TestCalculator_FindingCountBySeverity(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "mixed",
+			Findings: []interfaces.Finding{
+				{Category: interfaces.CategorySecurity, Severity: interfaces.SeverityCritical, Confidence: 1.0},
+				{Category: interfaces.CategoryComplexity, Severity: interfaces.SeverityMedium, Confidence: 1.0},
+				{Category: interfaces.CategoryPattern, Severity: interfaces.SeverityMedium, Confidence: 1.0},
+				{Category: interfaces.CategoryConvention, Severity: interfaces.SeverityLow, Confidence: 1.0},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	if ts.FindingCount[interfaces.SeverityCritical] != 1 {
+		t.Errorf("expected 1 critical, got %d", ts.FindingCount[interfaces.SeverityCritical])
+	}
+	if ts.FindingCount[interfaces.SeverityMedium] != 2 {
+		t.Errorf("expected 2 medium, got %d", ts.FindingCount[interfaces.SeverityMedium])
+	}
+	if ts.FindingCount[interfaces.SeverityLow] != 1 {
+		t.Errorf("expected 1 low, got %d", ts.FindingCount[interfaces.SeverityLow])
+	}
+}
+
+func TestCalculator_ZeroConfidenceTreatedAsOne(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "security",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategorySecurity,
+					Severity:   interfaces.SeverityHigh,
+					Confidence: 0.0, // zero confidence should be treated as 1.0
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	// penalty = 15 * 1.5 * 1.0 = 22.5 → score = 78
+	if ts.Score >= 80 {
+		t.Errorf("expected score < 80 when zero confidence is treated as 1.0, got %d", ts.Score)
+	}
+}
+
+func TestCalculator_CustomWeights(t *testing.T) {
+	calc := NewCalculator(
+		WithSeverityWeights(SeverityWeights{
+			interfaces.SeverityCritical: 50, // double the default
+		}),
+	)
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "security",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategorySecurity,
+					Severity:   interfaces.SeverityCritical,
+					Confidence: 1.0,
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	// penalty = 50 * 1.5 * 1.0 = 75 → score = 25
+	if ts.Score > 30 {
+		t.Errorf("expected score <= 30 with doubled critical weight, got %d", ts.Score)
+	}
+}
+
+func TestCalculator_CustomThresholds(t *testing.T) {
+	calc := NewCalculator(WithThresholds(90, 70))
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "patterns",
+			Findings: []interfaces.Finding{
+				{
+					Category:   interfaces.CategoryPattern,
+					Severity:   interfaces.SeverityMedium,
+					Confidence: 1.0,
+				},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	// penalty = 8 * 0.5 * 1.0 = 4 → score = 96
+	// With green threshold 90, score 96 is GREEN.
+	if ts.Rating != interfaces.RatingGreen {
+		t.Errorf("expected GREEN with custom thresholds, got %s (score: %d)", ts.Rating, ts.Score)
+	}
+}
+
+func TestCalculator_InfoFindingsNoPenalty(t *testing.T) {
+	calc := NewCalculator()
+	results := []*interfaces.AnalysisResult{
+		{
+			AnalyzerName: "patterns",
+			Findings: []interfaces.Finding{
+				{Category: interfaces.CategoryConvention, Severity: interfaces.SeverityInfo, Confidence: 1.0},
+				{Category: interfaces.CategoryConvention, Severity: interfaces.SeverityInfo, Confidence: 1.0},
+				{Category: interfaces.CategoryConvention, Severity: interfaces.SeverityInfo, Confidence: 1.0},
+			},
+		},
+	}
+
+	ts := calc.Score(results)
+	if ts.Score != 100 {
+		t.Errorf("info findings should carry zero penalty, expected 100 got %d", ts.Score)
+	}
+	if ts.FindingCount[interfaces.SeverityInfo] != 3 {
+		t.Errorf("expected 3 info findings counted, got %d", ts.FindingCount[interfaces.SeverityInfo])
+	}
+}
+
+// sentinel error for tests
+var errTestAnalyzer = errorString("test: analyzer failed")
+
+type errorString string
+
+func (e errorString) Error() string { return string(e) }
diff --git a/pkg/scorer/thresholds.go b/pkg/scorer/thresholds.go
new file mode 100644
index 0000000..25318d7
--- /dev/null
+++ b/pkg/scorer/thresholds.go
@@ -0,0 +1,24 @@
+package scorer
+
+import "github.com/toyinlola/shipsafe/pkg/interfaces"
+
+// Default threshold values.
+const (
+	DefaultGreenThreshold  = 80
+	DefaultYellowThreshold = 50
+)
+
+// RatingFromScore returns the trust rating for a given score based on thresholds.
+// GREEN: score >= greenThreshold
+// YELLOW: score >= yellowThreshold
+// RED: score < yellowThreshold
+func RatingFromScore(score int, greenThreshold int, yellowThreshold int) interfaces.Rating {
+	switch {
+	case score >= greenThreshold:
+		return interfaces.RatingGreen
+	case score >= yellowThreshold:
+		return interfaces.RatingYellow
+	default:
+		return interfaces.RatingRed
+	}
+}
diff --git a/pkg/scorer/weights.go b/pkg/scorer/weights.go
new file mode 100644
index 0000000..0a32c43
--- /dev/null
+++ b/pkg/scorer/weights.go
@@ -0,0 +1,72 @@
+// Package scorer calculates trust scores from analysis findings.
+package scorer
+
+import "github.com/toyinlola/shipsafe/pkg/interfaces"
+
+// Default severity weights define the base penalty points for each severity level.
+const (
+	DefaultWeightCritical = 25
+	DefaultWeightHigh     = 15
+	DefaultWeightMedium   = 8
+	DefaultWeightLow      = 3
+	DefaultWeightInfo     = 0
+)
+
+// Default category multipliers amplify or reduce penalties based on finding category.
+const (
+	DefaultMultiplierSecurity   = 1.5
+	DefaultMultiplierSecrets    = 2.0
+	DefaultMultiplierLogic      = 1.3
+	DefaultMultiplierComplexity = 0.8
+	DefaultMultiplierCoverage   = 0.7
+	DefaultMultiplierPattern    = 0.5
+	DefaultMultiplierImport     = 0.6
+	DefaultMultiplierConvention = 0.3
+)
+
+// SeverityWeights maps severity levels to their base penalty points.
+type SeverityWeights map[interfaces.Severity]int
+
+// CategoryMultipliers maps categories to their penalty multipliers.
+type CategoryMultipliers map[interfaces.Category]float64
+
+// DefaultSeverityWeights returns the default severity weight map.
+func DefaultSeverityWeights() SeverityWeights {
+	return SeverityWeights{
+		interfaces.SeverityCritical: DefaultWeightCritical,
+		interfaces.SeverityHigh:     DefaultWeightHigh,
+		interfaces.SeverityMedium:   DefaultWeightMedium,
+		interfaces.SeverityLow:      DefaultWeightLow,
+		interfaces.SeverityInfo:     DefaultWeightInfo,
+	}
+}
+
+// DefaultCategoryMultipliers returns the default category multiplier map.
+func DefaultCategoryMultipliers() CategoryMultipliers {
+	return CategoryMultipliers{
+		interfaces.CategorySecurity:   DefaultMultiplierSecurity,
+		interfaces.CategorySecrets:    DefaultMultiplierSecrets,
+		interfaces.CategoryLogic:      DefaultMultiplierLogic,
+		interfaces.CategoryComplexity: DefaultMultiplierComplexity,
+		interfaces.CategoryCoverage:   DefaultMultiplierCoverage,
+		interfaces.CategoryPattern:    DefaultMultiplierPattern,
+		interfaces.CategoryImport:     DefaultMultiplierImport,
+		interfaces.CategoryConvention: DefaultMultiplierConvention,
+	}
+}
+
+// SeverityWeight returns the penalty weight for a severity, falling back to 0 for unknown levels.
+func (w SeverityWeights) SeverityWeight(s interfaces.Severity) int {
+	if v, ok := w[s]; ok {
+		return v
+	}
+	return 0
+}
+
+// Multiplier returns the multiplier for a category, falling back to 1.0 for unknown categories.
+func (m CategoryMultipliers) Multiplier(c interfaces.Category) float64 {
+	if v, ok := m[c]; ok {
+		return v
+	}
+	return 1.0
+}
diff --git a/pkg/vcs/diff.go b/pkg/vcs/diff.go
new file mode 100644
index 0000000..bbbe17a
--- /dev/null
+++ b/pkg/vcs/diff.go
@@ -0,0 +1,338 @@
+// Package vcs provides version control integration for ShipSafe.
+// It handles diff parsing, git operations, and VCS platform API clients.
+package vcs
+
+import (
+	"bufio"
+	"bytes"
+	"context"
+	"errors"
+	"fmt"
+	"os"
+	"path/filepath"
+	"regexp"
+	"strconv"
+	"strings"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+var (
+	ErrEmptyDiff   = errors.New("vcs: empty diff input")
+	ErrInvalidDiff = errors.New("vcs: invalid diff format")
+)
+
+var (
+	diffHeaderRegex = regexp.MustCompile(`^diff --git a/(.+) b/(.+)$`)
+	hunkHeaderRegex = regexp.MustCompile(`^@@ -(\d+)(?:,(\d+))? \+(\d+)(?:,(\d+))? @@`)
+	binaryFileRegex = regexp.MustCompile(`^Binary files .+ and .+ differ$`)
+)
+
+// extToLanguage maps file extensions to language identifiers.
+var extToLanguage = map[string]string{
+	".go":     "go",
+	".py":     "python",
+	".js":     "javascript",
+	".ts":     "typescript",
+	".tsx":    "typescriptreact",
+	".jsx":    "javascriptreact",
+	".java":   "java",
+	".rs":     "rust",
+	".rb":     "ruby",
+	".c":      "c",
+	".cpp":    "cpp",
+	".cc":     "cpp",
+	".h":      "c",
+	".hpp":    "cpp",
+	".cs":     "csharp",
+	".swift":  "swift",
+	".kt":     "kotlin",
+	".php":    "php",
+	".sh":     "shell",
+	".bash":   "shell",
+	".zsh":    "shell",
+	".yml":    "yaml",
+	".yaml":   "yaml",
+	".json":   "json",
+	".xml":    "xml",
+	".html":   "html",
+	".css":    "css",
+	".scss":   "scss",
+	".sql":    "sql",
+	".md":     "markdown",
+	".toml":   "toml",
+	".ini":    "ini",
+	".tf":     "terraform",
+	".proto":  "protobuf",
+	".lua":    "lua",
+	".ex":     "elixir",
+	".exs":    "elixir",
+	".erl":    "erlang",
+	".hs":     "haskell",
+	".scala":  "scala",
+	".pl":     "perl",
+	".pm":     "perl",
+	".r":      "r",
+	".R":      "r",
+}
+
+// nameToLanguage maps special filenames to language identifiers.
+var nameToLanguage = map[string]string{
+	"Dockerfile":   "dockerfile",
+	"Makefile":     "makefile",
+	"Jenkinsfile":  "groovy",
+	"Vagrantfile":  "ruby",
+	"Gemfile":      "ruby",
+	"Rakefile":     "ruby",
+	".gitignore":   "gitignore",
+	".dockerignore": "dockerignore",
+}
+
+// detectLanguage returns the language name for a file path based on extension or filename.
+func detectLanguage(path string) string {
+	base := filepath.Base(path)
+	if lang, ok := nameToLanguage[base]; ok {
+		return lang
+	}
+	ext := filepath.Ext(path)
+	if lang, ok := extToLanguage[ext]; ok {
+		return lang
+	}
+	return ""
+}
+
+// diffParser implements the interfaces.DiffParser interface.
+type diffParser struct{}
+
+// NewDiffParser creates a new DiffParser that handles unified diff format.
+func NewDiffParser() interfaces.DiffParser {
+	return &diffParser{}
+}
+
+func (p *diffParser) Parse(ctx context.Context, raw []byte) (*interfaces.Diff, error) {
+	if len(bytes.TrimSpace(raw)) == 0 {
+		return nil, ErrEmptyDiff
+	}
+
+	diff := &interfaces.Diff{}
+	scanner := bufio.NewScanner(bytes.NewReader(raw))
+	var current *fileState
+	var files []interfaces.FileDiff
+
+	for scanner.Scan() {
+		select {
+		case <-ctx.Done():
+			return nil, fmt.Errorf("vcs: parsing cancelled: %w", ctx.Err())
+		default:
+		}
+
+		line := scanner.Text()
+
+		// Start of a new file diff
+		if matches := diffHeaderRegex.FindStringSubmatch(line); matches != nil {
+			if current != nil {
+				files = append(files, current.toFileDiff())
+			}
+			current = &fileState{
+				gitOldPath: matches[1],
+				gitNewPath: matches[2],
+			}
+			continue
+		}
+
+		if current == nil {
+			continue
+		}
+
+		switch {
+		case strings.HasPrefix(line, "--- "):
+			current.minusHeader = strings.TrimPrefix(line, "--- ")
+		case strings.HasPrefix(line, "+++ "):
+			current.plusHeader = strings.TrimPrefix(line, "+++ ")
+		case strings.HasPrefix(line, "rename from "):
+			current.renameFrom = strings.TrimPrefix(line, "rename from ")
+		case strings.HasPrefix(line, "rename to "):
+			current.renameTo = strings.TrimPrefix(line, "rename to ")
+		case strings.HasPrefix(line, "new file mode"):
+			current.newFile = true
+		case strings.HasPrefix(line, "deleted file mode"):
+			current.deletedFile = true
+		case binaryFileRegex.MatchString(line):
+			current.binary = true
+		case hunkHeaderRegex.MatchString(line):
+			current.beginHunk(line)
+		case strings.HasPrefix(line, "\\"):
+			// "\ No newline at end of file" — skip
+		case current.hasActiveHunk():
+			current.appendLine(line)
+		}
+	}
+
+	if err := scanner.Err(); err != nil {
+		return nil, fmt.Errorf("vcs: reading diff: %w", err)
+	}
+
+	if current != nil {
+		files = append(files, current.toFileDiff())
+	}
+
+	if len(files) == 0 {
+		return nil, ErrInvalidDiff
+	}
+
+	diff.Files = files
+	return diff, nil
+}
+
+func (p *diffParser) ParseFile(ctx context.Context, path string) (*interfaces.Diff, error) {
+	data, err := os.ReadFile(path)
+	if err != nil {
+		return nil, fmt.Errorf("vcs: reading diff file %s: %w", path, err)
+	}
+	return p.Parse(ctx, data)
+}
+
+// fileState holds mutable state while parsing a single file's diff.
+type fileState struct {
+	gitOldPath  string
+	gitNewPath  string
+	minusHeader string
+	plusHeader   string
+	renameFrom  string
+	renameTo    string
+	newFile     bool
+	deletedFile bool
+	binary      bool
+	hunks       []hunkState
+}
+
+// hunkState holds mutable state while parsing a single hunk.
+type hunkState struct {
+	oldStart int
+	oldLines int
+	newStart int
+	newLines int
+	lines    []string
+}
+
+func (f *fileState) beginHunk(line string) {
+	matches := hunkHeaderRegex.FindStringSubmatch(line)
+	if matches == nil {
+		return
+	}
+
+	hs := hunkState{}
+	hs.oldStart, _ = strconv.Atoi(matches[1])
+	if matches[2] != "" {
+		hs.oldLines, _ = strconv.Atoi(matches[2])
+	} else {
+		hs.oldLines = 1
+	}
+	hs.newStart, _ = strconv.Atoi(matches[3])
+	if matches[4] != "" {
+		hs.newLines, _ = strconv.Atoi(matches[4])
+	} else {
+		hs.newLines = 1
+	}
+
+	f.hunks = append(f.hunks, hs)
+}
+
+func (f *fileState) hasActiveHunk() bool {
+	return len(f.hunks) > 0
+}
+
+func (f *fileState) appendLine(line string) {
+	if len(f.hunks) == 0 {
+		return
+	}
+	f.hunks[len(f.hunks)-1].lines = append(f.hunks[len(f.hunks)-1].lines, line)
+}
+
+func (f *fileState) status() interfaces.FileStatus {
+	if f.renameFrom != "" || f.renameTo != "" {
+		return interfaces.FileRenamed
+	}
+	if f.newFile || f.minusHeader == "/dev/null" {
+		return interfaces.FileAdded
+	}
+	if f.deletedFile || f.plusHeader == "/dev/null" {
+		return interfaces.FileDeleted
+	}
+	return interfaces.FileModified
+}
+
+func (f *fileState) toFileDiff() interfaces.FileDiff {
+	st := f.status()
+	fd := interfaces.FileDiff{
+		Status:   st,
+		IsBinary: f.binary,
+	}
+
+	switch st {
+	case interfaces.FileAdded:
+		fd.Path = f.gitNewPath
+	case interfaces.FileDeleted:
+		fd.Path = f.gitOldPath
+	case interfaces.FileRenamed:
+		if f.renameTo != "" {
+			fd.Path = f.renameTo
+			fd.OldPath = f.renameFrom
+		} else {
+			fd.Path = f.gitNewPath
+			fd.OldPath = f.gitOldPath
+		}
+	default:
+		fd.Path = f.gitNewPath
+	}
+
+	fd.Language = detectLanguage(fd.Path)
+
+	for _, hs := range f.hunks {
+		fd.Hunks = append(fd.Hunks, hs.toHunk())
+	}
+
+	return fd
+}
+
+func (hs *hunkState) toHunk() interfaces.Hunk {
+	hunk := interfaces.Hunk{
+		OldStart: hs.oldStart,
+		OldLines: hs.oldLines,
+		NewStart: hs.newStart,
+		NewLines: hs.newLines,
+		Content:  strings.Join(hs.lines, "\n"),
+	}
+
+	oldLine := hs.oldStart
+	newLine := hs.newStart
+
+	for _, line := range hs.lines {
+		if len(line) == 0 {
+			// Blank context line (some git versions omit the leading space)
+			oldLine++
+			newLine++
+			continue
+		}
+
+		switch line[0] {
+		case '+':
+			hunk.AddedLines = append(hunk.AddedLines, interfaces.Line{
+				Number:  newLine,
+				Content: line[1:],
+			})
+			newLine++
+		case '-':
+			hunk.RemovedLines = append(hunk.RemovedLines, interfaces.Line{
+				Number:  oldLine,
+				Content: line[1:],
+			})
+			oldLine++
+		default: // context line (starts with space)
+			oldLine++
+			newLine++
+		}
+	}
+
+	return hunk
+}
diff --git a/pkg/vcs/diff_test.go b/pkg/vcs/diff_test.go
new file mode 100644
index 0000000..d879982
--- /dev/null
+++ b/pkg/vcs/diff_test.go
@@ -0,0 +1,504 @@
+package vcs
+
+import (
+	"context"
+	"os"
+	"path/filepath"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+)
+
+func TestDiffParser_Parse(t *testing.T) {
+	tests := []struct {
+		name  string
+		input string
+		check func(t *testing.T, diff *interfaces.Diff)
+	}{
+		{
+			name: "single file modification",
+			input: `diff --git a/main.go b/main.go
+index abc1234..def5678 100644
+--- a/main.go
++++ b/main.go
+@@ -10,7 +10,8 @@ func main() {
+ 	fmt.Println("hello")
+-	fmt.Println("old")
++	fmt.Println("new")
++	fmt.Println("extra")
+ 	fmt.Println("world")
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				if len(diff.Files) != 1 {
+					t.Fatalf("expected 1 file, got %d", len(diff.Files))
+				}
+				f := diff.Files[0]
+				assertEqual(t, "path", "main.go", f.Path)
+				assertEqual(t, "status", interfaces.FileModified, f.Status)
+				assertEqual(t, "language", "go", f.Language)
+				assertFalse(t, "binary", f.IsBinary)
+
+				if len(f.Hunks) != 1 {
+					t.Fatalf("expected 1 hunk, got %d", len(f.Hunks))
+				}
+				h := f.Hunks[0]
+				assertIntEqual(t, "old_start", 10, h.OldStart)
+				assertIntEqual(t, "old_lines", 7, h.OldLines)
+				assertIntEqual(t, "new_start", 10, h.NewStart)
+				assertIntEqual(t, "new_lines", 8, h.NewLines)
+				assertIntEqual(t, "added_count", 2, len(h.AddedLines))
+				assertIntEqual(t, "removed_count", 1, len(h.RemovedLines))
+
+				// Verify line numbers
+				assertIntEqual(t, "removed_line_number", 11, h.RemovedLines[0].Number)
+				assertEqual(t, "removed_content", "\tfmt.Println(\"old\")", h.RemovedLines[0].Content)
+				assertIntEqual(t, "added_line_1_number", 11, h.AddedLines[0].Number)
+				assertIntEqual(t, "added_line_2_number", 12, h.AddedLines[1].Number)
+			},
+		},
+		{
+			name: "multi-file diff",
+			input: `diff --git a/foo.go b/foo.go
+index 1111111..2222222 100644
+--- a/foo.go
++++ b/foo.go
+@@ -1,3 +1,4 @@
+ package main
++// added comment
+
+ func foo() {}
+diff --git a/bar.py b/bar.py
+index 3333333..4444444 100644
+--- a/bar.py
++++ b/bar.py
+@@ -5,6 +5,7 @@ def bar():
+     pass
+
++# new line
+ def baz():
+     pass
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				if len(diff.Files) != 2 {
+					t.Fatalf("expected 2 files, got %d", len(diff.Files))
+				}
+				assertEqual(t, "file1_path", "foo.go", diff.Files[0].Path)
+				assertEqual(t, "file1_lang", "go", diff.Files[0].Language)
+				assertEqual(t, "file2_path", "bar.py", diff.Files[1].Path)
+				assertEqual(t, "file2_lang", "python", diff.Files[1].Language)
+
+				assertIntEqual(t, "file1_added", 1, len(diff.Files[0].Hunks[0].AddedLines))
+				assertIntEqual(t, "file2_added", 1, len(diff.Files[1].Hunks[0].AddedLines))
+			},
+		},
+		{
+			name: "new file",
+			input: `diff --git a/new_file.go b/new_file.go
+new file mode 100644
+index 0000000..abc1234
+--- /dev/null
++++ b/new_file.go
+@@ -0,0 +1,5 @@
++package main
++
++func hello() {
++	fmt.Println("hi")
++}
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				if len(diff.Files) != 1 {
+					t.Fatalf("expected 1 file, got %d", len(diff.Files))
+				}
+				f := diff.Files[0]
+				assertEqual(t, "path", "new_file.go", f.Path)
+				assertEqual(t, "status", interfaces.FileAdded, f.Status)
+				assertIntEqual(t, "hunks", 1, len(f.Hunks))
+
+				h := f.Hunks[0]
+				assertIntEqual(t, "old_start", 0, h.OldStart)
+				assertIntEqual(t, "old_lines", 0, h.OldLines)
+				assertIntEqual(t, "new_start", 1, h.NewStart)
+				assertIntEqual(t, "new_lines", 5, h.NewLines)
+				assertIntEqual(t, "added_count", 5, len(h.AddedLines))
+				assertIntEqual(t, "removed_count", 0, len(h.RemovedLines))
+
+				// First added line should be at line 1
+				assertIntEqual(t, "first_line_number", 1, h.AddedLines[0].Number)
+				assertEqual(t, "first_line_content", "package main", h.AddedLines[0].Content)
+			},
+		},
+		{
+			name: "deleted file",
+			input: `diff --git a/old_file.go b/old_file.go
+deleted file mode 100644
+index abc1234..0000000
+--- a/old_file.go
++++ /dev/null
+@@ -1,3 +0,0 @@
+-package main
+-
+-func old() {}
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				if len(diff.Files) != 1 {
+					t.Fatalf("expected 1 file, got %d", len(diff.Files))
+				}
+				f := diff.Files[0]
+				assertEqual(t, "path", "old_file.go", f.Path)
+				assertEqual(t, "status", interfaces.FileDeleted, f.Status)
+
+				h := f.Hunks[0]
+				assertIntEqual(t, "added_count", 0, len(h.AddedLines))
+				assertIntEqual(t, "removed_count", 3, len(h.RemovedLines))
+				assertIntEqual(t, "first_removed_number", 1, h.RemovedLines[0].Number)
+			},
+		},
+		{
+			name: "renamed file",
+			input: `diff --git a/old_name.go b/new_name.go
+similarity index 100%
+rename from old_name.go
+rename to new_name.go
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				if len(diff.Files) != 1 {
+					t.Fatalf("expected 1 file, got %d", len(diff.Files))
+				}
+				f := diff.Files[0]
+				assertEqual(t, "path", "new_name.go", f.Path)
+				assertEqual(t, "old_path", "old_name.go", f.OldPath)
+				assertEqual(t, "status", interfaces.FileRenamed, f.Status)
+				assertIntEqual(t, "hunks", 0, len(f.Hunks))
+			},
+		},
+		{
+			name: "renamed file with changes",
+			input: `diff --git a/utils/helper.go b/pkg/helper.go
+similarity index 85%
+rename from utils/helper.go
+rename to pkg/helper.go
+--- a/utils/helper.go
++++ b/pkg/helper.go
+@@ -1,4 +1,4 @@
+-package utils
++package pkg
+
+ func Helper() string {
+ 	return "help"
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				f := diff.Files[0]
+				assertEqual(t, "path", "pkg/helper.go", f.Path)
+				assertEqual(t, "old_path", "utils/helper.go", f.OldPath)
+				assertEqual(t, "status", interfaces.FileRenamed, f.Status)
+				assertIntEqual(t, "hunks", 1, len(f.Hunks))
+				assertIntEqual(t, "added", 1, len(f.Hunks[0].AddedLines))
+				assertIntEqual(t, "removed", 1, len(f.Hunks[0].RemovedLines))
+			},
+		},
+		{
+			name: "binary file",
+			input: `diff --git a/image.png b/image.png
+index abc1234..def5678 100644
+Binary files a/image.png and b/image.png differ
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				if len(diff.Files) != 1 {
+					t.Fatalf("expected 1 file, got %d", len(diff.Files))
+				}
+				f := diff.Files[0]
+				assertEqual(t, "path", "image.png", f.Path)
+				assertTrue(t, "binary", f.IsBinary)
+				assertIntEqual(t, "hunks", 0, len(f.Hunks))
+			},
+		},
+		{
+			name: "new binary file",
+			input: `diff --git a/logo.png b/logo.png
+new file mode 100644
+index 0000000..abc1234
+Binary files /dev/null and b/logo.png differ
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				f := diff.Files[0]
+				assertEqual(t, "path", "logo.png", f.Path)
+				assertEqual(t, "status", interfaces.FileAdded, f.Status)
+				assertTrue(t, "binary", f.IsBinary)
+			},
+		},
+		{
+			name: "multiple hunks in one file",
+			input: `diff --git a/main.go b/main.go
+index abc1234..def5678 100644
+--- a/main.go
++++ b/main.go
+@@ -2,6 +2,7 @@ package main
+
+ import "fmt"
+
++// first change
+ func main() {
+ 	fmt.Println("hello")
+ }
+@@ -20,6 +21,7 @@ func other() {
+ 	x := 1
+ 	y := 2
+
++	// second change
+ 	fmt.Println(x + y)
+ }
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				f := diff.Files[0]
+				assertIntEqual(t, "hunks", 2, len(f.Hunks))
+
+				assertIntEqual(t, "hunk1_old_start", 2, f.Hunks[0].OldStart)
+				assertIntEqual(t, "hunk1_new_start", 2, f.Hunks[0].NewStart)
+				assertIntEqual(t, "hunk1_added", 1, len(f.Hunks[0].AddedLines))
+				assertIntEqual(t, "hunk1_added_number", 5, f.Hunks[0].AddedLines[0].Number)
+
+				assertIntEqual(t, "hunk2_old_start", 20, f.Hunks[1].OldStart)
+				assertIntEqual(t, "hunk2_new_start", 21, f.Hunks[1].NewStart)
+				assertIntEqual(t, "hunk2_added", 1, len(f.Hunks[1].AddedLines))
+				assertIntEqual(t, "hunk2_added_number", 24, f.Hunks[1].AddedLines[0].Number)
+			},
+		},
+		{
+			name: "no newline at end of file marker",
+			input: `diff --git a/file.go b/file.go
+index abc1234..def5678 100644
+--- a/file.go
++++ b/file.go
+@@ -1,3 +1,3 @@
+ package main
+
+-var x = 1
+\ No newline at end of file
++var x = 2
+\ No newline at end of file
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				f := diff.Files[0]
+				h := f.Hunks[0]
+				assertIntEqual(t, "added", 1, len(h.AddedLines))
+				assertIntEqual(t, "removed", 1, len(h.RemovedLines))
+				assertEqual(t, "added_content", "var x = 2", h.AddedLines[0].Content)
+				assertEqual(t, "removed_content", "var x = 1", h.RemovedLines[0].Content)
+			},
+		},
+		{
+			name: "single line hunk header without line count",
+			input: `diff --git a/file.txt b/file.txt
+index abc1234..def5678 100644
+--- a/file.txt
++++ b/file.txt
+@@ -1 +1 @@
+-old
++new
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				h := diff.Files[0].Hunks[0]
+				assertIntEqual(t, "old_lines", 1, h.OldLines)
+				assertIntEqual(t, "new_lines", 1, h.NewLines)
+				assertIntEqual(t, "added", 1, len(h.AddedLines))
+				assertIntEqual(t, "removed", 1, len(h.RemovedLines))
+			},
+		},
+		{
+			name: "mixed file types",
+			input: `diff --git a/Dockerfile b/Dockerfile
+index 1111111..2222222 100644
+--- a/Dockerfile
++++ b/Dockerfile
+@@ -1,2 +1,3 @@
+ FROM golang:1.23
++RUN apt-get update
+ CMD ["./app"]
+diff --git a/config.yaml b/config.yaml
+index 3333333..4444444 100644
+--- a/config.yaml
++++ b/config.yaml
+@@ -1,2 +1,3 @@
+ key: value
++new_key: new_value
+ other: data
+`,
+			check: func(t *testing.T, diff *interfaces.Diff) {
+				assertIntEqual(t, "files", 2, len(diff.Files))
+				assertEqual(t, "file1_lang", "dockerfile", diff.Files[0].Language)
+				assertEqual(t, "file2_lang", "yaml", diff.Files[1].Language)
+			},
+		},
+	}
+
+	parser := NewDiffParser()
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			diff, err := parser.Parse(context.Background(), []byte(tt.input))
+			if err != nil {
+				t.Fatalf("unexpected error: %v", err)
+			}
+			tt.check(t, diff)
+		})
+	}
+}
+
+func TestDiffParser_Parse_Errors(t *testing.T) {
+	tests := []struct {
+		name    string
+		input   string
+		wantErr error
+	}{
+		{
+			name:    "empty input",
+			input:   "",
+			wantErr: ErrEmptyDiff,
+		},
+		{
+			name:    "whitespace only",
+			input:   "   \n\t\n  ",
+			wantErr: ErrEmptyDiff,
+		},
+		{
+			name:    "no diff headers",
+			input:   "this is not a diff\njust some random text\n",
+			wantErr: ErrInvalidDiff,
+		},
+	}
+
+	parser := NewDiffParser()
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			_, err := parser.Parse(context.Background(), []byte(tt.input))
+			if err == nil {
+				t.Fatal("expected error, got nil")
+			}
+			if tt.wantErr != nil && err != tt.wantErr {
+				t.Errorf("expected error %v, got %v", tt.wantErr, err)
+			}
+		})
+	}
+}
+
+func TestDiffParser_Parse_ContextCancellation(t *testing.T) {
+	input := `diff --git a/file.go b/file.go
+index abc1234..def5678 100644
+--- a/file.go
++++ b/file.go
+@@ -1,3 +1,3 @@
+ package main
+-var x = 1
++var x = 2
+`
+
+	ctx, cancel := context.WithCancel(context.Background())
+	cancel() // cancel immediately
+
+	parser := NewDiffParser()
+	_, err := parser.Parse(ctx, []byte(input))
+	if err == nil {
+		t.Fatal("expected error for cancelled context, got nil")
+	}
+}
+
+func TestDiffParser_ParseFile(t *testing.T) {
+	diffContent := `diff --git a/main.go b/main.go
+index abc1234..def5678 100644
+--- a/main.go
++++ b/main.go
+@@ -1,3 +1,4 @@
+ package main
+
++// new comment
+ func main() {}
+`
+
+	t.Run("valid file", func(t *testing.T) {
+		dir := t.TempDir()
+		path := filepath.Join(dir, "test.diff")
+		if err := os.WriteFile(path, []byte(diffContent), 0644); err != nil {
+			t.Fatalf("failed to write temp file: %v", err)
+		}
+
+		parser := NewDiffParser()
+		diff, err := parser.ParseFile(context.Background(), path)
+		if err != nil {
+			t.Fatalf("unexpected error: %v", err)
+		}
+		if len(diff.Files) != 1 {
+			t.Fatalf("expected 1 file, got %d", len(diff.Files))
+		}
+		assertEqual(t, "path", "main.go", diff.Files[0].Path)
+	})
+
+	t.Run("nonexistent file", func(t *testing.T) {
+		parser := NewDiffParser()
+		_, err := parser.ParseFile(context.Background(), "/nonexistent/path/file.diff")
+		if err == nil {
+			t.Fatal("expected error for nonexistent file, got nil")
+		}
+	})
+}
+
+func TestDetectLanguage(t *testing.T) {
+	tests := []struct {
+		path     string
+		expected string
+	}{
+		{"main.go", "go"},
+		{"src/app.py", "python"},
+		{"index.js", "javascript"},
+		{"component.tsx", "typescriptreact"},
+		{"lib/utils.rs", "rust"},
+		{"Dockerfile", "dockerfile"},
+		{"Makefile", "makefile"},
+		{"deploy/config.yaml", "yaml"},
+		{"deploy/config.yml", "yaml"},
+		{".gitignore", "gitignore"},
+		{"data.json", "json"},
+		{"style.css", "css"},
+		{"query.sql", "sql"},
+		{"infra/main.tf", "terraform"},
+		{"unknown.xyz", ""},
+		{"no_extension", ""},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.path, func(t *testing.T) {
+			got := detectLanguage(tt.path)
+			if got != tt.expected {
+				t.Errorf("detectLanguage(%q) = %q, want %q", tt.path, got, tt.expected)
+			}
+		})
+	}
+}
+
+// Test helpers
+
+func assertEqual[T comparable](t *testing.T, field string, want, got T) {
+	t.Helper()
+	if got != want {
+		t.Errorf("%s: got %v, want %v", field, got, want)
+	}
+}
+
+func assertIntEqual(t *testing.T, field string, want, got int) {
+	t.Helper()
+	if got != want {
+		t.Errorf("%s: got %d, want %d", field, got, want)
+	}
+}
+
+func assertTrue(t *testing.T, field string, got bool) {
+	t.Helper()
+	if !got {
+		t.Errorf("%s: expected true, got false", field)
+	}
+}
+
+func assertFalse(t *testing.T, field string, got bool) {
+	t.Helper()
+	if got {
+		t.Errorf("%s: expected false, got true", field)
+	}
+}
diff --git a/tests/fixtures/diffs/clean.diff b/tests/fixtures/diffs/clean.diff
new file mode 100644
index 0000000..21b5449
--- /dev/null
+++ b/tests/fixtures/diffs/clean.diff
@@ -0,0 +1,110 @@
+diff --git a/pkg/validator/email.go b/pkg/validator/email.go
+new file mode 100644
+index 0000000..a1b2c3d
+--- /dev/null
++++ b/pkg/validator/email.go
+@@ -0,0 +1,43 @@
++// Package validator provides input validation utilities.
++package validator
++
++import (
++	"errors"
++	"regexp"
++	"strings"
++)
++
++var (
++	// ErrEmptyEmail is returned when an empty email is provided.
++	ErrEmptyEmail = errors.New("validator: email must not be empty")
++
++	// ErrInvalidFormat is returned when the email format is invalid.
++	ErrInvalidFormat = errors.New("validator: invalid email format")
++
++	// ErrDomainTooLong is returned when the domain exceeds the max length.
++	ErrDomainTooLong = errors.New("validator: domain exceeds maximum length")
++)
++
++// maxDomainLength is the RFC 5321 maximum domain length.
++const maxDomainLength = 253
++
++// emailRegex validates the basic structure of an email address.
++var emailRegex = regexp.MustCompile(`^[a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,}$`)
++
++// ValidateEmail checks if the given email address is valid.
++// It returns nil if the email is valid, or an appropriate error.
++func ValidateEmail(email string) error {
++	email = strings.TrimSpace(email)
++	if email == "" {
++		return ErrEmptyEmail
++	}
++
++	if !emailRegex.MatchString(email) {
++		return ErrInvalidFormat
++	}
++
++	parts := strings.SplitN(email, "@", 2)
++	if len(parts[1]) > maxDomainLength {
++		return ErrDomainTooLong
++	}
++
++	return nil
++}
+diff --git a/pkg/validator/email_test.go b/pkg/validator/email_test.go
+new file mode 100644
+index 0000000..d4e5f6a
+--- /dev/null
++++ b/pkg/validator/email_test.go
+@@ -0,0 +1,58 @@
++package validator
++
++import (
++	"testing"
++)
++
++func TestValidateEmail_ValidAddresses(t *testing.T) {
++	tests := []struct {
++		name  string
++		email string
++	}{
++		{"simple", "user@example.com"},
++		{"with plus", "user+tag@example.com"},
++		{"subdomain", "user@mail.example.com"},
++		{"numbers", "user123@example.com"},
++	}
++
++	for _, tt := range tests {
++		t.Run(tt.name, func(t *testing.T) {
++			if err := ValidateEmail(tt.email); err != nil {
++				t.Errorf("ValidateEmail(%q) = %v, want nil", tt.email, err)
++			}
++		})
++	}
++}
++
++func TestValidateEmail_InvalidAddresses(t *testing.T) {
++	tests := []struct {
++		name    string
++		email   string
++		wantErr error
++	}{
++		{"empty", "", ErrEmptyEmail},
++		{"whitespace only", "   ", ErrEmptyEmail},
++		{"no at sign", "userexample.com", ErrInvalidFormat},
++		{"no domain", "user@", ErrInvalidFormat},
++		{"no local part", "@example.com", ErrInvalidFormat},
++		{"double at", "user@@example.com", ErrInvalidFormat},
++	}
++
++	for _, tt := range tests {
++		t.Run(tt.name, func(t *testing.T) {
++			err := ValidateEmail(tt.email)
++			if err == nil {
++				t.Errorf("ValidateEmail(%q) = nil, want error", tt.email)
++				return
++			}
++			if err != tt.wantErr {
++				t.Errorf("ValidateEmail(%q) = %v, want %v", tt.email, err, tt.wantErr)
++			}
++		})
++	}
++}
diff --git a/tests/fixtures/diffs/complexity-spike.diff b/tests/fixtures/diffs/complexity-spike.diff
new file mode 100644
index 0000000..a673003
--- /dev/null
+++ b/tests/fixtures/diffs/complexity-spike.diff
@@ -0,0 +1,205 @@
+diff --git a/pkg/processor/handler.go b/pkg/processor/handler.go
+new file mode 100644
+index 0000000..c3d4e5f
+--- /dev/null
++++ b/pkg/processor/handler.go
+@@ -0,0 +1,180 @@
++// Package processor handles request processing pipelines.
++package processor
++
++import (
++	"errors"
++	"log"
++)
++
++// Request represents an incoming processing request.
++type Request struct {
++	ID         string
++	Type       string
++	Priority   int
++	Items      []Item
++	MaxRetries int
++	Timeout    int
++	Debug      bool
++}
++
++// Item represents a single item to process.
++type Item struct {
++	Type     string
++	Priority int
++	Status   string
++	Count    int
++	Flag     bool
++	Active   bool
++	Legacy   bool
++	Value    int
++	Retries  int
++	Subs     []SubItem
++	Deps     []Dependency
++}
++
++// SubItem is a nested sub-item.
++type SubItem struct {
++	Valid bool
++	Value int
++	Kind  string
++}
++
++// Dependency tracks an item dependency.
++type Dependency struct {
++	Ready   bool
++	Name    string
++	Version int
++}
++
++// ProcessRequest handles the main request processing with multiple code paths.
++func ProcessRequest(r *Request) error {
++	if r == nil {
++		return errors.New("nil request")
++	}
++
++	if r.Timeout > 0 && r.MaxRetries > 0 {
++		log.Printf("processing with timeout=%d retries=%d", r.Timeout, r.MaxRetries)
++	}
++
++	for _, item := range r.Items {
++		if item.Type == "A" {
++			if item.Priority > 5 {
++				for _, sub := range item.Subs {
++					if sub.Valid {
++						if sub.Value > 100 {
++							log.Printf("high value sub-item: %d", sub.Value)
++						} else if sub.Value > 50 {
++							log.Printf("medium value sub-item: %d", sub.Value)
++						} else if sub.Value > 25 {
++							log.Printf("low value sub-item: %d", sub.Value)
++						}
++					}
++				}
++			}
++		} else if item.Type == "B" {
++			switch item.Status {
++			case "active":
++				if item.Count > 0 {
++					for _, dep := range item.Deps {
++						if dep.Ready {
++							log.Printf("dep %s ready", dep.Name)
++						}
++					}
++				}
++			case "pending":
++				if item.Count > 10 {
++					for _, dep := range item.Deps {
++						if dep.Ready && dep.Version > 1 {
++							log.Printf("dep %s v%d pending", dep.Name, dep.Version)
++						}
++					}
++				}
++			case "error":
++				if item.Retries < 3 {
++					log.Printf("retrying item, attempt %d", item.Retries)
++				}
++			}
++		} else if item.Type == "C" {
++			if item.Flag && item.Active {
++				log.Printf("active flagged item")
++			} else if item.Flag || item.Legacy {
++				log.Printf("legacy or flagged item")
++			}
++		}
++	}
++
++	return nil
++}
++
++// ValidateItems performs multi-layered validation on request items.
++func ValidateItems(items []Item) error {
++	if items == nil {
++		return errors.New("nil items")
++	}
++
++	for _, item := range items {
++		if item.Type == "" {
++			return errors.New("empty type")
++		}
++
++		if item.Priority < 0 {
++			return errors.New("negative priority")
++		}
++
++		if item.Type == "A" || item.Type == "B" {
++			if item.Count <= 0 {
++				return errors.New("invalid count")
++			}
++
++			for _, sub := range item.Subs {
++				if sub.Valid {
++					if sub.Value < 0 {
++						return errors.New("negative sub value")
++					}
++					if sub.Kind == "" {
++						return errors.New("empty sub kind")
++					}
++					switch sub.Kind {
++					case "numeric":
++						if sub.Value > 1000 {
++							return errors.New("numeric value too large")
++						}
++					case "text":
++						if sub.Value != 0 {
++							return errors.New("text sub should have zero value")
++						}
++					case "binary":
++						if sub.Value != 0 && sub.Value != 1 {
++							return errors.New("binary value must be 0 or 1")
++						}
++					}
++				}
++			}
++		} else if item.Type == "C" {
++			if item.Flag && !item.Active {
++				return errors.New("flagged items must be active")
++			}
++			for _, dep := range item.Deps {
++				if !dep.Ready {
++					if dep.Version < 1 {
++						return errors.New("invalid dep version")
++					}
++				}
++			}
++		}
++	}
++
++	return nil
++}
++
++// RouteByPriority routes items based on their priority and status.
++func RouteByPriority(items []Item, threshold int) (high []Item, low []Item) {
++	for _, item := range items {
++		if item.Priority >= threshold {
++			if item.Status == "active" {
++				if item.Flag || item.Active {
++					high = append(high, item)
++				} else if item.Legacy {
++					if item.Count > 0 {
++						high = append(high, item)
++					}
++				}
++			} else if item.Status == "pending" {
++				if item.Retries < 3 && item.Count > 0 {
++					high = append(high, item)
++				}
++			}
++		} else {
++			if item.Status == "error" {
++				if item.Retries >= 3 || item.Count <= 0 {
++					low = append(low, item)
++				}
++			} else if item.Status == "active" || item.Status == "pending" {
++				low = append(low, item)
++			}
++		}
++	}
++
++	return high, low
++}
diff --git a/tests/fixtures/diffs/missing-tests.diff b/tests/fixtures/diffs/missing-tests.diff
new file mode 100644
index 0000000..2909286
--- /dev/null
+++ b/tests/fixtures/diffs/missing-tests.diff
@@ -0,0 +1,95 @@
+diff --git a/pkg/service/auth.go b/pkg/service/auth.go
+new file mode 100644
+index 0000000..e5f6a7b
+--- /dev/null
++++ b/pkg/service/auth.go
+@@ -0,0 +1,42 @@
++// Package service implements business logic services.
++package service
++
++import (
++	"crypto/rand"
++	"encoding/hex"
++	"errors"
++	"time"
++)
++
++// Session represents an authenticated user session.
++type Session struct {
++	ID        string
++	UserID    string
++	Token     string
++	ExpiresAt time.Time
++}
++
++// ErrSessionExpired is returned when a session has expired.
++var ErrSessionExpired = errors.New("service: session expired")
++
++// ErrInvalidToken is returned when the provided token is invalid.
++var ErrInvalidToken = errors.New("service: invalid token")
++
++// GenerateToken creates a cryptographically secure random token.
++func GenerateToken(length int) (string, error) {
++	if length <= 0 {
++		return "", errors.New("service: token length must be positive")
++	}
++
++	bytes := make([]byte, length)
++	if _, err := rand.Read(bytes); err != nil {
++		return "", err
++	}
++
++	return hex.EncodeToString(bytes), nil
++}
++
++// IsSessionValid checks if a session is still valid.
++func IsSessionValid(s *Session) bool {
++	if s == nil {
++		return false
++	}
++	return time.Now().Before(s.ExpiresAt)
++}
+diff --git a/pkg/service/user.go b/pkg/service/user.go
+new file mode 100644
+index 0000000..f8a9b0c
+--- /dev/null
++++ b/pkg/service/user.go
+@@ -0,0 +1,39 @@
++package service
++
++import (
++	"errors"
++	"strings"
++)
++
++// User represents a user in the system.
++type User struct {
++	ID       string
++	Username string
++	Email    string
++	Active   bool
++}
++
++// ErrUserNotFound is returned when a user cannot be found.
++var ErrUserNotFound = errors.New("service: user not found")
++
++// ErrInvalidUsername is returned when the username is invalid.
++var ErrInvalidUsername = errors.New("service: invalid username")
++
++// ValidateUsername checks if a username meets requirements.
++func ValidateUsername(username string) error {
++	username = strings.TrimSpace(username)
++	if username == "" {
++		return ErrInvalidUsername
++	}
++
++	if len(username) < 3 {
++		return errors.New("service: username must be at least 3 characters")
++	}
++
++	if len(username) > 64 {
++		return errors.New("service: username must not exceed 64 characters")
++	}
++
++	return nil
++}
diff --git a/tests/fixtures/diffs/mixed-issues.diff b/tests/fixtures/diffs/mixed-issues.diff
new file mode 100644
index 0000000..87c79e1
--- /dev/null
+++ b/tests/fixtures/diffs/mixed-issues.diff
@@ -0,0 +1,84 @@
+diff --git a/go.mod b/go.mod
+index a1b2c3d..d4e5f6a 100644
+--- a/go.mod
++++ b/go.mod
+@@ -3,6 +3,7 @@ module github.com/toyinlola/shipsafe
+ go 1.25.4
+
+ require (
++	github.com/google/uuid v1.6.0
+ 	github.com/inconshreveable/mousetrap v1.1.0 // indirect
+ 	github.com/spf13/cobra v1.10.2 // indirect
+ 	github.com/spf13/pflag v1.0.9 // indirect
+diff --git a/pkg/handler/api.go b/pkg/handler/api.go
+new file mode 100644
+index 0000000..c4d5e6f
+--- /dev/null
++++ b/pkg/handler/api.go
+@@ -0,0 +1,65 @@
++// Package handler implements HTTP request handlers.
++package handler
++
++import (
++	"encoding/json"
++	"fmt"
++	"net/http"
++)
++
++// Response wraps a standard API response.
++type Response struct {
++	Status  int         `json:"status"`
++	Message string      `json:"message"`
++	Data    interface{} `json:"data,omitempty"`
++}
++
++// HandleCreateUser processes user creation requests.
++func HandleCreateUser(w http.ResponseWriter, r *http.Request) {
++	// TODO: add rate limiting before production release
++	if r.Method != http.MethodPost {
++		writeJSON(w, http.StatusMethodNotAllowed, "method not allowed")
++		return
++	}
++
++	var input struct {
++		Name  string `json:"name"`
++		Email string `json:"email"`
++		Role  string `json:"role"`
++	}
++
++	if err := json.NewDecoder(r.Body).Decode(&input); err != nil {
++		writeJSON(w, http.StatusBadRequest, "invalid request body")
++		return
++	}
++
++	fmt.Println("creating user:", input.Name, input.Email)
++
++	if input.Name == "" {
++		writeJSON(w, http.StatusBadRequest, "name is required")
++		return
++	}
++
++	if input.Email == "" {
++		writeJSON(w, http.StatusBadRequest, "email is required")
++		return
++	}
++
++	if input.Role != "admin" && input.Role != "user" && input.Role != "viewer" {
++		writeJSON(w, http.StatusBadRequest, "invalid role")
++		return
++	}
++
++	fmt.Println("user created successfully:", input.Name)
++
++	writeJSON(w, http.StatusCreated, "user created")
++}
++
++func writeJSON(w http.ResponseWriter, status int, message string) {
++	w.Header().Set("Content-Type", "application/json")
++	w.WriteHeader(status)
++	resp := Response{
++		Status:  status,
++		Message: message,
++	}
++	json.NewEncoder(w).Encode(resp)
++}
diff --git a/tests/fixtures/diffs/secrets-leak.diff b/tests/fixtures/diffs/secrets-leak.diff
new file mode 100644
index 0000000..1c1b2a7
--- /dev/null
+++ b/tests/fixtures/diffs/secrets-leak.diff
@@ -0,0 +1,51 @@
+diff --git a/internal/config/database.go b/internal/config/database.go
+new file mode 100644
+index 0000000..b7c8d9e
+--- /dev/null
++++ b/internal/config/database.go
+@@ -0,0 +1,45 @@
++// Package config provides application configuration.
++package config
++
++import (
++	"database/sql"
++	"net/http"
++)
++
++// AWS credentials for S3 uploads
++const awsAccessKeyID = "AKIAIOSFODNN7TJQMRWZ"
++
++const awsSecretKey = "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"
++
++// DatabaseURL is the production database connection string.
++var DatabaseURL = "postgres://admin:s3cr3tP4ss@db.prod.internal:5432/myapp"
++
++// ServiceAuthToken is the internal service auth header value.
++var ServiceAuthToken = "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIn0.dozjgNryP4J3jVmNHl0w5N"
++
++// ConnectDB establishes a database connection using hardcoded credentials.
++func ConnectDB() (*sql.DB, error) {
++	db, err := sql.Open("postgres", DatabaseURL)
++	if err != nil {
++		return nil, err
++	}
++
++	if err := db.Ping(); err != nil {
++		return nil, err
++	}
++
++	return db, nil
++}
++
++// MakeAuthRequest creates an authenticated HTTP request.
++func MakeAuthRequest(url string) (*http.Request, error) {
++	req, err := http.NewRequest("GET", url, nil)
++	if err != nil {
++		return nil, err
++	}
++
++	req.Header.Set("Authorization", ServiceAuthToken)
++	req.Header.Set("X-AWS-Key", awsAccessKeyID)
++
++	return req, nil
++}
diff --git a/tests/helpers.go b/tests/helpers.go
new file mode 100644
index 0000000..9524543
--- /dev/null
+++ b/tests/helpers.go
@@ -0,0 +1,162 @@
+// Package tests provides integration test utilities for the ShipSafe pipeline.
+package tests
+
+import (
+	"bytes"
+	"context"
+	"io"
+	"path/filepath"
+	"runtime"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/analyzer"
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+	"github.com/toyinlola/shipsafe/pkg/report"
+	"github.com/toyinlola/shipsafe/pkg/scorer"
+	"github.com/toyinlola/shipsafe/pkg/vcs"
+)
+
+// fixturesDir returns the absolute path to the test fixtures/diffs directory.
+func fixturesDir() string {
+	_, filename, _, _ := runtime.Caller(0)
+	return filepath.Join(filepath.Dir(filename), "fixtures", "diffs")
+}
+
+// LoadFixtureDiff parses a fixture diff file by name (e.g., "clean" loads "clean.diff").
+func LoadFixtureDiff(t *testing.T, name string) *interfaces.Diff {
+	t.Helper()
+
+	path := filepath.Join(fixturesDir(), name+".diff")
+	parser := vcs.NewDiffParser()
+
+	diff, err := parser.ParseFile(context.Background(), path)
+	if err != nil {
+		t.Fatalf("LoadFixtureDiff(%q): %v", name, err)
+	}
+
+	return diff
+}
+
+// PipelineResult holds the output of a full pipeline run.
+type PipelineResult struct {
+	Diff    *interfaces.Diff
+	Results []*interfaces.AnalysisResult
+	Score   *interfaces.TrustScore
+	Report  *interfaces.Report
+}
+
+// RunPipeline executes the full analysis pipeline (parse → analyze → score → report)
+// and returns all intermediate results.
+func RunPipeline(t *testing.T, diff *interfaces.Diff) *PipelineResult {
+	t.Helper()
+	ctx := context.Background()
+
+	// Set up the analyzer registry with all analyzers.
+	registry := analyzer.NewRegistry()
+	for _, a := range []analyzer.Analyzer{
+		analyzer.NewComplexityAnalyzer(),
+		analyzer.NewCoverageAnalyzer(),
+		analyzer.NewSecretsAnalyzer(),
+		analyzer.NewPatternsAnalyzer(),
+		analyzer.NewImportsAnalyzer(),
+	} {
+		if err := registry.Register(a); err != nil {
+			t.Fatalf("registering analyzer %s: %v", a.Name(), err)
+		}
+	}
+
+	// Run all analyzers.
+	engine := analyzer.NewEngine(registry)
+	results, err := engine.Run(ctx, diff)
+	if err != nil {
+		t.Fatalf("engine.Run: %v", err)
+	}
+
+	// Calculate trust score.
+	calc := scorer.NewCalculator()
+	score := calc.Score(results)
+
+	// Generate report.
+	gen := report.NewGenerator()
+	rpt := gen.Generate(results, score, diff)
+
+	return &PipelineResult{
+		Diff:    diff,
+		Results: results,
+		Score:   score,
+		Report:  rpt,
+	}
+}
+
+// AssertScoreInRange asserts that the trust score falls within [min, max] inclusive.
+func AssertScoreInRange(t *testing.T, score int, min, max int) {
+	t.Helper()
+	if score < min || score > max {
+		t.Errorf("score %d is outside expected range [%d, %d]", score, min, max)
+	}
+}
+
+// AssertRating asserts that the trust score has the expected rating.
+func AssertRating(t *testing.T, got, want interfaces.Rating) {
+	t.Helper()
+	if got != want {
+		t.Errorf("rating = %q, want %q", got, want)
+	}
+}
+
+// AssertHasFindingCategory checks that at least one finding has the given category.
+func AssertHasFindingCategory(t *testing.T, findings []interfaces.Finding, cat interfaces.Category) {
+	t.Helper()
+	for _, f := range findings {
+		if f.Category == cat {
+			return
+		}
+	}
+	t.Errorf("no finding with category %q found in %d findings", cat, len(findings))
+}
+
+// AssertHasFindingSeverity checks that at least one finding has the given severity.
+func AssertHasFindingSeverity(t *testing.T, findings []interfaces.Finding, sev interfaces.Severity) {
+	t.Helper()
+	for _, f := range findings {
+		if f.Severity == sev {
+			return
+		}
+	}
+	t.Errorf("no finding with severity %q found in %d findings", sev, len(findings))
+}
+
+// CountFindingsWithCategory returns the number of findings matching the given category.
+func CountFindingsWithCategory(findings []interfaces.Finding, cat interfaces.Category) int {
+	count := 0
+	for _, f := range findings {
+		if f.Category == cat {
+			count++
+		}
+	}
+	return count
+}
+
+// FindingCategories returns the unique set of categories present in findings.
+func FindingCategories(findings []interfaces.Finding) map[interfaces.Category]bool {
+	cats := make(map[interfaces.Category]bool)
+	for _, f := range findings {
+		cats[f.Category] = true
+	}
+	return cats
+}
+
+// Formatter is the interface shared by all report formatters.
+type Formatter interface {
+	Format(w io.Writer, report *interfaces.Report) error
+}
+
+// FormatReport formats a report using the given formatter and returns the output as a string.
+func FormatReport(t *testing.T, formatter Formatter, rpt *interfaces.Report) string {
+	t.Helper()
+	var buf bytes.Buffer
+	if err := formatter.Format(&buf, rpt); err != nil {
+		t.Fatalf("formatter.Format: %v", err)
+	}
+	return buf.String()
+}
diff --git a/tests/integration_test.go b/tests/integration_test.go
new file mode 100644
index 0000000..f6d1d5f
--- /dev/null
+++ b/tests/integration_test.go
@@ -0,0 +1,213 @@
+package tests
+
+import (
+	"context"
+	"testing"
+
+	"github.com/toyinlola/shipsafe/pkg/interfaces"
+	"github.com/toyinlola/shipsafe/pkg/report"
+	"github.com/toyinlola/shipsafe/pkg/vcs"
+)
+
+func TestCleanDiff_ScoresGreen(t *testing.T) {
+	diff := LoadFixtureDiff(t, "clean")
+	result := RunPipeline(t, diff)
+
+	AssertScoreInRange(t, result.Score.Score, 80, 100)
+	AssertRating(t, result.Score.Rating, interfaces.RatingGreen)
+
+	// A clean diff with source + tests should have minimal or zero findings.
+	if len(result.Report.Findings) > 0 {
+		t.Logf("clean diff had %d findings (expected 0):", len(result.Report.Findings))
+		for _, f := range result.Report.Findings {
+			t.Logf("  [%s] %s: %s (%s)", f.Severity, f.Category, f.Title, f.File)
+		}
+	}
+}
+
+func TestSecretsLeak_ScoresRed(t *testing.T) {
+	diff := LoadFixtureDiff(t, "secrets-leak")
+	result := RunPipeline(t, diff)
+
+	// Must score RED (below 50).
+	if result.Score.Score >= 50 {
+		t.Errorf("secrets-leak score = %d, want < 50 (RED)", result.Score.Score)
+	}
+	AssertRating(t, result.Score.Rating, interfaces.RatingRed)
+
+	// Must have at least one CRITICAL or HIGH finding in the secrets category.
+	AssertHasFindingCategory(t, result.Report.Findings, interfaces.CategorySecrets)
+
+	secretsCount := CountFindingsWithCategory(result.Report.Findings, interfaces.CategorySecrets)
+	if secretsCount < 3 {
+		t.Errorf("expected at least 3 secrets findings, got %d", secretsCount)
+	}
+
+	// Verify at least one finding is CRITICAL or HIGH severity.
+	hasHighSeverity := false
+	for _, f := range result.Report.Findings {
+		if f.Category == interfaces.CategorySecrets &&
+			(f.Severity == interfaces.SeverityCritical || f.Severity == interfaces.SeverityHigh) {
+			hasHighSeverity = true
+			break
+		}
+	}
+	if !hasHighSeverity {
+		t.Error("expected at least one CRITICAL or HIGH severity secrets finding")
+	}
+}
+
+func TestComplexitySpike_ScoresYellow(t *testing.T) {
+	diff := LoadFixtureDiff(t, "complexity-spike")
+	result := RunPipeline(t, diff)
+
+	AssertScoreInRange(t, result.Score.Score, 50, 79)
+	AssertRating(t, result.Score.Rating, interfaces.RatingYellow)
+
+	// Must have at least one complexity finding.
+	AssertHasFindingCategory(t, result.Report.Findings, interfaces.CategoryComplexity)
+
+	// Verify findings include HIGH severity for extreme complexity.
+	hasHighComplexity := false
+	for _, f := range result.Report.Findings {
+		if f.Category == interfaces.CategoryComplexity && f.Severity == interfaces.SeverityHigh {
+			hasHighComplexity = true
+			break
+		}
+	}
+	if !hasHighComplexity {
+		t.Error("expected at least one HIGH severity complexity finding")
+	}
+}
+
+func TestMissingTests_HasCoverageFindings(t *testing.T) {
+	diff := LoadFixtureDiff(t, "missing-tests")
+	result := RunPipeline(t, diff)
+
+	// Must have coverage findings.
+	AssertHasFindingCategory(t, result.Report.Findings, interfaces.CategoryCoverage)
+
+	coverageCount := CountFindingsWithCategory(result.Report.Findings, interfaces.CategoryCoverage)
+	if coverageCount < 2 {
+		t.Errorf("expected at least 2 coverage findings (one per new file), got %d", coverageCount)
+	}
+
+	// Coverage findings for new files should be MEDIUM severity.
+	for _, f := range result.Report.Findings {
+		if f.Category == interfaces.CategoryCoverage {
+			if f.Severity != interfaces.SeverityMedium {
+				t.Errorf("coverage finding for new file %q has severity %q, want %q",
+					f.File, f.Severity, interfaces.SeverityMedium)
+			}
+		}
+	}
+}
+
+func TestMixedIssues_ScoresReasonably(t *testing.T) {
+	diff := LoadFixtureDiff(t, "mixed-issues")
+	result := RunPipeline(t, diff)
+
+	// Should score above RED (not catastrophic).
+	if result.Score.Score < 50 {
+		t.Errorf("mixed-issues score = %d, want >= 50", result.Score.Score)
+	}
+
+	// Must have findings from multiple categories.
+	cats := FindingCategories(result.Report.Findings)
+	if len(cats) < 2 {
+		t.Errorf("expected findings from at least 2 categories, got %d: %v", len(cats), cats)
+	}
+
+	// Should have pattern findings (TODO and/or debug print).
+	AssertHasFindingCategory(t, result.Report.Findings, interfaces.CategoryPattern)
+
+	// Log all findings for visibility.
+	t.Logf("mixed-issues score: %d [%s], %d findings across %d categories",
+		result.Score.Score, result.Score.Rating, len(result.Report.Findings), len(cats))
+	for _, f := range result.Report.Findings {
+		t.Logf("  [%s/%s] %s: %s", f.Category, f.Severity, f.File, f.Title)
+	}
+}
+
+func TestAllReporters_DontPanic(t *testing.T) {
+	fixtures := []string{"clean", "secrets-leak", "complexity-spike", "missing-tests", "mixed-issues"}
+
+	formatters := map[string]Formatter{
+		"terminal": report.NewTerminalFormatter(),
+		"json":     report.NewJSONFormatter(),
+		"markdown": report.NewMarkdownFormatter(),
+	}
+
+	for _, fixtureName := range fixtures {
+		diff := LoadFixtureDiff(t, fixtureName)
+		result := RunPipeline(t, diff)
+
+		for fmtName, formatter := range formatters {
+			t.Run(fixtureName+"_"+fmtName, func(t *testing.T) {
+				output := FormatReport(t, formatter, result.Report)
+
+				if output == "" {
+					t.Errorf("formatter %q produced empty output for fixture %q", fmtName, fixtureName)
+				}
+
+				// Sanity check: output should mention the score.
+				if len(output) < 10 {
+					t.Errorf("formatter %q output too short for fixture %q: %d bytes",
+						fmtName, fixtureName, len(output))
+				}
+			})
+		}
+	}
+}
+
+func TestEmptyDiff_ScoresPerfect(t *testing.T) {
+	// An empty diff (no files) should score 100, GREEN, zero findings.
+	// The diff parser requires at least one file, so we construct the diff manually.
+	diff := &interfaces.Diff{
+		Files: []interfaces.FileDiff{},
+	}
+
+	result := RunPipeline(t, diff)
+
+	if result.Score.Score != 100 {
+		t.Errorf("empty diff score = %d, want 100", result.Score.Score)
+	}
+	AssertRating(t, result.Score.Rating, interfaces.RatingGreen)
+
+	if len(result.Report.Findings) != 0 {
+		t.Errorf("empty diff had %d findings, want 0", len(result.Report.Findings))
+		for _, f := range result.Report.Findings {
+			t.Logf("  unexpected finding: [%s] %s", f.Category, f.Title)
+		}
+	}
+}
+
+func TestDiffParserLoadsAllFixtures(t *testing.T) {
+	// Ensure every fixture diff can be parsed without errors.
+	fixtures := []struct {
+		name     string
+		minFiles int
+	}{
+		{"clean", 2},
+		{"secrets-leak", 1},
+		{"complexity-spike", 1},
+		{"missing-tests", 2},
+		{"mixed-issues", 2},
+	}
+
+	parser := vcs.NewDiffParser()
+
+	for _, tt := range fixtures {
+		t.Run(tt.name, func(t *testing.T) {
+			diff, err := parser.ParseFile(context.Background(),
+				fixturesDir()+"/"+tt.name+".diff")
+			if err != nil {
+				t.Fatalf("ParseFile(%q): %v", tt.name, err)
+			}
+
+			if len(diff.Files) < tt.minFiles {
+				t.Errorf("expected at least %d files, got %d", tt.minFiles, len(diff.Files))
+			}
+		})
+	}
+}
